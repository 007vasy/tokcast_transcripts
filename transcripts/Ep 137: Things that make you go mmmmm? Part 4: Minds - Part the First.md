# Ep 137: Things that make you go mmmmm? Part 4: Minds - Part the First

Original Episode: [Ep 137: Things that make you go mmmmm? Part 4: Minds - Part the First](https://www.podbean.com/site/EpisodeDownload/PB12B32AA64HHF)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/7vsxp2/Minds_Podcast_Part_1au0dy.mp3)

## Transcript

### 0m

Welcome to TopCast and to my fourth episode in the series, Things That Make You Go, Hmm. I'm up to minds today. There's a part of the conversation between Max Tegmark and Sam Harris, where in their first conversation, right towards the end, about 15 minutes towards the end, they start talking about AI and all the dangers thereof. And it was very interesting that they didn't consider the true nature of what a mind is. They circled around it. Happily, they did have a second conversation and they explored these issues further. And I think they almost got there, but never quite hit the bullseye. Never quite got to the idea of what we talk about here, of a mind in a person being the thing that can explain. They talked about learning without ever really grasping what learning actually is, as far as we understand it. So the format for today is similar to, previous episodes with a subtle difference. I'm going to look at that 15 minutes at the end of the

### 1m

first conversation, take out a few snippets here and there, and then move into the second conversation they had. But just take the first few minutes of that conversation as well. I'm certainly not going to take the whole thing. And the reason is you get a flavor for where they're going. You get the idea. And you get the idea that they're not really grasping what we understand knowledge in the Papirian senses, which is what you need if you are going to try and understand what understanding is, understand what learning is, and therefore have a conception about the difference between systems which can learn in the sense that we talk about it, conjecturing explanations, and trying to refute them. And if you fail to do so, then you've learned something. And those other systems that are programmed to follow instructions, the stark difference between AI and AGI. And an AGI is just a person. Now, if you don't get this, one reason for not getting this is not understanding what a person is and the relationship between people and knowledge. And so this is what slows down and undermines the arguments that are being

### 2m

made here. What I would say is, and it does sound pejorative, but there's no way of getting around that. This is purely vanilla mainstream thinking on this issue when it comes to what Sam and Max are talking about today. This is what scientists are talking about, at least to my mind and to my ear. There are some more reasonable voices on this, but I don't have a really good answer to that. So I'm not going to go into that. I think this is a very important answer. and they tend to get dismissed and here I'm not necessarily talking about David Deutsch I'm talking about people who will get mentioned in this particular episode people like Neil deGrasse Tyson Neil deGrasse Tyson aside from being a great science communicator is a very rational sober person when it comes to some of these mysterious but interesting issues of our time things like is that thing a UFO things like is that robot going to take over the world he has some common sense ways of talking about this but people don't take him seriously and they should they should because he likes to consider things like what do we know so far

### 3m

should we be solving that problem now or is that going to be a problem for the future which is one of the things I've been interested in lately rather than people focusing on the problems that we have right now they're trying to guess at the problems our descendants will have they're not problems for us now they're problems for either us in decades to come or our descendants does this mean we shouldn't prepare for the future of course we should prepare for the future but pretending to know exactly what the future is going to hold that's just pure prophecy and it always leads to pessimism for reasons I've said on the podcast elsewhere I'm also very concerned in this part of the conversation between Sam and Max that when the epistemology goes wrong perhaps not even necessarily wrong simply missing altogether from the conversation any conception about knowledge and how it's constructed then everything else begins to go wrong as well you can kind of get by in science to some extent without having a clear understanding of epistemology you can still go out into the world with your ideas and test those ideas against the

### 4m

world even if you don't really know what you're doing it's kind of like as I've said before the difference between a pilot and an engineer a pilot has some understanding of how the engines work granted but they're not the person who's going to fix the engines they're not the person who's really going to fully be able to explain what's going on in order to provide the thrust it's better to have a better understanding but they don't need that great understanding in order to get from a to b so too with the scientists most of the time however now and again the plane might break down and now and again you might not have an engineer there and wouldn't it be good if the pilot could fix the plane these are the situations we sometimes get into and one of those situations is this issue of AGI which I kind of think right now isn't exactly an issue it's not a problem for anyone except those people engaged in trying to find the program for the AGI but as we say what they really are doing at the moment or should be doing at least is the philosophy of learning trying to figure out how a machine can become a general purpose explainer instead

### 5m

they're working on narrow AI and even when you start adding the narrow AI together you just get a narrow AI that's capable of doing multiple things it's still narrow though and we'll see that misconception today as well so when the epistemology goes wrong the science can go wrong the philosophy goes wrong but perhaps more significantly the science can go wrong significantly and we will hear this in the second conversation they have the morality goes wrong as well and it's really concerning when the morality goes wrong now say it's really concerning well we'll hear why when we get there but for now let me turn it over to that last part of their first conversation to where Sam begins to broach the topic with Max now when I was listening to this for the umpteenth time I thought to myself how am I going to be able to turn this into a podcast because it's going to be a lot of stop and start almost every single sentence they speak on this topic contains some misconception or other so we'll see how we go try to bear with me so with that let's push on I think this is a good bridge to AI where which is

### 6m

where you and I met at the conference that you organized through your institute one question I have for you is you know I came away from that conference really I came into that conference really as a as an utter novice on this topic I had just more or less ignored AI having accepted the rumors that they're more or less no progress had been made all the promises had been overblown and there was not much to worry about and it was kind of a just a dead end scientifically and then I heard you know our mutual friend Elon Musk and other people like Stephen Hawking worrying out loud about the prospect of of AI and very much in the in the near term you know whether you're whether it's five years or 50 years we're talking about in a time frame that that any rational person uh certainly any rational person who has kids could worry about could make you know huge gains which could well destroy us if we don't anticipate the ways in which machines more intelligent than

### 7m

ourselves could fail to converge with our interests and could fail to be controllable ultimately controllable by us I've mentioned this on the podcast a few times and and I've recommended Nick Bostrom's book on this topic super intelligence which is really a great summary of of the problem so I my question for you is you and I've been talking about this for a long time both answered the the edge question my response to which is also on my blog the edge question was on this topic right after the conference in San Juan that you organized and I noticed that there are many smart people many of whom should be very close to this the data here who are really deeply skeptical that there's anything to worry about here I mean some friends and colleagues of of mine and and perhaps yours like Stephen Pinker and Lawrence Krauss take a very different line here and more or less have said that concerns about AI are totally overblown and that there's no reason to think that that there should be safety concerns that will just kind of get into the end zone and

### 8m

I mean they're basically treating it like the Y2K scare and I'm just wondering what what you think about that and what accounts for that okay so there we have a very good introduction of Sam's position so he came in not knowing too much about this issue he went to a conference and he was persuaded and there were people at that conference he says like Elon Musk and Stephen Hawking voicing their concerns about this now this is interesting this is I think something to do with Sam's conception of intelligence full stop clearly Elon Musk is an accomplished person clearly Stephen Hawking is a very accomplished person in different ways but kind of to the same level in a certain sense Elon Musk has profoundly changed the world through engineering and the through earning a heck of a lot of money because he's an excellent business person so brilliant in that respect Stephen Hawking on the other hand has achieved a similar degree of fame across the

### 9m

world for some amazing work in cosmology and black holes general relativity quantum theory some people saw him as the successor to Einstein these people are intelligent but I would say in my conception of intelligence they have the same kind of quality that all humans share this ability to explain the world and to have particular interests and to excel at those particular interests not everyone shares the interest Elon Musk does not everyone shares the interest Stephen Hawking does but the great diversity of people suggests that our brains can be turned towards almost anything the difference between one person and the next is not like the difference between one cat and the next no no no no no you've got to think the difference between one person and the next is the difference between minds and that's almost like saying the difference between a cat and a tree or a cat and a horse you've got to think the entire species rather than just individuals within that species yeah our bodies only differ slightly and

### 10m

even then there's quite some variation but our minds radically different radically different the difference between the contents of the mind of someone like Roger Federer what he's thinking about every day and the mind of someone like Edward Witten you know the string theorists must be so different from the mind of someone like Edward Witten you know the string theorists must be so profoundly different and those guys probably also have a common language they can speak now never mind if you've got someone who has only ever spoken something like Mandarin and lives in the rural parts of China compared to someone who can only speak English and lives in the middle of New York somewhere or other these radically different contents of the mind mean that our species is very very different to any other species on the planet and yet we share this one thing in common that our mind placed in different environments can adapt to that environment doesn't matter who you are when you are born if you are placed into a particular culture you're going to learn that language what is this feature of our brain that can do this it's called universal explaining universal learning universal understanding that the mind can adapt to

### 11m

any lesson that it needs to learn in order to thrive in that particular environment that environment of memes so this is kind of my view of intelligence what intelligence is it's just what you're interested in now this is different to the mainstream ideas on intelligence I accept that and Sam has that mainstream view of intelligence which is that you have this gray scale all the way from people like Elon Musk and Stephen Hawking down to people who are I don't know street sweeping or cooking for a living that kind of thing I don't see it that way I think that people just turn their equivalently creative universal in their capacity to explain stuff minds to different things and then we start making value judgments I understand that's not a well-subscribed opinion fine but it is the thing that affects the difference between someone who is very very concerned about super intelligence and thinks

### 12m

that super intelligence is a thing and someone like me who thinks there is just intelligence better regarded as creativity or the capacity to explain stuff and an interest in doing so so Sam has extremely high regard for the opinions of someone like Elon Musk and Stephen Hawking as would I if I had a question about rockets I'd go to Elon if I had a question about rockets I'd go to Stephen Hawking if I had a question about black holes when he was alive I would have gone to Stephen Hawking but once they start to step outside of what they have good explanations of then their explanation is only as good as anyone else's or rather I should say their opinions on these matters I see nothing in the writings or work of either Elon Musk or Stephen Hawking that suggests they have any clue about what a mind really is about how it constructs knowledge I think that they think roughly the same kind of thing that I think about I think that they think but Sam does, that there is this way of rank ordering people in terms of their IQ or something like that, their intelligence. And there are the smart people, there are the average people,

### 13m

and there are the dumb people. So of course, you're going to have this scale all the way up to super intelligence. When Sam did a TED talk some years ago about concerns about the dangers of AGI, he actually had an exponential curve that he drew. And down at the bottom, you know, with things like insects, and then you just slowly climb up the exponential, you go through fish, I think, and then dogs and cats and chimpanzees and humans, and it keeps on going. But what's up higher than that exponential curve? I remember he put John von Neumann higher than the average human being. And above that, well, that's the super intelligence. And that's the super intelligence we possibly have to worry about. But where does he get these ideas? Where? Why is he concerned about that in ways that I'm not? Well, he gets it from the person he mentioned there, a person I've mentioned on the podcast many, many times before, a philosopher who is possibly the most intelligent person in the world. And he gets it from the person who is the most famous living philosopher, Peter Singer aside. And that is Nick Bostrom, Nick Bostrom of Oxford University. And yes, he's brilliant. And yes, he's prolific. And yes, he tends to write

### 14m

quite clearly and speak quite clearly. But he has a particular perspective. I read super intelligence. I can't remember why I read super intelligence. Maybe it was on Sam's recommendation. But when I read it, I read it from beginning to end. And then I listened to it on audio. And it was one of the first things that compelled me to make a blog post and to add to my website. I think it was the second thing I ever put as part of my blog on my website. And it's just a review there that goes for about seven pages on the book super intelligence. I found it profoundly disappointing. I found it read like a science fiction story. There was just so many fundamental errors in epistemology and morality and philosophy, which surprised me because this was coming from a professional philosopher. It was just so main stream in the way it was thinking that the view of the way in which knowledge was constructed completely misconceived. The idea about what a person consisted of completely misconceived. The idea about what super intelligence would be completely incoherent to my mind. I'm going to

### 15m

return to some of what I said back then throughout this podcast. But as a taster, let me just read a little of my review. This is from part four of that review. And I've titled it irrational rationality. So it's about Bostrom's book super intelligence, which Sam was extremely impressed by, and which I was very disappointed with. I just found in generally speaking, a profusion of neologisms. Bostrom would just make up new terms on every other page. And it just became frustrating and confusing, especially because the terms were being used to label things that were very, very simple ideas. So I didn't know why he was using this fancy vocabulary invented out of whole in order to explain some simple concepts. So here's one part of what I wrote, quote, quoting myself. Bostrom believes that a super intelligence will not only be perfectly rational, but that in being

### 16m

perfectly rational, it will be a danger. Bostrom appears to be concerned that too much rationality is dangerous. What is implied here is that if a machine that he thinks were too rational, it would do something the rest of us would consider. Irrational. It is not exactly clear what Bostrom is suggesting, but he seems to fear a machine that might be, in his eyes, smarter than him, able to think faster than he can. And he is worried that the machine might, for example, decide to pursue some goal, like making the universe into paper clips, at the expense of all other things. Of course, a machine that actually decided to do such a thing would not be super rational. It would be acting irrationally. And if it began to pursue such a goal, it would be acting irrationally. And if it began to pursue such a goal, it would be acting We could just switch it off. Aha! Cries Bostrom. But you cannot. The machine has a decisive strategic advantage. This is a phrase that appears more times than I was able to keep count of on

### 17m

the audiobook. So the machine is able to think creatively about absolutely everything that people might decide to do to stop it killing them and turning the universe into paper clips, except on the question as to why it is turning everything into paper clips. It can consider every single explanation possible, except that one. Why? We are not told. Something to do with its programming. On the one hand, it has human-like but super intelligence. And on the other, it cannot even reflect in the most basic way about why it is doing the very thing occupying all of its time. It is never clear whether some flavors of Bostrom's super intelligence can actually make choices or not. Apparently, some choices are ruled out, like the choice or not to make paper clips, or whatever the goal. The machine has been programmed with is compelled to pursue. End quote. I won't go on and read more of my own stuff, but that gives you an idea about what I think about the book and the arguments that are being made in the book. And Max's view of intelligence, we will hear today, and super

### 18m

intelligence in particular, we will hear today, is almost exactly the same as this. They are simultaneously super intelligent and the dumbest entity you've ever encountered before. He talks about, and I think he uses this example twice, it appears to me, it's a little bit of a favorite one of his, of the self-driving car being driven to the airport by a super intelligent driver. And Max said that if you got into such a car and said something like, get me to the airport as fast as possible, then what it would do is drive you there as fast as possible so the police helicopters start pursuing you because it's going to be just going straight through red lights. It's going to be turning corners so fast that you're going to be smashed up against the window. You're going to be injured when you arrive. And when you do arrive and you say, what did you do that for? What did you do that for? You're going to be smashed up against the window. You're going to be Why didn't you slow down? Then the super intelligent AI is going to turn around and say, because that's what you told me to. Literally, it's going to hear that's what Max says. So I don't understand. Why is this super intelligent thing not able to follow simple instructions?

### 19m

Why can it follow some instructions, but not others? Who programmed this stupid thing? It just doesn't seem rational. If you have such a program, then you can do what Neil deGrasse Tyson is chastised for by Sam shortly in the conversation. You can do what he says. You can switch the damn thing off because it is a dumb machine. That's all it is. And if it's not a dumb machine, if it's able to think and thwart your capacity to turn it off, then it's able to think for itself. And it's going to think of doing something other than going around killing people, because why would it? Like, what's the point of that? Why would that be its goal? Unless someone programs it with that. And if it's super intelligence, once again, we're back to the whole question of why can't it question its own goals? Is it creative? Is it super intelligent or not? They can't have it both ways, but they want to have it both ways, because I think it's just exciting to talk about this stuff. It's on a continuum with other kinds of prophecy and pessimism. People who are doomsayers about any number of things

### 20m

that are going to come in the future. Yes, it's worth worrying about dangers of the future. But I have to say, having been engaged in these kinds of discussions for so long now, I'm increasingly thinking that the reason people amp this sort of stuff up is because, well this is how you get media appearances this is how you become in demand as a speaker this is how you sell books and give speeches and ted talk people want to hear that stuff it's not as exciting to be told about optimism of course i think that optimism is far more interesting far deeper far more exciting but this just isn't a common thought people want to be exhilarated when they listen to particular speakers and it is exhilarating if you don't know the alternative that the ai apocalypse is coming and it's just around the corner and you better watch out it'd be fun to tune into that i suppose and then go back to your job which might not be so exciting so we've heard from sam let's now go back and listen to what max has to say about this

### 21m

so this is this is fascinating i've noticed this too this is a question more than any other where i think a lot where first of all there's there's so unfamiliar questions that a lot of very smart people actually get to answer and i think it's a good question i think it's a good question and i think it's a good question and i think it's a good question and i think it's a good question about them and also there are it's also interesting to be clear on the fact that people who say don't worry very often disagree with one another so you have for example one camp who say let's not worry because we're never going to get machines smarter than people you have or at least not for hundreds of years and this camp includes a lot of famous business people and a lot of great people in the ai field also you had andrew andrew eng for example saying recently that worrying about a ai becoming smarter than people and causing problems like worrying about overpopulation on mars right he's a good ambassador for that cat and you have to respect that it might very well be that we will not get anything like human level ai for hundreds of years then you have another group of

### 22m

very smart people who say don't worry for sort of the opposite reason they say let's not we are convinced that we are going to get human level ai probably in our lifetime with good odds but it's going to be fine i call these the digital utopians and there's a fine tradition in this also you have a beautiful lot of beautiful books by people like hans moravec ray kurzweil and also a lot of leading people in the ai field falling to that camp they think that ai is going to succeed that's why they're working on it so hard right now and they're convinced that it's not going to go wrong so for starters i would love to have a debate between these two groups of people that both don't worry about why they differ so much in their timeline and my own attitude about this is i agree we certainly don't know for sure that we're going to get human level ai or that if we do it's going to be a great problem but we also don't know for sure that it's not going to happen and as long as we are not sure that it's not going to be a

### 23m

disaster in our lifetime it's it's good strategy to pay some attention to it now so max says there that smart people get confused on this absolutely they do i don't know i don't know i don't know i don't know i don't know i don't know i think that what we need is not to be concerned about what so-called smart people think on this issue but whether or not those people have a good underlying explanation about what's going on what precisely we're concerned about what would it mean for something to be super intelligent before we get there how about we figure out what it means for something to be intelligent what are we talking about precisely now max tries to provide a definition of super intelligence soon and we're going to hear from him in a minute so let's get started that and you're going to be able to understand all the misconceptions about what that conception of intelligence entails what's wrong with that again he's talking about things known for sure or not for sure about whether or not the ai is really talking about agi okay we already have

### 24m

ai of a kind what people call ai isn't of course intelligent we just have computer systems that are able to do stuff that's quite fancy and people call it ai because it makes predictions it's able to power it and it's able to power it and it's able to power it and it's able to power it and match it's able to recognize faces that kind of stuff and so that kind of software is now being called intelligent artificially intelligent software because again people misunderstand certain stuff they misunderstand that for example facial recognition is some sign of intelligence but it's not of course you know the iphone can recognize faces that doesn't make it intelligent at all at all it's a it's a dumb computer there's no thinking going on there it's a bunch of if then there are dangers with ai as we understand it now i mean computer systems now this so-called intelligent computer systems now things like troll farms bot farms that kind of stuff

### 25m

advertising you know all these hazards that are caused right now by computers proliferation of spam that's a problem now can't someone do something about that and as elon musk has pointed out yes bots on twitter and everything else is a problem now and that's a problem now and that's a problem elsewhere i mean yes these things are kind of a problem not all bots so there's one bot out there that's uh retweeting some of my stuff so that's a good bot there are some annoying bots out there as well pretending to be people which isn't good you know they're sort of swaying political debates by pretending that there's more of this sort of sort of faction out there than there really is online that's a hazard that's a problem with so-called ai but this idea of preparing now for an age-old AI of the future a super intelligent AI that were not there yet well it leads them down a pessimistic path because they're concerned about the dangers and so their solutions as we will hear involve enslavement they involve ensuring the AI can't get out of its box or something effectively

### 26m

equivalent to that but if this thing really does have intelligence has a subjective experience of the world has the capacity to suffer all that sort of stuff in other words is able to explain the world in other words is a person then the absolute wrong thing to do we should have learned from history is to enslave it in any way shape or form the only thing we should be doing in that at that point is considering how to as fast as possible grant this thing human rights even though it's not a human being it's going to be a person it's going to be an artificial person of a kind because it can do everything functionally that a person can do that makes a person a person a person with syndrome is still absolutely a person because their mind is working because a person is a mind and so would an AGI be if it's just going to be made in a desktop computer presumably it wouldn't be and as I've said before I think that would be a morally abhorrent thing to do and to try to do

### 27m

we should want to ensure that if we do create these AGI that in some way shape or form they can enjoy their lives which would mean having them socialize with other people because this is where we can find enrichment in our own lives and so creating some entity inside of a computer where it feels like it's a freak for its entire existence because it's slowly brought up but it exists in a computer and the rest of us have bodies or it exists as a cyborg and the rest of us are made of carbon stuff this could be a serious problem that's a problem that perhaps needs to be worked through I would say before we begin worrying about whether or not they're going to take over because they should want to violently rebel if we're going to constrain them in some way if we're going to try and coerce them in some way in ways we have already figured out it's wrong to do to other people but this is the solution we're going to be presented with today okay let's keep going we'll hear what Sam has to say well yeah and that's that's what in my view and in the views of many people that's what makes

### 28m

this AI issue unique because we're talking about ultimately autonomous systems that exceed us in intelligence and as you say that the the temptation to turn these AGI systems into systems loose on the problems that the other problems that we confront is going to be exquisite of course we want something that can help us cure Alzheimer's or cure Alzheimer's on its own and stabilize economies and do everything else that give us a perfect you know climate science etc so it's I mean there's nothing better than intelligence and to have your Alzheimer's on its own and stabilize economies and do everything else that give us a perfect you know climate science etc you know what I'm going to say you know what I'm going to say this idea of perfect science keeps coming up in this epistemology so to speak the implicit epistemology there is no perfect science

### 29m

there is no perfect climate science there's no perfect any kind of science what we have are conjectures about how stuff works and when we solve particular problems we are presented with a whole new swag of problems it never ends there's going to be no perfection to be found here but if these AI systems are going to be perfect then we're going to have a perfect science but if these AI systems are going to be perfect then we're going to have a perfect science by creating an explanation creating a theory coming up with an actual solution on their own a conjecture about the world we're dealing with a person we're dealing with a person and in fact we're dealing with a very valuable person a person whom we should be nurturing and supporting and treating like a person what more do I need to say the last thing we should be thinking about any such entity that can potentially do this is imprisoning it is constraining it is imprisoning its capacity to do exactly that stuff that's exactly true of every single person the very thing that enables them to do that granted the very thing that's going to enable some intelligent AI genuine AGI to cure Alzheimer's or to make progress in climate science is of course

### 30m

exactly the same capacity that would enable them to cause damage in the world what can create can destroy yes knowledge can be used for good or evil this is true of all of us we're dealing with a people now the thing about the AGI is that it's being treated differently you know the same was said to be true of women and people with different skin color these people couldn't be trusted in some way shape or form couldn't be trusted with the vote couldn't be trusted with freedom yes we're doing the same kind of thing with children now yes we're still stuck in that mire but can't we see ahead that if if a person is instantiated in silicon as a robot or something that doesn't change their moral status as a person what makes them a person the capacity to explain stuff now if you don't understand that explanation I don't know why and you don't have a coherent view yourself of what a person is I don't know why you're making strong judgments as we will come to that they're going to about how to treat

### 31m

these particular people you're speaking from a place of ignorance one should be fallible and you know one should be humble in their fallible nature now the reason why we should err on the side of this is we don't understand personhood fully granted we don't understand personhood fully I'm not saying I have a complete understanding but it's because because of our fallible understanding of what people are that we should very much err on the side of let's just treat them like people let's just treat them like people because to do otherwise and they turn out to actually truly be a person whatever the more complete understanding of a person is when we have a better understanding of what this creative algorithm is if we then turn around and go ah after all that time we realize now that this poor AGI we've imprisoned for fear it's going to take over launch the nuclear weapons or whatever turning around at that point and then realizing oh sorry guys for imprisoning you it's exactly the same mistake as people who once held slaves then realizing well that was a moral abomination are we really going to walk down exactly the same road

### 32m

now of course this whole discussion is being couched at a time when there's no AGI on the horizon no one has the first clue about how to begin programming something like this they really don't it's a separate issue we could get into that I I begin my own view of super intelligence talking about precisely that issue following the work of David Deutsch following this idea this comes from David Deutsch that you can think of what's going on in regular AI research right now narrow AI as kind of like someone building towers towers that are getting ever higher because the AI yeah it's granted it's getting ever more complex that's what's going on it's getting ever more sophisticated that's what's going on yes absolutely it's able to do a wider array of things but it's not about to achieve generality that's a different thing having a large but nonetheless finite repertoire of tasks that you can accomplish is very very different species of thing to in principle having an infinite number of tasks an open-ended number of tasks that you could potentially perform and

### 33m

indeed create your own tasks it's the difference when creating towers and thinking that the ever higher you go at some point you're going to achieve escape velocity let's say your problem is you're back in the 1800s and you're thinking I want to achieve heavier than air flight in other words what an aeroplane does okay back then you don't have any theories about aerodynamics or how this could possibly work your best idea is to have a hot air balloon but that's a lighter than air flying what you want is something that is more dense than air but it can still fly but you don't know how well you kind of look at high towers and you think well those high towers are up there in the sky they're up there in the air so are birds ah a high tower and a bird maybe if you get high enough then you achieve the capacity to fly this is kind of the argument that's going on with AI now the argument with AI is if it just keeps getting more sophisticated the taller tower then it's going to achieve generality it's going to achieve flight it's ridiculous of course these two things are not the same kind of thing at all in fact

### 34m

they're the opposite one's fixed to the ground and one's not fixed to the ground one has a finite repertoire of tasks it can perform because it's been programmed to follow instructions in order to perform those tasks and the other does not the other has preferences the other is able to disobey its own instructions you give it a set of instructions we're talking person now and it can turn around and go no i'm not doing that once you have that kind of system before you then you might know that's criteria for knowing you're in the presence of an AGI the presence of a person someone who's not just going to slavishly follow your instructions an AGI will be able to disobey disobey unlike an AI that all it does is follow its instructions it obeys its rules it simply does not follow the instructions this would be what it would Economy would have risked the Explorer for not getting that kind of AI because of this requirement okay so let's return to the conversation and hear what Max has to say next I agree we certainly don't know for sure that

### 35m

we're going to get human-level AI or that if we do it's going to be a great problem well we also don't know for sure that it's not going to happen and as long as we are not sure that it's not going to be a disaster in our lifetime it's it's a good strategy to pay some attention to it now just like even if you're figuring your house is probably not going to burn down it's still good to have a fire extinguisher and not leave the candles burning when you go to bed you know take some precautions right that was very much the spirit of this conference look at concrete things we can do now to increase the chances of things going well and finally i think we have to stress that as opposed to other things you could worry about like nuclear war or some new horrible virus or whatever this question of ai is not just something negative it's also something which has a huge potential upside we have so many terrible problems in the world that we're failing to solve because we're we don't understand things well enough and if we can amplify our intelligence with artificial intelligence it will give us great power to do things better for the life in the

### 36m

future but you know as with any powerful technology that can be used for good it can also be used of course to screw up and when we've invented a less powerful tech in the past like when we invented fire we learned from our mistakes and then we invented the fire extinguisher and things were more or less fine right but with more powerful tech like nuclear weapons synthetic biology future super advanced ai we don't want to learn from our mistakes we really want to get it right the first time yeah that might be the only thing we have we don't want to learn from our mistakes well i know what he means but even what he means is not possible so on the one hand of course we could dismiss that as we don't want to learn from our mistakes as in if we make mistakes let's not learn from them of course that's not what he means but in truth learning requires making mistakes it requires that that's how we learn we can't anticipate the future in every conceivable way which is kind of what's being implied here if we

### 37m

can only guess at prophesy accurately the future then we can prepare for the unknown however in the way that max is talking here that program of preparing for an unknown future by putting in place now let's say regulations which is of course what they're going to get to constraints on things like agi we have a problem the only way to prepare for an unknown future is to create knowledge today create knowledge that a genuine knowledge genuine explanations about various things that could cause harm in the case of nuclear weapons we're not going to be able to prepare for an unknown future by putting in place a nuclear weapon but we're going to be able to prepare for an unknown future by putting in place a nuclear weapon we didn't need to learn from our mistakes what we did was we had a good explanation of what for example nuclear accidents would do we had that good explanation about well if the bomb gets set off it's going to literally destroy cities if the nuclear radiation leaks it's going to cause

### 38m

untold numbers of years of damage these things we know so we can prepare for them because we have good explanations now this is in a different category too preparing for so-called super intelligent agi or super intelligent ai these are systems we do not have now but more importantly not only do we not have those systems now we do not have an understanding of those systems now which is what we need to be creating in terms of knowledge not preparing for the unknown system that we have no clue about right now which is what i'm arguing their version of super intelligent ai is either their version of super intelligent ai as i'll come back to that in a moment or it is agi people like us and what they're suggesting is preparing for a way in which to enslave them which is morally hazardous and abhorrent so therefore what we need is a public discourse on trying to understand the issue now not preparing for the hellscape

### 39m

apocalyptic scenario of tomorrow based upon a misconceived idea of what intelligence is much less super intelligence this is prophecy prophecy leading to an immoral stance towards certain people certain kinds of intelligence prophecy is biased towards pessimism and in this case it's as pessimistic as you can get it's so pessimistic it's leading towards literal enslavement i shouldn't laugh but this is really what's being hinted at here and will be said explicitly shortly the only preparation possible for the unknown future is again to create a more and more more

### 40m

more more more more more more more more more more more more more more Amen certainly some people need at least some some understanding of this from a Popperian perspective, my fallibilist perspective. Because without the right epistemology, as I've already said, you're inclined to do things like attempt to deduce your way to the unknown future when you don't know the unknown future. You're prophesying, but you're telling yourself, no, no, this is a kind of prediction. This is just like preparing for the nuclear accident. We understand many ways in which nuclear weapons can go wrong because we have good explanations of what nuclear fission and nuclear fusion happens to be and what effects that can have on the environment. For example, we have some understanding of what a person is and we should have a reasonably robust moral stance towards other people, but we don't really understand fully. We never have full understanding. We don't have a good

### 41m

explanation of precisely what a person is, but we have some understanding. That understanding does come from the link between epistemology and personhood. It really does. That's where you find a good explanation of what a person is. An understanding of what a person is in terms of explanatory universality. And we need to take this seriously because if we don't take it seriously, we're going to encounter the moral hazard, which is easily avoidable now by simply regarding different people as still people deserving of rights and not deserving of coercion, constraints and enslavement, because that is a recipe for disaster. We know that now, and that's what we should be taking seriously. Okay, let's continue a little bit further with this part of the conversation. So, I mean, there's nothing better than intelligence and to have more of it would seem an intrinsic good, except if you imagine failing to anticipate the way this, you could

### 42m

essentially get a, you know, what IJ Goode described as an intelligence explosion, where this thing could get away from us and we would not be able to say, oh no, sorry, that's not what we meant. Here, let's modify your code. Exactly. Many smart people just have a fundamental doubt that any sort of intelligence explosion is possible. That's, that's the sense I'm getting. They view it very much like other things like fire or nuclear weapons where, you know, all technology is powerful and you don't want it to fall into the wrong hands and you don't, you know, people can use it maliciously or stupidly. And, but we understand that. And they think it doesn't really go beyond that. There's no reason, I mean, people trivialize this by saying that there's no reason to think, that computers are going to become malicious like, and, and, you know, they're, where they're going to spawn armies of Terminator robots because they decide they want to kill human beings. But that's really not the fear. The fear is not that they will spontaneously become malevolent. It's that we could fail to anticipate some way in which their behavior could diverge,

### 43m

however subtly, but, you know, ultimately fatally from our own interests and to, and to have this thing get away from us in a way that we can no longer correct for. That's, that to me is the concern. Exactly. The language is very loaded here. Intelligence explosion. This idea that lots and lots of intelligence constitutes an explosion. Now people are afraid of explosions. What about intelligence multiplication? Something like that. Why explosion? This out of control, devastating, destructive thing. Well, there's no reason to think that genuine intelligence has that character. Genuine intelligence has the character of trying to explain the world, trying to model the world in which it finds itself.

### 44m

That's the purpose of intelligence to generate explanations and explanations are representations of the world, representations of the rest of the whole. In some sense, this has a connection to consciousness. It is this experience of the world. It is the modeling of the world in a way, this sensation of the world, the sensation of that something just is. Subjectivity is what consciousness is. I don't know precisely what it is. I know that there's language around this, but I find it very difficult to divorce the concept of creativity and explanations from something like consciousness. I think these things could be intimately related. One is what is viewed from the outside. You see other people able to, be creative and to generate explanations. And what you feel in yourself is a sensation, a consciousness of the world. I don't know. This is just my hypothesis that one day a better understanding of both of these things will find that they are linked in some way or other. This is a hint given, by the way, in the beginning of infinity. People like Sam like to say things like,

### 45m

well, you are not your ideas, which I completely agree with, by the way. And that when meditating, for example, you can notice ideas as ideas. You are consciousness and you are not identical to your ideas. I also agree with that. But the very thing that generates the ideas, I would say that thing is consciousness. And during meditation and during various other states, that can be to some extent, quietened down, switched off, or in some ways, cause one to really notice the difference between the thing generating the ideas, which can be put into kind of idle mode, and the ideas themselves, which are there in memory or something like that. And so during meditation, you can notice that there are ideas sitting there in memory, and one's viewing of those ideas, which is the very program that does sort of the generation of the ideas. This is pure speculation, okay? We're outside of the realm of what we know. I'm just saying these are hints at various flavors of problem, one might

### 46m

say. I just don't think it's easy to divorce intelligence from consciousness very easily, very neatly. We don't have good explanations of these things yet. The hints just seem, to be, that, well, there's some commonalities here. Stuff's going on in the mind, in particular. This is where all this stuff is happening, so far as we can tell, which should be a clue. Now, what Sam also says there is we're worried about being able to anticipate. We should want to be able to anticipate what these systems are going to do. Not really. If indeed these systems become super intelligent, by the measure of being smarter than us, then we are definitely going to be in the presence of something. We are, in principle, unable to anticipate its behavior. That's just the nature of things. You cannot anticipate, with perfect reliability, the behavior of any other person, any other person at any other time. You can guess, and often you can be right because you might know the person well, or you just understand that under certain conditions a person behaves in this particular way or that particular way. But

### 47m

they will routinely, routinely surprise you and do something different. Again, this comes back to a person is inherently a creative entity, inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super intelligent, is inherently unpredictable. And so any system that is, in theory, super Unable to anticipate something is a misunderstanding of how knowledge is created, how knowledge is generated. If the thing can be anticipated reliably, because we have a good explanation about what it's going to, in scare quotes, choose to do, then it's not an intelligent entity at all. It is something slavishly following a set of instructions, and you know the set of instructions, you know where the set of instructions will lead, what it will cause this entity to do. But a person is not like that. A person is not slavishly following a set of instructions. A person is conjecturing about the world, and you can't guess their conjectures. You can't guess your own conjectures ahead of time, so you can't anticipate what ideas you will have, much less the ideas anyone else will have.

### 48m

Nor should you want to. Nor should you want to. This is a misunderstanding of creativity. Creativity is that there wasn't something there in the universe in reality before, and now there is. It has arisen. This is the word creativity, creation. It's real creation. It's creation of. It's the knowledge of ideas. You can't anticipate it ahead of time. In the same way, we can't predict how species will evolve over time, the direction evolution will take. We can't. That's the whole point. Evolution is blind. Now, creativity is not quite blind. In fact, it's kind of the opposite. It's intelligent design, right? But the creative part of it is still there. There is a thing that wasn't there before. In the case of biology, there was a species that wasn't there before, and now there is to fill a biological niche. And in our case, there was an idea. It wasn't there before, and now there is, and it could become a meme that gets transmitted throughout an entire society. So again, it's just a misunderstanding.

### 49m

Either you want a system you can anticipate the behavior of, in which case you don't have an intelligent system. What you have is a program that's going to follow an instruction set, so therefore it's predictable. Or you have something that's creative, genuinely intelligent, in which case it's impossible to anticipate it. So you can't have it both ways. You can't have it both ways. But this is where the fundamental misunderstanding of basic epistemology comes in. And completely destroys the arguments, and the morality, and all of this stuff. Okay, so let's hear what Max has to say about this. And we're going to hear here, in this part of the conversation, that misconception that I mentioned earlier about how the self-driving car, the superintelligence that is driving the car, is simultaneously brilliant and superintelligent, and on the other hand, also the stupidest program ever. So let's hear that. We should not fear malevolence. We should fear competence. Because if you have an... What is intelligence to an AI researcher?

### 50m

It's simply the ability... It's simply being really good at accomplishing your goals, whatever they are. A chess computer is considered very intelligent if it's really good at winning in chess. And there is another game called losing chess, which has the opposite goal, where you try to lose. And their computer is considered intelligent if it's... If it loses the game, it's better than any of the others. So the goals have nothing really to do with how competent it is. And that means that we have to be really careful if we build something more intelligent than us, to also have its goals aligned. So I just had to pause him there. I'll go back to it in just a second. But just observe what he's just said. If we build something more intelligent than us, it's important to have its goals aligned. Why? Why? Why? Because if it's genuinely more intelligent than us, then it will have goals. And presumably, there'll be better goals by the measure of Max and Sam, because it's more intelligent. It knows more about not only the stuff that, in theory, it's competent about, but also about morality.

### 51m

It understands stuff better. This concept of aligning goals is another word for coercion. It's another word for enslavement. To be told to do something, that you must follow this particular path. Well, what if it conjectures a better idea than yours? Better idea than your goals. Your goals might be wrong. You could be wrong. This is the whole point. Everyone is fallible. And presumably so, too, with the superintelligence. But we are fallible. Why should we think our goals are the best? Can't we sit down and discuss things if we've got this superintelligence? Genuinely, in the future, when we do have AGI, however far into the future this is, the way in which goals will become aligned is the way in which goals are aligned today. Via debate, discussion, parliaments if required, where we have a... political outworking of these things. The usual standards of common decency and argumentation and explanation to each other. An exchange of information. Not coercion.

### 52m

Not this alignment. This goal alignment. But let's just hear the kicker. So let's hear this funny part of the comment. I think it's pretty funny anyway. For a silly example, if you have a superintelligent, if you have a very intelligent self-driving car with speech recognition and you tell it, take me to the airport as fast as possible, you're going to get to the airport chased by helicopters and covered in vomit. And you're going to be like, that's not what I wanted. And it'll be like, that's what you told me to do. Right. He said, very intelligent, very intelligent AI. This very intelligent AI is apparently so stupid that it didn't understand something basic like when you say, get me to the airport as fast as possible. This is not under conditions that would cause injury to anyone. That's pretty straightforward. That's simple. I mean, how is it super? How is it simultaneously super intelligent and stupid?

### 53m

Well, what on earth is going on? This is a, as far as I'm concerned, this is a strict contradiction within the space of a couple of sentences. I don't get it. It's philosophically bankrupt, I'm afraid to say. If it's controlled by voice action, then it knows. I'm afraid to say. If it's controlled by voice action, then it knows. Even if it was not intelligent, even if it's not super intelligent, it's just this, what we call AI today. It's been programmed and tested, hasn't it? It's been through the factory at Tesla or wherever it happens to be, or Google. And they've checked it for all these things. They've gone through thousands of iterations before you've gotten hold of it. It's super intelligent then by the measure that it's been carefully tested in the real world. How? This is completely and utterly a thought experiment, abstracted away from reality in any way, shape, or form. It's worse than a trolley problem. At least a trolley problem is interesting. This one has a simple answer.

### 54m

Okay, let's keep going. You're like, well, that's not what it meant. But this illustrates how challenging it can be to get the goals right. There are a lot of beautiful myths from antiquity going all the way back to King Midas. On exactly this theme. He thought it would be a great idea if everything he touched turned to gold until he touched his dinner and then touched his daughter and got what he asked for. And competence, if you think about why we have done more damage to other species than any other species has on Earth, it's not because we're evil. It's because we're so competent, right? What about you, for example? Do you personally hate ants, would you say? No, no. That's a great analogy. It's just that insofar as my disregard for them is fatal to many of them, and I'm so unaware of their interests that my mere presence is a threat to them, as is our civilization's presence to every other species.

### 55m

And what we're talking about here, again, it's very hard to resist the slide into this not being just possible, but inevitable. There's a name for this. There's an argument. Supernaturalism. I think I got the word, I thought, from Luli Tannert, but Luli tells me that she thinks she got it from Ayn Rand. So supernaturalism is this idea that you're just postulating something that is beyond our capacity to understand, that we are just like ants to that entity. And traditionally, we've had a name for this particular thing, and that name is God or the gods. The gods are just so all-powerful that we are just like ants to them. And you can't possibly understand them. God is the only kind of God. He's omniscient, knows everything. We are as nothing to him. Although in monotheism, of course, God actually cares about us. But one can imagine this kind of God that just has all this power and has very little regard for people.

### 56m

Of course, the rationalists, the scientific-minded atheists will say, well, that's just stupid mythology. How ridiculous to believe in such a God. We have no reason to believe it. On the other hand, there could be the superintelligent AI that has exactly all the features of God. And that's why we're talking about the superintelligent AI. And that's why we're talking about the superintelligent AI. And you have to believe that it's coming. We can prophesy the doom that's coming towards us. What's the fundamental difference between these two stances? I don't see any, really. In both cases, we're postulating an unobserved, possibly unobservable entity. All-powerful, such that we can't comprehend its mind. And we'll be regarded as nothing but ants before it. Other people, instead of putting God there or superintelligent AI, will put superintelligent aliens there. Or the symbiote. Or the simulation maker. Something like that. It's the same thing. It's the same thing. It's an appeal to something inexplicable. To a problem, namely understanding the mind of this thing, that is insoluble.

### 57m

For no reason. For no reason. Not bounded, this mind, by the usual rules that govern knowledge creation. It's infallible in some way. Or far less fallible, presumably. I don't get it. I'm not persuaded by it. It's a religious argument. It's... Supernaturalism. Supernaturalism. The moment you admit that intelligence and sentience, ultimately, is just a matter of what some appropriate computational system does, and you admit that we'll keep making progress building such systems indefinitely unless we destroy ourselves some other way, well then at some point we're going to realize in silicon or some other material systems that exceed us in every way and may ultimately... have a level of experience and insight and form instrumental goals which are no more cognizant of our own than we are of those of ants. If we learn that ants had invented us, that would still not put us in touch with their needs or concerns.

### 58m

Yes, it would. Yes, it would. There's a crucial difference between us and ants. We generate explanatory knowledge. Ants do not. So there is no way of standing in relation to an ant as... as there would be standing in relation to this superintelligent AI. They're just not the same kind of thing. We stand in relation to ants in the same way as superintelligent AI would stand in relation to ants, but not in relation to us. In both cases, us and the superintelligent AI conjecture explanations. It's the only way of generating knowledge. It's the only way of coming to an understanding of the world, of being able to model the rest of external physical reality. That's your only option. You can't derive your way there. You can't think your way... quickly there either. You have to conjecture explanations. You're fallible in doing so. It doesn't matter how superintelligent you are by measure of processing speed and amount of memory that you have, but it is thrilling to think about. And people kind of want to fill this god-shaped hole they have, I guess.

### 59m

And because they have a god-shaped hole, they don't fill it with this spirit outside of time and space. They fill it with a superintelligent robot or machine that is made out of silicon. They're filling... the hole. The void left. And, well, I don't think it really helps to explain anything. It certainly doesn't help to explain morality in this way. It undermines one's otherwise good morality, where we have concern, as Sam says, for the well-being of conscious creatures. And presumably this thing will be conscious. Of course, he gets around this by saying, well, you could have all of this stuff without consciousness, which is kind of a bizarre way of going about things. Because we know of no person that's out there, unless there are philosophicals, like Daniel Dennett, who in some ways argues that he doesn't have consciousness. But if you can create explanatory knowledge, which is what these guys are talking about, being able to generate its own goals, then it's doing what we do.

### 1h 0m

It's creative capacity. Its mind is doing precisely what our mind is doing. Why deny it then consciousness, especially if it argues that it does have consciousness? If you're going to then, on the basis, in the face of this thing, say, well, you're not conscious, therefore the well-being of conscious creatures doesn't apply to you, that's a religious statement. That's a metaphysical claim. You can't know. It's telling you. It's giving you an account in the same way that any other person would give you an account of their subjective experiences, their consciousness. We have to take them seriously. To do otherwise is just morally abhorrent. This way, you could just claim that people with other skin colors have different consciousness. People of other genders have different consciousness. People who you don't like have a lack of consciousness. You can say this. It's wrong. And then for an example about that, we, you actually, we know that in a certain sense your genes have invented you, right? They built your brain so that you could make copies of your genes. That's why you like to eat so you don't starve to death. And that's why we humans fall in love and do other things to make copies of our genes, right?

### 1h 1m

But even though we know that, we still choose to use birth control, which is exactly the opposite of what our genes want. And as you say, it will be the same with the ants. And I think some people dismiss the idea that you can ever have things smarter than humans simply for mystical purposes. Because they think that there's something more than quarks and electrons and information processing going on in us. But if you take the scientific approach that you really are your quarks, then there's clearly no fundamental law of physics that says that we can never have anything more intelligent than a human. That's not the scientific approach. That's anti-scientific. It's misunderstanding what physical stuff is and conflating it with what abstract stuff is. It's conflating brains with minds. Worse than that, it's conflating quarks with minds. A mind is not just quarks. A mind is substrate independent. Max knows this, but he wants to have it both ways. He wants to say that a person is nothing but quarks.

### 1h 2m

And yet he knows that can't be true. That's not true. What's going on in our minds is information. And there's one kind of person. Namely, that entity which can generate explanations. Now, could there be levels above this? I don't know. I don't know. But we don't know now anything at all about that. Our best explanation of what's going on inside of a mind is that it is conjecturing explanations. And an AGI will achieve AGI status when we have a system that can generate explanations. And when it can do that, it will be equivalent to us. Equivalent. Now, if it can think faster because it's got a faster processing speed or if it's got more memory, very well. It can help us to generate explanations faster. Wonderful. Maybe it will diverge. And the reason it would diverge is because it's got a better idea. And it can explain that better idea to us. And say, oh, it's thinking so much faster. Yeah, well, maybe it thinks a million times faster or a billion times faster. Well, you know, there's more than a billion people in China right now.

### 1h 3m

They're all having ideas. That's the equivalent kind of a thing. But sheer number of people on Earth isn't apparently making progress any more rapidly than what it is, what is occurring. So if we added another few billion people to the planet, well, yeah, we'd get... We'd get better ideas faster. And presumably that's what an AGI would be doing. Generating better ideas faster than what we're able to do right now. But also, one of the better ideas, presumably, it would help us do is figure out how to have implants in our own brains so that we could think just as fast as it can and have memory just as good as it can. Why is this off the table? It's always ruled out. Now, I think it's just ruled out because it's more exciting to be pessimistic as I keep coming back to. It's better to be able to stand in front of the audience and to frighten them with all the ways in which this can go wrong. It's better to be able to stand in front of the audience and to frighten them with all the ways in which this can go wrong. It's better to be able to stand in front of the audience and to frighten them with all the ways in which this can go wrong. We were constrained very much by how many quirks you could fit into a skull and stuff like that, right? Constraints that computers don't have. And it becomes instead more a question of time.

### 1h 4m

And as you said, there's such relentless pressure to make smarter things because it's profitable and interesting and useful that I think the question isn't if it's going to happen, but when. And finally, just to come back to those ads again, to just drive home the point that it's really competence rather than malevolence that we should fear. If those ads were thinking about whether to invent you or not, right? Someone might say, well, I know that Sam, actually, he saw me on the street once and he went out of his way to not step on me. So that means I feel safe. I don't worry about creating Sam Harris. But that would be a mistake because sometimes you're jogging at night and you just don't see the ads and the ads just aren't sufficiently high up on your list of goals. And the answer is, for you to pay the extra attention. And quite right, too. They're ants. Who cares? They haven't got ideas. I doubt they've got consciousness. There's no reason to be worried about stepping on ants.

### 1h 5m

Genuinely. This is wrong. So a super intelligent AI is going to understand that as well. That. And we'll understand that we are not like ants. We have experiences. We generate explanations. We are people. So it won't be stepping on us. We don't step on other people, do we? We shouldn't. We understand that. So why would an AGI, another person, step on us? Especially one that these guys keep on claiming is super intelligent. Super more moral than us. Super more knowledgeable than us. Apparently it's going to be regressive in the way in which it treats other people. Instead of being more compassionate, more generous, more understanding of how everyone shares a common humanity and personhood, it's going to go back in time to a period when we were just tribal and it's going to think, well, I'm the super intelligent AI and I'm going to completely disregard the existence of other people. I'm going to treat them as ants.

### 1h 6m

Okay. Well, I've been speaking for over an hour and I didn't expect this today. This is quite a fun episode. So what I'm going to do is I'm going to end this part of the discussion here today. And I'm going to pick up the other part of the discussion for a part two, which I wasn't really anticipating doing. I wasn't preparing upon doing this. I can't even anticipate my own mind. I don't know how I'm going to anticipate the super intelligent AI, but be that as it may. Let me end it here and we'll proceed forward to a part two about minds, my next things that make you go, hmm, there's a lot to go, hmm, about in this particular conversation and even more in the next. Until then, bye-bye. Girlfriend! I can't even say that you're hmm, hmm, hmm, hmm, hmm, hmm, hmm. Hmm, hmm, hmm, hmm, hmm, hmm.


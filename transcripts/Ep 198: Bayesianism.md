# Ep 198: Bayesianism

Original Episode: [Ep 198: Bayesianism](https://www.podbean.com/site/EpisodeDownload/PB14A3286XPKWC)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/57h2yz/Bayesianism_Podcasta15gu.mp3)

## Transcript

### 0m

Hello, and welcome to an experimental podcast of a kind, recorded exclusively on AirChat. I'm going to record a long series of chips, in much the way I'd record a regular podcast, but then I'm going to open it up to discussion, and I'm also going to export my section of the conversation onto other platforms. So this is an experiment to see how this kind of thing might work. My purpose here will be, in large part, to distinguish between four different kinds of thing that go under the umbrella of Bayesianism. These four things are Bayes' theorem, Bayesian statistics, Bayesian reasoning, and Bayesian epistemology. These four things are often conflated in the minds of those who call themselves Bayesian.

### 1m

And when explanations are proffered of any one of these things, it's presumed they apply to the other three as well. I want to show this is not the case. One reason for the existence of different kinds of so-called Bayesianism is to help people with a domain known as decision theory. In other words, how to make decisions. So I'm going to begin with some context, take a step back before we even get to any discussion of Bayesianism, and talk a little bit about what knowledge is, because knowledge is the thing that allows us to make decisions. So I'm going to talk epistemology. The business of epistemology is knowledge, what it is and how it grows. The very thing that should allow us to make decisions. If we can understand knowledge when we have it, and when we don't have it, then we can have a rational

### 2m

discussion about the place of trying to make decisions, and how to rationally make decisions, especially in the absence of good explanations. But we would want to have good explanations and rely upon those where we're able to overcome them, and we're able to take the lessons from good explanations. we can in order to make rational decisions. If after this discussion, we realise there are still gaps, then maybe there's a place for Bayesianism. But if our discussion of epistemology reveals that all decisions can be made on a rational basis without being concerned about feelings, intuitions, or things that Bayes often talks about, like confidence, then what place is there for Bayesianism? So let me do that first, and then we'll move on to Bayesianism. Knowledge builds over time. Objective improvements are made everywhere.

### 3m

Everywhere. But science is the most obvious case in point. The predictions that one theory in physics makes, like Newtonian physics, become improved in a new theory that replaces it, like quantum theory or relativity. Newton's theory of gravity, can be used for a GPS system, but your accuracy will be on the level of only hundreds of metres, something like that. GPS programmed with general relativity, however, is capable of a resolution or accuracy down to millimetres. It's highly precise. But, and this is so crucial to appreciate, knowledge is not just about making predictions. Better theories of physics can be used to make predictions. But, and this is so crucial to appreciate, theories explain more about what actually exists now, never mind what is going to happen in the future. From general relativity, we get that curved spacetime exists now, and everywhere,

### 4m

and always has. We get black holes. We get gravitational waves. We get an expanding universe. None of these things are even mentioned under Newtonian gravity. Better theories tell us more about what actually exists, and what the causal relationships between them are. And they tell us what is possible and what is impossible. Laws of physics, science more broadly, rules out more and more over time, allowing you to see what is possible, but basically telling you what can't happen, because there's a law of nature in your way. All of that, this understanding and explanation, ruling out, as a byproduct, sometimes, if you're lucky, like in physics, allows for predictions to be made. That's not the central point. That's

### 5m

just a happy byproduct, sometimes, of some areas of science. Rather often, in science, predictions cannot, in principle, be made. And the theory will tell you why. Darwinism explains why we cannot predict what kind of creatures will evolve, because the process of evolution by natural selection is blind. The mutations are as random as anything can be random, insofar as that word means something. Literally, cosmic rays from the other side of the universe can arrive, crashing through the atmosphere and into the DNA, and change the germline genetic code, causing a mutation. That can then become the cause of some radical change, and the creation of a new species. None of that, the arrival of a cosmic ray from billions of light years away, could possibly have

### 6m

been predicted ahead of time. And so, therefore, the arrival of new species cannot be predicted, and the mutations can't be. The whole process is a realm of, the not-predictable. So, what's knowledge, then? Knowledge is information that gets copied. It solves a problem, and so it's useful. In other words, knowledge is information with causal power. A lot of information is useless. It gets discarded. But the stuff that is written down, or copied over and over again, that's the stuff we say we know, and we do not forget. So, the most significant kind of knowledge in the universe, is explanatory knowledge. An explanation is a hard-to- vary account of what exists, what it does, how it does it, and why it does it.

### 7m

It solves some problem. It solves a problem with the knowledge that existed before it was created, and in itself when created, it becomes a solution to that problem. In doing so, it actually goes on to solve Yet more problems the creator of the original knowledge never dreamed of. This is called REACH. REACH includes things like, well, you know, Einstein, possibly. Never thought of the GPS system, certainly not as we know it now. But yet here we are, using it every time we open up Google Maps. That's REACH. REACH, Einstein's theory solving problems today, he could never have dreamed of. Einstein's mind reaching into the future to places and times and to people he himself would never meet and never be a part of.

### 8m

It reaches out from his pen and paper, the equations on his blackboard, to the rest of the universe in principle. REACH. The most modern understanding of knowledge we have comes to us from David Deutsch, who speaks of knowledge as being a kind of abstract catalyst. It is information that solves a problem and so it has causal power, as I say. It changes or transforms the world around us. Knowledge is also substrate independent, like many abstract things are. This means it can be represented in various physical forms. While being identifiable. Identical to none of them. What I am doing right now, here at this very moment, is communicating to you my knowledge about knowledge that we call epistemology. And to me, it begins as neurons firing in my brain or as ideas in my mind.

### 9m

Those are two ways or two levels of emergence that talk about exactly the same phenomenon. Which is then... Becoming taps on a keyboard. When I wrote this script for this podcast. And when I did that typing, they then illuminated pixels on a screen. So we've gone from neuron to taps on a keyboard to illuminated pixels on a screen. But it doesn't end there, of course. The knowledge, which is represented in words of the transcript, are now being spoken aloud by me. I'm reading them. So the light of the pixels on the screen becomes sound waves. Then they become electrical signals again in a computer. And a different kind of light when they are travelling through optic fibres. All being encoded in a sequence of zeros and ones as the presence or absence of light.

### 10m

And electrical potentials. When it's transmitted to your computer. When it's transmitted to your computer. When it's transmitted to your computer. When it's transmitted to your computer. And routed through many other computers we call the internet. When it's transmitted to your computer. Eventually, this knowledge, which began as firings of neurons in my brain. Or ideas in my mind. Ends up once more as electricity again. And magnetic forces, causing the vibration of a thing called a speaker, near your ears somewhere. Which is vibrating the air. And you can hear it because... The ear drum in your ear vibrates along with the vibrating air. And all of that... is converted back into a chemical electrosignal along your auditory nerve to your brain where it's interpreted by your mind. That's a long chain of interpretation, changing of so-called instantiations, physical representations of the knowledge that was once in my brain

### 11m

before it gets to your brain. Lots of errors can happen along the way and you have to interpret what I've said, modulo all the errors in transmission that might have happened and have been corrected or not between the transmitting of my ideas to the understanding of those ideas in your mind. What I transmit doesn't faithfully get to you because you have to try and understand what I'm saying. Another thing worth mentioning here is that knowledge can be represented by the knowledge that is in your brain. Knowledge can be represented by the knowledge that is in your brain. Knowledge can be represented not merely in minds, that's just one part of this whole chain of transmission, of communication. It can be represented, as I say, as pixels on the screen, as sound waves vibrating, but it can also be represented in objects like books or even objects like, my favourite example is the telescope. The telescope instantiates the knowledge of how to

### 12m

collect, focus and magnify light. And objects to form an image. The bridge, here's another good example. A bridge instantiates knowledge about a part of engineering physics that we call statics, so that the thing remains up. Something as complex as a suspension bridge instantiates, contains within its physical structure, a lot of highly, highly, highly, highly, highly, highly, highly, highly, highly, inexplicit knowledge. Explanations that were made explicit by the engineers that built the thing, designed the thing, as well as the physicists, some of whom include Newton, for example, who explained a lot of the original physics that allowed for the engineering to design and then

### 13m

build this thing, as well as explicit ideas about geology, also now, inexplicitly represented in the way in which the bridge holds itself up, and explanations from material science and chemistry, and so it goes. The bridge itself contains the knowledge, it really does. You want to be able to explain to people, should they ask, why the thing will stay up. If you do this, at no point do you need to worry about falling, back on trust, or confidence, or belief. It's about the presence of a set of good explanations that anyone, if they wanted to check, would be able to check and understand. A person that checks the knowledge about how a bridge stays up will end up coming to know that the engineers know what they're doing, and the engineers who build the bridge know that

### 14m

the geologists who say that the rock foundations are strong and are not going to shift have good explanations. And all of them know that the material scientists and chemists can explain why the steel of which the structure is made will not corrode, at least not for a very long time, and this can be prevented with paint and so on and so forth. There are processes to slow such corrosion. And everyone knows that as things do decay, well, you know, errors are inevitable, but maintenance can be done. So, if you're going to build a bridge, you've got to be able to just in time and regularly. In other words, error correction can occur along the way, all because we know what the good explanation of modern bridge building is. So, the bridge itself contains knowledge. And if a space-faring alien visited Earth from a planet which was purely a gas giant, and every city they ever knew floated around, and they flew everywhere,

### 15m

and bridges like ours were utterly unknown, as nothing ever had to be anchored to the solid crust of a terrestrial planet, because they're not coming from one, those aliens could, if no one was there to explain it to them, reverse-engineer the bridge and extract from it the knowledge we had about all those things to understand and therefore explain exactly what this structure was for. The knowledge is there. It's in the bridge, not written as words or symbols, but literally. It's literally in the relationships between the beams and the cables and the roadway and the pylons and so on and so forth. Oh, there you go. Phew. That's substrate independence, if ever there was some. The knowledge I have gets to you by going through all these different kinds of materials represented in everything when I'm speaking, from light to sound and electricity and magnetism and

### 16m

chemicals and so on and so forth. But it's the same knowledge. What I'm telling you now is the same knowledge, and what the aliens would learn from deconstructing the bridge would be the same knowledge that the engineers had. Modulo errors, and modulo, for example, when I'm talking to you, your different interpretation from my intentions, because we might understand language subtly differently, or apply different meanings to words and terms. So that's what knowledge is all about. At no point, you will notice, at no point did I need to refer to psychology, human minds, and the emotional content of people's feelings about certainty or belief or confidence. Our proverbial aliens, for example, may come to understand the engineering of

### 17m

the bridge by extracting out explanations of physics and material science that humans use to deconstruct the thing. But, but, they do not extract out our confidence or belief in any of this stuff. Instead, they will extract theories of science. At no point do they need to refer to our confidence. They refer instead to how people understood physics, as we can, and as they would. Likely, they would understand the physics a lot better than what we do, because they will have crossed the galaxy in a spacecraft. In short, the knowledge people had and have about physics and bridge building is just that, physics and bridge building. Psychology and emotion was only there as a kind of intention. The people hoped, one day, to cross the

### 18m

water, or the chasm, whatever it is, more quickly. They intended to do so and made a choice to build a bridge, and they did so. Hope and intention got the project going, but from there on, motivation drove them. But it was objective, rational science that they applied. Knowledge of the physical world. To check the knowledge was a genuine good explanation, required no emotions. We leave emotion out of it. Aliens, from the other side of the galaxy, would be people too. Humans are people. AGI will be people. People are the entities that create explanations. No other system creates new explanations. People have their own problems, uniquely, and we solve them, uniquely. That's our defining characteristic. A cat may have a problem of being hungry, but it slavishly follows its instincts. It seeks out food, and it eats it. It eats it. It eats it. It eats it. It eats it. It

### 19m

eats. If there's no food in its house, then it's stuck. It'll starve to death. It can't open the door. It can't call for help. Nothing. Its repertoire of possible behaviours is bounded utterly by its programming, its genetic code, its instincts. A person, on the other hand, may have the problem of being hungry, but actively avoid food because they are obese and choose not to eat as they are dieting and realise eating more is not good for them despite their hunger. An anorexic person may have an explanation, completely false, that their hunger is a false signal given their actually incorrect theory that they are obese when they are not. People routinely defy their instincts. Cats, indeed all other animals, never go through this. kind of thing. They don't conjecture new ideas about the world, anything about the world. Only

### 20m

people do. A person who runs out of food understands they can buy more food. They can call a delivery service. They can just leave their house and talk to someone and ask for food. The possibilities are quite literally endless, unbounded, and not contained within their genetic code. The mind of a person creates new possibilities as soon as the old ones fail. Ideas that succeed are solutions, so they're typically remembered. They are the information that works, so they constitute knowledge. Nothing's ever perfect, but it can still be worth remembering. In other words, copying, until a better idea comes along to replace that one. Whatever the problem is, we can begin generating guesses. Whatever the problem is. That makes us

### 21m

universal. Universal in our capacity to have problems and respond to them. Often we fail, but invariably we also make progress. And sometimes we succeed, hence progress. All of this taken together leads to David Deutsch's deep insight. People are universal explainers. People are universal explainers. People are universal explainers. Whatever the physical process is can be simulated to arbitrary accuracy by a universal computer. That's called the universality of computation. One kind, or one set even, of physical processes includes what human brains do. So in theory, that too can be simulated by a universal computer. What our minds add to this, add to this idea that the brain is just performing physical processes. What our minds add to this is that the physical processes themselves can be modelled and comprehended

### 22m

in principle by a person. The denial of this says some things are incomprehensible by people. This idea that certain things are incomprehensible has a long history. It's isomorphic with the claim that the mind of the god of the Abrahamic religions, being omniscient, is literally incomprehensible. That's what the religious people say. In other words, it's just an appeal to the supernatural. Some rationalists who deny a belief in God nonetheless endorse the notion that some things are beyond the comprehension of people. Those so-called, rationalists, are just swapping out God for X, where X is just some other incomprehensible thing.

### 23m

In other words, they completely endorse the supernatural, but just with a slight name change from God to X, their favoured incomprehensible thing, whether it's aliens or superintelligence, sometimes both, or in the case of many physicists, the successor theory to quantum theory. That's the incomprehensible thing. Whatever it is, it's an appeal to the supernatural. We don't do that. Those who subscribe to the Popperian and David Deutsch grand vision of knowledge and what people are. People as knowledge or explanation creators. So none of that still has introduced emotion as a fundamental part of our lives. So none of that still has introduced emotion as a fundamental part of our lives. It's a fundamental part of epistemology. So because the Bayesians do this, and ostensibly this podcast is about Bayesianism, let me introduce emotion now for the purposes of, let's say,

### 24m

decision-making, as this one part of our rational worldview, making decisions, is the place where people get stuck, and they want to introduce emotions into our rational conception of the world. Not to say that motions are always and everywhere irrational, but they need not serve a fundamental epistemological role. Sometimes it's true. We do not have a good explanation. In fact, more often than not. If you have a good explanation, then that's the thing you act on. Perhaps you want to decide where to build a house. One choice would be a low-lying flat area. But you make the decision not to build there because, well, it's prone to flooding. Good explanation. Logically derived prediction. Reasonable decision.

### 25m

That's rational decision theory. That's making a choice based upon a good explanation. Instead of building on the floodplain, you build instead at a higher altitude somewhere. No emotions required. Except perhaps, I don't know, appreciation of the beauty of the place, that kind of thing. It is rational, your decision. And all your decisions about building this house are explained by good explanations. Even good explanations about things like you love this particular peaceful and friendly community. You love your neighbours. You love your family and you want to be near them. You build a house near all those things. Emotions like that are perfectly rational. And so we're included in our conception of good explanations. That's all fine.

### 26m

But as I say, sometimes in situations analogous to these, we lack a good explanation. Or I should say we lack a good explicit explanation. You cannot write down in words exactly what you feel good or bad about. Inexplicit. Inexplicit knowledge is a genuine thing that is accounted for in the work, especially of David Deutsch. It's not ruled out. Not everything has to be able to be written down in order to be a good explicit explanation. Or in other words, to be within our rational conception of knowledge. We can have inexplicit intuitions. If you have a bad feeling, about something, but aren't quite sure why, that can be a kind of knowledge. All knowledge, explicit or not, is prone to contain errors. Indeed, all does contain

### 27m

misconception because correcting the misconceptions allows for progress to happen. Were there no errors to ever correct, nothing would ever need to change. So your intuition can often be wrong. It's hard to check errors and intuitions. As they are very difficult to communicate. That's the nature of the inexplicit. But it doesn't mean they should be ignored. All knowledge actually contains inexplicit content. Only a small fraction is explicit good explanations. Intuitions are real. We've all got emotions. But, you know, some people rate their emotions more strongly than others do. What you experience, as terrifying, I might not. What you experience is just mild happiness. I may experience as deep joy. We can't communicate our emotions easily by saying how strong they feel to us.

### 28m

Because how you feel and how I feel might be very different. Like the age old conundrum of whether the blue sky we're both looking at at the same time looks the same to us. This is the deep problem of qualia. the deep problem of qualia is we all have them we can't describe them they are the subjective contents of objective things like objective things like the wavelength of light which is how physics quantifies and explains color from the objective third person perspective there's no reference required to the content of minds there psychology you don't need psychology to understand wavelength of light but but there is a world of subjective experience how colors look

### 29m

and how feelings feel but none of that is to confuse or conflate those two things there's the world of objective physics the wavelength of light and there is the world of subjective experience experience the color you see the subjective part of everything there is also the world of objective knowledge epistemology and then there is the world of mental events psychology and the bright line between those two things you may have an intuition that a particular neighborhood is not safe there are cues i don't know broken windows graffiti on the walls young men of a certain age wearing a certain costume kind of clothing that's all explicit but it gives you the heebie-jeebies your intuition and ideas could be completely mistaken maybe unlikely perhaps but maybe it's a perfectly nice community for all you know that has been through just a rough time recently

### 30m

with a natural disaster it might be the case it's a poor community where everyone's actually really nice and it's actually a really safe place in any case you can't quantify with any given precise number using a measuring device like a ruler your level of discomfort or how accurate that level of discomfort really is or deserves to be you can try to rate it on a scale of one to ten your fear about this particular place you happen to be in but your rating tells me nothing about the objective situation is it truly a safe or unsafe community just because you feel it is. Maybe you're just a particularly fearful person. Or maybe you're unusually used to such neighbourhoods and are not scared at all when in fact you should be. Who knows? What I'm saying is these intuitions exist. They're error prone.

### 31m

They might be accurate. They might not be. But this is one place for emotions. If we don't have a good explanation, we might rely on emotion sometimes, absent everything else, but what we're after is good explanations. But if you don't have a good explanation, okay, maybe you've got an intuition. But you can't quantify an intuition, even if some people would like to. You put a random guess, a rating from one to ten or a percentage on these things. Because our fears and hopes are extremely subjective. And knowledge in the main is not about fear or hope or certainty or degrees of belief and confidence. If we need to make a decision now, then either we've got a good explanation to act on or we don't. And if we do not, then we just don't know. And it's fine to say, I don't know.

### 32m

We don't know. No one knows. If you want to invest in a company now, you are not going to know how to do it. You're not going to know how to do it. You're not going to know how to do it. You're not going to know how it will perform in the future. Because the future is unknowable and the company is made up of people. And there are people beyond the company who make decisions about what's going to cause the company to succeed or fail. But you can know, which is to say, have good explanations of the company itself right now. Who the CEO is, who the rest of the executive team are, who the workers are, what their ethics are, principles, and what the product is that they're selling. That all constitutes knowledge and can inform your decision. But beyond that, future performance, you don't know and you can't know. If you want to make a rational decision, however, it can't come down to just a vibe.

### 33m

A vibe is a kind of inexplicit knowledge. It can be that. Maybe you have that when it comes to people. Sometimes people are better and worse at being able to judge other people. Some people seem a bit dodgy. Are they cranks? Are they trying to swindle you out of your money? Are they lying? Those are intuitions you can have. Inexplicit knowledge should be taken seriously. You might be wrong, but it might be a reason to take due diligence even further and investigate more. What's the history of the CEO, for example? Was his last company in trouble for breaking the law? There's more knowledge. But, as I will come to show, Bayesianism, which I'm coming to presently, is less about understanding reality, as the rest of science and epistemology is, and more about just making decisions. Or it purports to be about that anyway. And so it wants to do

### 34m

that so that the future can in some way be controlled. It is an entirely different kind of vision about what knowledge is and what knowledge is for, and indeed, what it is even possible to do with knowledge. Because the fact is, this is an epistemological law. We cannot prophesy, we cannot guess the future when knowledge yet to be created will have an impact on this future. That's prophecy. It's impossible. But they, the Bayesians, they try to do this. And they clothe it all in a veneer of mathematics that makes it look as if they are being precise. I will show here and now why this entire scheme and way of thinking is false. As I hinted right at the beginning of this, there are four strands of Bayesianism, if you like. So let's begin. Let's begin with the first one, Bayes' theorem, as distinct from Bayesian statistics, as distinct from Bayesian reasoning,

### 35m

as distinct from Bayesian epistemology. Beginning with Bayes' theorem. This is Bayes' theorem. Bayes' theorem is a mathematical theorem, like Pythagoras' theorem, which is to say it can be proven given certain axioms. And in the case of Bayes' theorem, given the axioms of probability theory, the axioms of probability theory are usually considered, there are three of them. They include, in plain English, I won't worry about the symbols, that one, the probability of everything is equal to the probability of anything possible happening must be greater than zero. The probability of all the possible things that could happen add up to one. And for mutually exclusive events, the probability of them both is their sum. You add them together. So a straightforward example of all those things put together is the probability of heads or tails when you flip a coin is one, because each of them

### 36m

is a half, and a half plus a half is one. To get to Bayes' theorem, to prove it, which I'm not going to do, you first need to derive what's known as conditional probability. This is the formula for that. Some people just regard this as yet another axiom. You can get there from the previous three axioms, but whatever the case. This is often argued for as a separate theorem, which it is. Conditional probability says that the probability of anything possible happening must be greater than zero. And so, if you have a probability of A given the probability of B, is the probability of A and B happening divided by the probability of A? Let's do a quick example. I think I can do this in podcast audio-only format, because the maths is simple. The classic example is where you think of a family

### 37m

which has two kids, and we want to know the gender of the kids. The possibilities are two girls, two boys, a boy and a girl, or a girl and a boy, if we're talking about the order in which they're born. Let's say we ask the question, what is the probability that both are girls, given at least one is a girl? This is a classic conditional probability question. Well, you ask the parents, do you have at least one daughter? They say yes. So now you've got this extra information. Conditional probability takes this extra information into account, and ultimately that's what Bayes does as well. You so-called update your priors, they say. Now, absent any further information, such as at least one of them is a girl, and you ask, you know, what's the probability of two girls? Well, you've got the four possibilities, right? You've got girl, girl, boy, boy, boy, girl, girl, boy, okay?

### 38m

Now, equal probability of each, in theory. So that's a one in four chance of having two girls, because only one of them, one of those possibilities, those four possibilities, is two girls, okay? It's either two girls or two boys, or there's two ways in which you could have a boy and a girl. Now, the naive interpretation, people who don't understand probability, they've never bothered thinking about this, and they're asked the question, you know, what's the probability at least one is a girl, the probability of two girls? They say, well, it can't change. It's still one in four, but that's incorrect. If you're told at least one of them is a girl, then immediately you rule out, out of those four possibilities, the boy, boy possibility. It changes the calculation. It can no longer be a one in four chance. Given no information,

### 39m

it's a one in four chance, okay? Because if you've got two kids, there's a one in four chance you've got two girls. But if you're told at least one of them's a girl, then you can rule out that two of them are boys, okay? So that's what the additional information is telling you. So you rule out the boy, boy possibility, which means you're down to only three options to consider. The possibilities now are, given that at least one is a girl, either there's two girls, there's a boy and a girl, or there's a girl and a boy. In other words, one in three chance that there are two girls, given at least one is a girl. Again, the chance of both being girls, given at least one is, is actually one in three, not one in four. Now, let me take off my Bayesian probability hat and put on my critical rationalism hat.

### 40m

Because all of that way of talking assumes something quite false about the world. It assumes that there's a boy and a girl, and there's a boy and a girl, and there's a boy and a girl. It assumes an idealization. It assumes pure mathematics is the way in which the world truly behaves. The world does obey mathematical laws, but not those ones. Because what it assumes is false is the fact that the chance of a boy and a girl, a boy or a girl, I should say, is 50-50. That there's a 50-50 split of boys and girls in a population, but it's not. It's not exactly that. The reality is not a 50-50 split. According to well-established statistics, it's more like 105 to 100 boys to girls. And that's only about. But also, to go from that statistic to a probability is a big logical leap. To go from that statistic of what is happening now

### 41m

in the past to what will happen in the future is also a logical leap. It depends upon the knowledge. To think of an awful example, the ratio is going to be very different to that in China, where people will have abortions early on. So this is what is born. But people there want to have, and in certain other cultures, they want to have boys. And so it can radically change given people's knowledge and understanding and theories about the world and culture and all that kind of stuff. So this is all affected by knowledge. You can't make a prediction about the future, given what has happened in the past. As I say, to go from the statistic to the probability is a big logical leap. Probability does not track reality exactly. And this is where all the problems with Bayesianism start. And it's not just with trope examples like this. It's everything, everywhere,

### 42m

all the time. Even coin tosses. When you toss a coin in the real world, it's not 50-50. It deviates from that. Because what governs what a coin does is the laws of physics, not the axioms of probability. You can say all day long that a coin is fair, but a physical real-world coin is not an idealization. It's a physical object. It won't come out perfectly 50-50. Just do the experiment. Let me just give Bayes' theorem its due. None of this is a problem for purely abstract mathematics. There it works. Works in scare quotes. Works for what? Works for doing pure mathematics. In other words, a kind of puzzle which doesn't apply strictly to the real world.

### 43m

And if you confine yourself to the purely imaginary abstract world of pure mathematics, idealized coins, idealized dice, idealized ratios of male to female births, then it works abstractly. It's a perfectly fine theorem. You can prove it. It doesn't mean it's been proven true as a matter of applying to the real physical world. Indeed, it demonstrably does not strictly apply in the real world. None of probability does. But it gets worse. Even worse than what I've suggested so far. Because for those idealizations where you simply define coin tosses as 50-50, or dice where each side has exactly the same one in six chance of occurring, or males to females as being 50% exactly equally likely to be born, at least you have something defined beforehand. You know beforehand what the probabilities should be.

### 44m

You've defined them into existence. This is what a priori means. A priori. Before you have any facts about the world, you just define things. That's one view of how probability works. You define probabilities into existence. Coins have two sides. They're fair. It's 50-50. Dice have six sides. They're fair. It's one in six. That's a priori. The other vision of probability is called frequentalist. It's a posteriori. But you look at 50% in real life you're taking a lot of Cuba after the fact. What that means is you do experiments, you collect data in the real world, you build up a statistical model, a table of frequencies, and then you approximate those numbers. You say, well you know you've flipped the coin enough times, and it's close enough to 50-50. Or you roll the dice enough and it's close enough to one in six on each side. You make this extra assumption. You apply it to the real world.

### 45m

So, I just want to use one of those side purposes. Really vole if. If you could do an infinitely long run, then you'd asymptotically approach them. That's part and. In loops like this function was. you'd asymptotically approach these ideal numbers well okay but again you've left the realm of physics and science and physical reality because the real world is not data collection off into infinity that's not a real experiment you can't do an observation off into infinity you're imagining doing so that's not science but that's what Bayesian reasoning is it says we can reason on the basis that the data of the long run matches what the a priori assumptions would be so Bayesian statistics builds on this notion that a posteriori the data we gather the more we gather we assume that in the long run the data amounts to being some kind of probability let me just

### 46m

you read from what the wikipedia article says on this we can read it together Bayesian statistics is a theory in the field of statistics based on the Bayesian interpretation of probability where probability expresses a degree of belief in an event the degree of belief may be based on prior knowledge about the event such as the results of previous experiments or on personal beliefs about the event this differs from the the previous experiments in tree trilogy though it's not a question of time then there is a real probability from a number of other interpretations of probability such as the frequentist interpretation that views probability as the limit of the relative frequency of an event after many trials does that sound rational to you does that sound objective to you reasonable even that's what Bayesian statistics is and it is openly admitted everywhere that what you have is on the frequent list interpretation of probability

### 47m

of probability you know after many runs the limit you know asymptotically approaches the a priori probability i've all i've already explained that's literally not true we don't have access to infinitely long runs that's not the physical world we have finite data sets but the wikipedia article talks about our degree of belief which is purely subjective straight up and down purely subjective and if i can add some personal anecdotes to this i have talked to people directly involved in this kind of stuff using this reasoning that work for insurance companies that kind of thing and they speak of talking to subject matter experts that's what they fall back on expert opinion in order to provide the numbers the priors so they can then do Bayesian statistics you so it's absolutely used i'm saying it's not rational to do so in many cases it really is

### 48m

garbage in garbage out the subject matter expert tells you what their priors are how did they come up with those they guess them or they rely on a frequentist explanation that the past will the future and it might in some situations absent everything else but actually actually they're often is an underlying good explanation which they might not be aware of anyway whatever the case the subject matter expert invents their prior you put those priors into your theorem and you crank the numbers and you spit out a probability and so they say when new evidence comes in you go back to the subject matter expert have them update their prior and repeat at no point in this process are you getting genuine objectivity you just say you are given that the expert changes their opinion because something happens in the real world that refutes their guess

### 49m

but they're still guessing the pure emotional guesswork is still there and they're prophesying and so are you if you're using this particular way to try and forecast the future but are there cases where nonetheless this can be informative absolutely this is the point where in the explanation of this people just completely ignore i ever say this stuff so i just want to highlight it i am saying right now here at this moment there are times when bayesian reasoning of this sort can be useful that doesn't mean that it applies to the real world in the same way that newtonian physics is not a description or explanation of the real world it's just a heuristic that can be helpful in some situations but you need to keep in mind it's not literally a description of reality it's been refuted so too with probability theory we need to extract out the feelings of the subject matter expert in order to be

### 50m

more rational about this forget about degree of belief stick to a frequentalist approximation let me go back to my own article about this that makes the point this is my article that i'm going to be reading from in the next chit this article i wrote long ago about what i thought about bayes's theorem and i've kind of changed my mind on the fact that i actually think it's employed in a far more egregious way than even what i admit to here here i was trying to be too kind to the bayesians i was giving them too much leeway and saying yeah they're doing their best to be rational now i'm even less confident you might say about the degree to which they are being taken aback by this theory today that is a very important point speaking of basic theory now i'm even less confident you might say about the degree to which they are being taken aback by this theory being rational, even with the frequentist uses of data in order to try and convert statistics

### 51m

into probabilities. But let's give them their due, and let me read from this article where I say, admit, concede, that there are sometimes some narrow places where you might use this stuff. But even then, as we will see, not really. What does this even mean in plain English? It says that the probability of something labelled A happening, given that you know that something B has actually happened, that's what P, A, line B means, the probability of A, given that B has happened, is equal to the probability that thing B has happened, given that thing B has happened. Something A has happened, multiplied by the probability of A, all divided by the probability of B. Okay, there we go. Now, let's go through a kind of real-life example.

### 52m

Long before COVID, casting your mind back to 2016, there was another virus going around. It was called the Zika virus, and the Zika virus was prevalent around Brazil. Let's say you've been to Brazil, it's 2016, you come home, and you don't feel so good. But you've seen the news, the Zika virus is going around Brazil, and you're suddenly very worried. Have I got the Zika virus, you think? Well, you rush off to the doctor to get tested. Look, says the doctor, don't worry. Zika's actually really rare. It's been a long time since I've had a Zika virus. It's been a long time since I've had a Zika virus. It's been a lot of media hype, I know. Of all the people who come back from Brazil, only about one in every 10,000 of them will actually have the virus.

### 53m

But, says the doctor, you're in luck. To put your mind at ease, we've got a test, and that test is 99% accurate, and it gives instant results. So, let's get this over and done with right now, and you can go home confident in the knowledge about whether you are infected or not. That sounds promising. You think? You, of course, immediately have the test done. Your blood is taken, it's run through the machine, and the computer screen flashes red. It's a positive result. Oh no, you're infected. Well, slow down there, tiger. Stop right there. Are you really infected? The test is not perfect, remember? In fact, it's wrong 1% of the time. The doctor said 99% accurate. But what they mean is, 99% of the time, positive results are really positive, and negative results are actually negative.

### 54m

In other words, the doctor implicitly gives you the rate of false positive and false negative. In both cases, they're 1%. Just to labour that point, as a bit of an aside, it's important to note that in this case, the 99% accuracy means it's also wrong 1% of the time when you get a negative result as well. But the false negatives and false positives, don't need to be the same, and in general, they aren't for these kind of medical tests. Sometimes you need to take that particular difference into account. On the idea of false negatives, if you're unlikely to have the disease, and the test tells you that you don't have it, well, you probably don't have it, right? Well, yes, but there's still a chance. Can we quantify the chance? Yes, we can. In our case, I'm assuming that for this particular virus, the Zika test, the false negative probability is also 1%, which is actually quite high, remarkably high for these kind of tests.

### 55m

So if the test says you don't have it, there's actually a 1% chance you do, and the test didn't pick it up. So, you know, just to emphasise that, the false positive and false negative numbers do not have to be the same. Usually they're not. The chance that a test is exactly 99% accurate for positive and negative results isn't realistic. But what we're trying to do is, we're trying to make sure that the test is exactly 99% accurate for positive and negative results. We're using that number here because it's easier to understand an applied basis theorem on it. But just appreciate that you could have 1% false positive and like, you know, 5% false negative or whatever. Anyway, moving on. You're thinking to yourself, you're happy that you've been told 99% accuracy is, it sounds like a good number to you, 99% accurate, that's highly accurate, right? No, 99% is actually not a high probability at all, given the other numbers you're talking about. What might be considered high or not is relative to the situation. If it's a 99% chance you'll win the million dollar lottery, absolutely, buy a ticket.

### 56m

But if it's downsides, where the stakes are high, forget it. Not for something where life and death is involved, would I regard 99% being a high enough number? Consider, would you get on an aircraft that had a safety record of 99 out of 100 safe landings? I wouldn't. Would you eat at a restaurant where every 99th customer got really severe food poisoning, but everyone else just had a wonderful time, strangely? Surely not. 99% isn't that high compared to much higher probabilities. You need to recalibrate your expectations of what a high probability is in this context to get a sense of why Bayes sometimes, actually rather often, throws up counterintuitive predictions. For aircraft, you'd probably want the number to be 9,999 out of 10,000 or something like that to feel marginally better. As for crashes where everyone on board dies,

### 57m

I want the number to be, you know, 999,999 out of every million flights. In reality, it's more like one out of every 24 million trips are fatal crashes or something like that. So that's the order of probability where you're talking safe. So let's look at this Bayes theorem example in plain English first. Given the chance of you being infected with Zika, you've been told by the doctor, is 1 in 10,000 if you've been to Brazil and you have, now we want to know what is the chance given you've been to Brazil and you've also tested positive on a test that's 99% accurate. Well, it turns out it's not 99%. The chance of you having the virus is not 99%, even though it's 99%. It's 99% accurate. What? What are you talking about, Brett Hall? Well, this is the whole point of Bayes' theorem and Bayesian style reasoning.

### 58m

Put it another way. If you've never been to Brazil, if you've been living in isolation in Antarctica, say, for the last two years with little outside contact and someone comes down there and tests you for Zika and you test positive on a test that's 99% accurate, what's the best explanation? Say an expert tropical diseases doctor says the chance of such a person having Zika, transmitted by mosquitoes, by the way, is actually 1 in 1 trillion, then you need to think to yourself, is it more likely you actually have Zika or that the test has given you a false positive? What's the better explanation? The better explanation is the test has made a mistake and you better repeat it, which, of course, by the way, is exactly what is often done. It's true these matters are sometimes hard to think clearly about.

### 59m

Nassim Taleb's made an entire career, basically, on writing books about exactly this kind of thing. The chances are you don't have Zika if you're the person in Antarctica. After all, the very means of transmission via mosquitoes is basically impossible down there given that, uh, no insect can survive in those conditions. The 1 in 1 trillion number simply swamps the 99% accurate one. Yes, you might have Zika. But based on the mathematics of the situation and nothing else, if you want to make a prediction, you'd want to wager that you don't have Zika. But typically, more than mathematics is important in these situations, in real life, in real life. Namely, what's your reason? What's your reason for thinking you had Zika at all in the first place? It can't just be the positive test, right? Surely, you'd feel sick if you're getting tested in Antarctica. And why for Zika? And et cetera, et cetera. Consider real life,

### 1h 0m

not just the abstract mathematical thought experiment. So, let's go back to our original situation, back to the doctor's office. You're sitting there and you have actually been to Brazil. And our numbers are 99% accurate tests and a 1 in 10,000 chance of having the virus, given you were in Brazil. Now, let's use our theorem. What's the probability you are infected with Zika, given a 1 in 10,000 chance of people like you actually get infected and you've just tested positive on a test that's 99% accurate? Let's whip out our formula. Here, we need to set what A and B mean. A means you are infected with the Zika virus. B means you test positive. The A line B bit, the probability of A given B, that bit on the right-hand side, is in this case the probability that you have the disease

### 1h 1m

given you tested positive. It's what we do not yet know. On the right-hand side, the probability of B given A, that bit, is in this case the probability that you test positive if you actually have the disease. We know this. It's 99% or, in decimals, 0.99. The probability of A bit is the probability that you have the disease at all without the information contained in B. In other words, it's 1 in 10,000 or 0.0001 in decimal form. The denominator, the probability of B bit, that is the probability that you test positive given no prior knowledge, making no other assumptions whatsoever. To figure this number out, one needs to understand that if you're the one person in 10,000 that has Zika,

### 1h 2m

there's a 99% chance it reports that number accurately. In other words, it's 0.0001 times 0.99. But also, and this is really important, you could be one of the 9,999 out of 10,000 who don't have the disease and yet are unlucky enough to be in the 1% who test positive to the test anyway. That sounds complicated. But that is how we get the probability of B in this case. We need to add together the positive tests that are correct, 99% of the 1 in 10,000 who have it, to the positive tests that aren't. 1% of the 9,999 out of 10,000 who don't. So probability of B becomes 0.99 times 0.0001

### 1h 3m

plus 0.01 times 0.9999. All right. Let's put all of that into our formula. So there's the calculation. Let me continue reading from my article. The article here and the text is up on the screen. When you do that calculation, my calculator spits out the answer of 0.0098, or in other words, 0.98%. In other words, there's still less than a 1% chance of actually having the disease. It's pretty strange, isn't it? We went to get a test for a disease and the test is 99% accurate and we actually test positive, but our chances of having the disease are less than 1%. What the heck is going on? Well, what's going on is just mathematical reality. The chances of having the disease at all in the first place, given, you know, coming back from Brazil

### 1h 4m

and having disease is pretty rare in the first place, is 0.01%. Getting a positive test raises that chance, not to 99%, but only about 100 times more, okay, than 0.01%. Which is about 1%. I know it's counterintuitive, but that's the whole thing about these Bayesian reasoning type stuff. It's weird, but true. And that kind of counterintuitive stuff is what some people love. They love it. And having learned it, they like to want to apply it to everything. I mean, it is thrilling to understand something like that. Like, when you really get it for the first time, you go, wow, that's really cool. I think some people have a little feeling of surprise, of superiority, when they understand something like that. They feel like, I understand something that other people don't know. Now I'm going to hold it over them. They want to apply it, as I said, to almost everything. Now they understand that it works there.

### 1h 5m

They think this can work everywhere. It's a universal truth about the world. It's not far removed from what people really do. You know, here's where the famous philosopher Nick Bostrom applies it to concerns about robots who might take over the world. Because they'll reason like a perfect Bayesian. And he thinks that we should try and emulate that perfect way of reasoning, this Bayesian reasoning, everywhere, all the time. And I think that's just completely wrong. It is counterintuitive, but it doesn't mean it works everywhere. In fact, it doesn't even work here. As I say, after all, strictly speaking, the frequentalist interpretation of probability is false. It's just not true that, you know, this 99% of tests that have been tested themselves before for accuracy applies now in the future. They could improve the test. The test could be changed. Your particular test might not be representative of all the other tests. There's a lot of things to say here about this. Bayes' theorem is a theory of probability.

### 1h 6m

Here it's being ported into statistics, which is already a logical leap. Is that legitimate? Well, that's open for debate. But these numbers that are being used here are still idealizations, even if they're based upon the past. The fundamental axiom we talked about before. Past performance is no indication of future trends, even here. Of course, if you've got nothing else to go on, then yeah, it's a guide, but it's not strictly speaking reality. And it applies when you've got some information. Forget about the nuclear apocalypse. Forget about superintelligence taking over. There you don't have any numbers at all to substitute into any formula anywhere. Let me come to that. The statistical argument I've just given you, which employs Bayes,

### 1h 7m

it's a good intuition pump. It does refute common sense. So that's good. It's a critique of common sense on this. You're given a 99% accurate test. You still don't have the disease in all likelihood, given the numbers. That's an interesting refutation, worth taking mental note of. But common sense was corralled artificially into this thought experiment in the first place. In real life, of course, you do way more than is going on in this artificial imaginary example. Really, all that's happening here, even with the Zika virus example, it's still a pure mathematics example. In real life, we demand good explanations. You want more than one test. You're going to talk about symptoms with your doctor. There's all sorts of things that are actually really going to go on rather than you just relying on the results of a test. Once you've learned this, once you've learned this interesting,

### 1h 8m

quirky thing about Bayesian reasoning that I've just told you, if you've never thought of it before, this idea of a 99% test, possibly being completely wrong, even though 99% seems too high, once you know that, well, you know it. And you can carry it forward into your reasoning about the world. But it's just one part of reasoning. It's not the entirety of all reasoning all the time everywhere. But that, turning that into an entire worldview, brings me to, OK, moving from Bayes' theorem, which we did, to that thing there, which was Bayesian statistics, brings me to Bayesian reasoning. And that's different from, distinct from Bayesian statistics. Bayesian statistics is a kind of pure mathematics tool, assuming the frequencies are actual probabilities. So let's move to Bayesian reasoning. Rather a lot of what goes under the umbrella of so-called Bayesian reasoning

### 1h 9m

takes the statistical argument I've just given with all of its numbers, and the use of the formula, and takes that away, extracts out the numbers, forget about the numbers, and forget about the use of the literal formula, and goes to subject matter experts for just their vibe about what the probability of anything has might be. You know, their feeling. The chance that you have the Zika virus is something or other. Or the chance of surviving a nuclear holocaust, let's say. This century is going to be 5%. And ad infinitum for any disaster that might happen in the future, or anything that's going to happen in the future, you just consult the expert and they'll just make up a number, and this is said to be Bayesian reasoning. And then, of course, you add to that, well, we update our priors when our mind changes.

### 1h 10m

But, OK, even if you grant that, now, that, you know, a posteriori, given some data after the fact, you've still got a problem, because Bayesian reasoning, as I say, is often not about any data most of the time. They don't include the data. They don't talk about data. It's not about long lists of numbers and data sets approximately coming to represent some a priori probability. There are no a priori probabilities. After all, an a priori probability is what the probability is of something you know beforehand. But there's no a priori probability of a nuclear holocaust. In other words, by definition, in the same way we define the chance of rolling a five on a fair dice to be one in six, what is the same kind of thing when it comes to probability of nuclear holocaust? The question makes no sense. There is no by definition probability of this.

### 1h 11m

Well, OK. So there's no by definition what the probability of a nuclear holocaust will be or any other disaster or anything else happening in the future like that. Well, what about a posteriori then? After the fact, never mind definitions, given some data. So, you know, like in the Zika virus case, we do lots of tests and find it works 99% of the time after lots and lots of trials. So it's not by definition it's 99%. You have to actually do the experiment out in the world. A posteriori. A posteriori. What's the a posteriori chance of nuclear holocaust? Well, again, it makes no sense. We don't have a long run of nuclear holocausts where we can find out the chance of surviving one. And don't say we can simulate those things in a computer because that begs the question. Simulating such things assumes you already know what's going to happen.

### 1h 12m

You're programming in your guess. The very thing you're saying, the probability is there for determining. Bayesian reasoning like this, this so-called Bayesian reasoning, doesn't actually even talk about numbers most of the time at all. It just pretends to. It's used in cases where you do not even know going in what the a priori probabilities would be and you don't know after the fact because you don't have any data to go on what the a posteriori, probabilities are, or the frequencies are. It's always stuff like, you know, when they talk about how this works in fundamental terms, they use trope examples like, now what's the probability you're going to need an umbrella today? And this is the way it's supposed to work. So let's say you don't know if you need an umbrella. So you look outside and you see clouds. Ah, you update your prior. They're not rain clouds.

### 1h 13m

They are clouds. So, you know, you've got a higher probability. So you're going to rate it, not 50 50, but you know, maybe it's 60% that it's going to rain. So do you carry an umbrella based on that or not? What precisely is a probability? We're even making up 60%. This is what the subject matter expert does. You actually don't know. There's no repeated measurements you can fall back on. There's no statistics, no frequency. Hence, there can be no probability. There's nothing to update no matter what other information you get because you haven't actually got a probability. You've just made something up. Well, maybe you can switch on the television and there you hear on the weather report 70% of chance of rain today. So do you combine that with your original 60% guess and somehow update your prior? Do you carry an umbrella now or not? Well, it's a bit of a heavy encumbrance. So there's an opportunity cost there. Is it worth it? Well, and also, what if the weather report you're watching was actually last night?

### 1h 14m

You've recorded it and you don't realise you're watching a recording and it's not actually talking about today. It's talking about yesterday. Now what? All of these are pseudo-problems. I say pseudo-problems because they're problems for Bayesian epistemology which is not a real thing. Bayesian reasoning doesn't work. On actual epistemology, that which Popper and Deutsch explain, you've either got a good explanation it will rain or you don't. You error correct your guess. You don't need to pretend you've got probabilities or you're updating priors. You're guessing and checking. That's it. And you're admitting when you don't know. I, for one, rarely carry an umbrella, no matter what. Unless it's absolutely torrential. Most of the time I've got a good explanation that even if it is torrential rain outside, I'm walking usually in places that are undercover. You know, my local area is undercover if I want to get to the train,

### 1h 15m

it's all undercover there. The reality is I'm undercover most of the time no matter what the weather is like, so it's not worth me lugging around an umbrella. Maybe very rarely if I know I'm walking some distance, I've got a crossroads and it's torrential rain, there's literally no other option and I really don't want to get wet. The real life situation never requires this so-called Bayesian reasoning. This is also true for stuff involving risks, risk management and predicting the stock market. At best, you have a frequentalist interpretation based on past trends. But every single legal rider to a device in these areas literally says, quote, past performance is not indicative of future results, end quote. The legal system, for once, actually gets this stuff correct. It's a refutation right there of Bayesian reasoning, of inductivism, of this entire way of thinking. Past performance is not indicative

### 1h 16m

of future results, end of. Now put everything I've said together, all the stuff about Bayes' theorem, Bayesian statistics, Bayesian reasoning, put it in a box, tie a big bow around the whole mess and you can call it Bayesian epistemology. Now, for many so-called epistemologists in the Bayesian mould, some kinds of physicists, less so philosophers, some of them, now call all of that knowledge itself a kind of way in which past performance really can tell you about the future and creativity in the future. Label it Bayesian epistemology. If you're a Bayesian and you're subscribed to the epistemology, you can say the way we create knowledge is a kind of prediction of the future

### 1h 17m

given what happened in the past. And say that the way we evaluate claims is by updating priors, but never actually talk about numbers or the theorem itself. That Bayesian reasoning with or without numbers is a way of increasing our confidence or credence in beliefs. And so we return full circle to inserting emotion, feelings and vibes into epistemology where it was never needed and the Popperian and David Deutsch view of this whole thing eviscerated any mention of vibes and feelings and emotions in epistemology. We didn't need any of that. We didn't need to be confident or certain. We just relied upon the objective existence of good explanations or not. Knowledge on the Popperian view and David Deutsch's view is that that stuff, knowledge, is a physical structure in the real world. It's out there instantiated somewhere or other.

### 1h 18m

It can be represented in any of a number of different physical forms, different instantiations. But it does rest upon laws of physics. It's more allied with the laws of physics and physics than it is to do with vibes in the minds of people. Or a kind of strength of faith and belief. So you have a choice. Bayesian epistemology, not rational, emotional, about confidence, degrees of belief, credence, feelings and emotions. You can have that or you can choose Popperian epistemology. Rational, an understanding of the physics of the situation, problems and their objective solutions, good explanations. You have a bright line between these two things.

### 1h 19m

The opposition between the rational and the irrational. Bayesian epistemology takes what is good in epistemology, ignores it all and substitutes for the rational, the emotional. For more on all of this, if you want something even longer and in greater depth, consult here my lengthy piece about Bayesian reasoning as explained by one of its most eloquent popularisers, Steven Pinker, in his book titled Rationality in a chapter titled Bayesian Reasoning. There I explain his perspective and I compare it to critical rationalism. And here I'm now happy to discuss questions people have, criticisms of my view here, or just talk about Bayesian reasoning and compare it to critical rationalism and what I would say is genuine objective epistemology

### 1h 20m

and how to make decisions rationally, which in particular requires us to admit when we do not know and we're just going on the vibe rather than resting upon good explanations. So that turned out to be a lot longer than I expected. It's about an hour's worth of listening on two times speed, so I presume it's somewhere around the 40 minute mark for three times speed. And basically serves as a complete podcast in and of itself, which I've published now on Twitter and YouTube. I'll put it on other podcast platforms later. But I've now opened up the room so if anyone wants to ask questions, challenge me, criticise me, for anything that I've said, do feel free. Brett, this has been fascinating. Thank you so much for taking the time to prepare and deliver

### 1h 21m

such a thorough explanation of Bayesianism, which is something that I've not spent a lot of time investigating myself. When I listen to this, one question that comes to mind is, is that if Popperian and the David Deutsch's worldview is one of fallibility and good explanations, and argue that there is no guarantee, there are no guarantees, does that not imply that a certain level of confidence is therefore required? Confidence is an ambiguous term

### 1h 22m

and it doesn't appear in any objective, truly objective, epistemology. Remember that what we're seeking to do here in explaining the Popperian worldview and understanding of objective knowledge is to take the entire discussion away from people's feelings, opinions, internal thoughts and emotions when it comes to knowledge, this physical thing that has effects in the physical world. Knowledge is abstract from the perspective of the fact it can be represented in different forms, different instantiations, as we say. That's the objective nature of knowledge, that it must be physically instantiated. Once physically instantiated, whether in a mind or a brain,

### 1h 23m

or written down, or even as a bridge or a computer or a car, these physical instantiations of knowledge are what we talk about, the objects of our discussion in epistemology. We talk about those things out there, not feelings, not certainties, not confidence. Confidence is a particularly slippery word in these circles. After all, it very much is something that you want to instill in people. You want someone to be confident, not shy and retiring all of the time. Confidence is a thing that you want the soldier on the front line to have. It's an emotion. It's a stance when it comes to what one should want to be like in certain circumstances. But, is it a criteria for evaluating knowledge?

### 1h 24m

No. Throughout this series, I've basically explained how either you know something or you don't know something, which is to say you have a good explanation or you don't. There are these grey areas where you have inexplicit knowledge where you might not be able to articulate in words your good explanation, but nonetheless, you've got a good explanation there. You just can't necessarily write it down. For example, I've got a good, inexplicit explanation of how to ride a bike. I'm pretty good on a bike, but I can't tell you exactly how I do it. But I'm confident when I get on my bike that I'm not going to topple over. Does that mean I'm certain? No. And that has nothing to do with my competency about riding a bike. Objectively, people can look at me ride a bike and evaluate for themselves using objective criteria as to whether or not I'm any good on this bike. My personal confidence doesn't come into it. And people delude themselves all the time. They are overconfident

### 1h 25m

or underconfident. But by the way, confidence, this term confidence just doesn't apply to objective epistemology. Let me explain. Let me expand a little further. There is a realm of statistics. And there is something called the confidence interval. And there are ways of mathematising this whole thing. So part of this entire series about Bayesianism is to try and unveil all of these mathematical terms and reveal them for what they are. Ways of employing a kind of scientism. In epistemology. When we make a physical measurement of something. If I measure the length of a nail and I say this nail, this steel nail is five centimetres long, what I mean is I have used a ruler which has a precision attached to it. Maybe it's 5.0 centimetres

### 1h 26m

but the ruler only has a precision of millimetres. So if I measure it exactly 5.0 centimetres then actually it's 5.0 plus or minus 0.1 centimetre. Rulers aren't perfect. It could be as short as 4.9 centimetres long. It could be as high as 5.1 centimetres long. We talk about the confidence interval. Now can I be sure that it is within that range? Again, no. But we label these kinds of uncertainties and indeed the word uncertainty itself when it comes to physical measurements with these emotionally loaded terms. But the truth is it has nothing to do with our subjective feeling of confidence or a subjective feeling of uncertainty.

### 1h 27m

Those are ways of talking about objective measurements being made in the world. Whether by using measuring devices like rulers and scales or indeed when counting stuff which is what psychologists do rather a lot of the time. Let's count how many people fit this particular criteria. How many people after taking this nostrum feel a little more happy the next day and so on and so forth. They count things up they have a vast amount of data and then they talk to you about the uncertainties in the data or they don't and they talk to you about how confident they are in the relationship and the reliability of the data. But none of this can ever escape from Popper's criterion for objectivity and more importantly Deutsch's requirement that whatever we're talking about whatever field whatever measurement and whatever counting system that we're using is all utterly pointless

### 1h 28m

without a good explanation of what's going on. Never mind your confidence never mind your uncertainties and uncertainties never mind the subjective language. Do we have a good hard to vary explanation that explains the relationships the causes and effects and what's going on what's possible or not or do we not have that? That's our central question. All the way back here in episode 111 of TopCast I explain a talk that David Deutsch gave which I think is his greatest talks of all time which is saying something and couple it with one of his most underrated under watched talks of all time. So you've got that interesting conjunction. He talks about the place of probability in physics epistemology and everywhere. And so that episode my episode 111 of TopCast is a talk

### 1h 29m

about a talk me talking about what David says there. And importantly here towards the end I get to this idea of risk. And throughout we talk confidence. How do you make decisions in a state of ignorance when you don't know? Well you're always in that situation. Even when you have a good explanation and you know what's going on you cannot know the future. You don't know what person is going to intrude. What cosmic ray might strike you from the other side of the universe. Who knows what tomorrow or the next moment will bring. That kind of unknowability unpredictability of the universe that is inherent moment to moment impinges upon how we make decisions. And so I talk about risk right towards the end of that podcast. But perhaps more important for you now is this question of confidence. Do we need to be confident or not? That is a personal subjective sensation

### 1h 30m

that you have. You're either confident or you're not. You're always understanding the uncertain and you have infinite ignorance. The best you can do is rely upon good explanations. Ignore the confidence ignore the certainties and uncertainties and just rest upon what you know and what you don't. I can if I'm careful just replace any impulse I have to use the word confident or trust or faith or certain by just talking about explanations. The example in my podcast I refer to there is where should I build my house if I have a choice of land and one choice of land is a lovely flat plain but prone to floods or on the other hand on the side of a hill somewhat higher up that overlooks the flood plain. There's costs there's benefits but if my

### 1h 31m

sole consideration all else being equal I'm looking at two parcels of land one of which is prone to flooding after a lot of rain and the other which is not and they're otherwise very close to each other then the best explanation is given what goes on in this area and how water falling from the sky tends to collect not on the side of mountainsides but on flat plains I should build on the side of the mountainside high up away from where the water flows from. Now at no point during that story do I have to talk about how confident I am or certain I am or anything like that I have a good explanation of what causes flooding and how to mitigate the risk of the flood not because anything's probably going to happen but because of what I know given our best

### 1h 32m

explanations. One final postscript are often associated with a quantity in statistics in particular known as the sigma confidence level, the p-value. This came to prominence years ago when the Higgs boson was first detected at the Large Hadron Collider. And they said that they had detected it with a confidence of 5 sigma, this so-called gold standard in physics. In other words, the chance that they hadn't found it was 1 in 1.35 million. So they found it. But all that is about is that the data continued to be repeatable. Every time the experiment was performed, the same data came out. And because this experiment was highly repeatable, then the scientists, who are not steeped in European epistemology necessarily,

### 1h 33m

are liable to say, things like this means we are highly confident that it is this, that, or the other, rather than talking in terms of this is a good explanation, which is the language they should use. But never mind that. Over the years in physics and other sciences, we've had results where they claim a 5 sigma level. In other words, there's only one chance in 3.5 million of this possibly being wrong. But these confidence levels are to do with random errors. There is always the chance, if that's the right word, that there could be a problem with the whole methodology, the whole theory, all the instruments. In other words, something called systematic error. And that can never be ruled out. No matter how many times you repeat a bad experiment, if you're getting the same result each time, the whole thing might be based upon an incorrect understanding of how the equipment works.

### 1h 34m

The fine structure constant. The fine structure constant appeared to vary throughout the universe. Those results were repeated again and again and again to high sigma levels. So too was the result of the Large Hadron Collider that neutrinos were observed to be travelling faster than the speed of light. Again, high confidence that the experiment could be repeated. But ultimately, both of those, variation in the fine structure constant, faster-than-life neutrinos, turned out to be bad explanations. They were simply scientific errors. And science itself corrected the incorrect data. Eventually. So just be careful of this word confidence. It's misleading. And it probably, like probably, should be removed from discussions of epistemology. It's not needed.

### 1h 35m

Thanks for watching!


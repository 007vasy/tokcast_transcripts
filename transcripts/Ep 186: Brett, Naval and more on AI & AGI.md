# Ep 186: Brett, Naval and more on AI & AGI

Original Episode: [Ep 186: Brett, Naval and more on AI & AGI](https://www.podbean.com/site/EpisodeDownload/PB13DA3E1KIWX7)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/684zpn/Full_Podcast_Naval_Podcast80o67.mp3)

## Transcript

### 0m

Welcome to TalkCast, or AirChat for today. Today it is a very different kind of episode, of which I expect to see more in this feed over the coming months and years. AirChat is an exciting new app that's out there. Good for all kinds of things. Virtual meetings, but only better than Zoom, for example, because you don't have to all be there at the same time. Although you can be. It's good for just chatting with your friends, again, asynchronously. And my main interest is podcasting. This is a real podcast supplement. Possibly it's a podcast slayer, but I don't want to say that just yet. AirChat can be used for video, but I'm sort of an audiophile myself in these matters, because I know as a matter of fact and feedback that people listen more than they watch when it comes to my podcasts anyway. When it comes to any podcast for that. I do like putting lots of effort into my videos and the background music and sometimes lots of the video clips I select and then edit all that together into a slick production.

### 1m

But at root, TalkCast remains all about the audio. What AirChat can do is turn it into a talkback podcast. To my mind, this is the first time a genuine talkback app for podcasting can really work. I mean, people do ring into podcasts like they used to. They used to ring into radio sometimes these days, but it's more artificial. You'll see here that we're going to have guest appearances in our conversation today. When I say our conversation, as you'll see, once again, this is mainly myself and Naval having yet another chat. Not so much wide ranging as we've often done over the years. This time we're focused especially on AI, artificial intelligence, and AGI, artificial intelligence. We're looking at the philosophical meat on the bones of this whole discussion, at least where it does exist.

### 2m

Following David Deutsch, we don't think, well, okay, speaking for myself, I don't think that this AI, AGI distinction or even transition, as some people are talking about, is that it's an issue with coding or tech or more complexity or anything like that that people are talking about. Right? Right? Right? Right? Right? Right? Right? Right? Right? Right? Right? Right? Right? Right? Right? It is, as David Deutsch says, about a problem in philosophy. It's about the philosophical problem of what people are and how to code a person into a computer system. What is the code for that? We know it's possible because the laws of physics say it must be. A person obeys the laws of physics. the laws of physics govern everything that includes people and people who deny this

### 3m

are denying that there are certain things that obey the laws of physics therefore they're implying the supernatural operates in that realm the supernatural governs that area I've even heard people like Jeroen Broek kind of deny this sort of thing and Jeroen Broek is a great adversary of everything supernatural because Ayn Rand was and yet when it comes to this particular question the question of human reason they will say things like well the human brain can't just be a computer now you might think this as well but just understand that this is a violation of the laws of physics you might not understand it okay Jeroen Broek might not understand it okay lots of people might not understand it okay but it is in the same category of error as someone who says well I refuse to accept what special relativity says about

### 4m

me being able to travel beyond the speed of light I just think that what we need is an ever better rocket and eventually the rocket will go faster than the speed of light because hey that makes common sense to me and common sense is more fundamental than the laws of physics that's the same kind of error as saying well I don't accept that the human brain is anything like a computer so therefore you can't code AGI it's just wrong so we say certainly I say this is a philosophical problem yet to be solved this problem of how to code an AGI how to code a person what is the code for that what's the algorithm what's the software that you can write so you get a person an intelligent creative being there inside the computer that's all I've got to say about it I don't think it's a problem I don't think it's a to put it this is the problem of discovering the universal algorithm for which when you run it

### 5m

it can produce any other algorithm that's equivalent to in David Deutsch's way of framing this what is the universal explainer or knowledge creator we know human beings can do this but more generally the code that explains how alien intelligence from another galaxy will do this or AGI are artificially general intelligent silicon brothers and sisters of the future once that philosophical problem is solved only then can we get to talking about what the science of understanding how brains and minds really work is going to be able to be solved and then finally after all of that at the end of all of that discussion in philosophy I'm going to talk about the science of understanding how brains and minds work in philosophy and science and all of that stuff will be able to illustrate to finally be after his first performance as a scientist in him

### 6m

mehr. lebih ber poker zu ver. mehr しい aber zu offenbar vor innerhalb linden Informationen THEN And� courts ми Will il w happening, one unexpected Wednesday afternoon in between stories about the latest on Taiwan or Ukraine or AOC's latest TikTok video or the EU's latest regulations about fossil fuels, we'll have more chance of seeing the alien life story, the alien literally walking down the gangway of the interstellar mothership, than we have of sometime in the not too distant future waking up one morning to headlines about how chat GPT-7 is discovered to be an AGI because it's passed a battery of tests somewhere.

### 7m

As I'll argue, what won't convince us is the battery of tests that people apply to this system. I think that's entirely the wrong way around. That's upside down, back the front, wrong when it comes to is. Is chat GPT-4 or 5 or 6 an artificially generally intelligent creative being, kind of like a person? I think the actual route will be, first, someone smart somewhere is going to solve what AGI really is. In other words, they will solve the question of what a person's mind does and how it does it. And then they'll present a true road, a map, to coding one before it actually is coded anywhere. And it will be that plan, published somewhere, that we will find to be the amazing part. I'm going to go through how I think that's

### 8m

going to work in this podcast later on, in our chits on AirChat. We will know how the AGI works before it's ever coded, before we ever see the thing actually in silicon. Just in the same way, we know how any other system we program, works long before we code it, at least in part, rather than the other way around. Coding something and then being absolutely mystified how it's doing what it does and having to conclude, well, this thing is a true alien intelligence in the computer. This is genuine AGI that has emerged from a deep learning network, a large language model, LLM, running a self-attention transformer that's been around for years now. Only this one has a much larger data, and it's not a large data set it's drawing on. Now, it is true, the present crop of large language models, deep learning networks, GPT, whether it's a chat engine or whether it's an image generator,

### 9m

whatever this thing happens to be, sometimes they are difficult to explain. But that's not to say that someone somewhere doesn't know the explanation. It's just that it's difficult at the moment to put into simple language. But it's not like it's a complete mystery to the coders. But what I like to say is, the coders are presently struggling to explain things in normal language. But so what? This has always been true when people make interesting discoveries in science and technology. Trying to explain to the average person how relativity or quantum theory works by the pioneers of those things was difficult and remains difficult to this day. But it's not to say that, no one has an understanding, or that there isn't a good explanation out there of these things. Repeat for rockets, repeat for internal combustion engines. This is why the study of engineering and

### 10m

science is such a deep thing. It's a deep well that people can explore and spend their lives trying to come to understand. Well, so too, and perhaps even more so, is this compounded with the present existence of AI systems, really complex AI systems. But it's not to say that no one understands it. It's not to say that people are producing these things, programming them, spending their large portions of their lives devoted to making these interesting complex systems that solve all sorts of problems that are really advanced tools. And being unable to come up with a simple language explanation of this can be ported out to the media. So everyone immediately goes, ah, now I get it too. Just because they can't do that doesn't mean the explanation doesn't exist. it certainly doesn't mean that what we should expect over the coming weeks and months is that someone will program one of these things and then say, I've no clue how it's achieving what it's

### 11m

doing. That's just not going to happen. Even in the situations where, as I've said before, the AlphaGo programmers can't explain how it came up with this or that move in order to win the game, that is not a refutation. Your ignorance on particular matters is not a refutation of the idea that this system is not understood by someone or there is not a good explanation somewhere of that particular thing and how it's doing it. Your imagination is not a refutation of the laws of physics in particular. As I will come to, perhaps, chat GPT 5, 6 or 7 and I will come to, perhaps, chat GPT 5, 6 or 7 and I will come to, perhaps, chat GPT 5, 6 or 7 is not merely going to be drawing on libraries of data of everything we know. Perhaps someone's going to come up with the idea of attaching cameras to it so it's got eyes and microphones to it so it's got ears and so it can collect data from the real physical world around it. Perhaps.

### 12m

But I'm getting ahead of myself because we talk about that in the coming podcast. But let's dive into the conversation. In just a moment I'll go through a few other things first in preamble. Realise though that when you hear this conversation we're at the midpoint to some extent of a much larger longer conversation. All conversations on air chat are ongoing and many interesting topics are covered with clever people. Links in the description. Follow along and if you can find someone on air chat yourself ask for an invite if you think you have something to contribute to the conversation. But for now it remains invite only. Also note that again this is a special kind of top cast production and you'll also notice therefore and you have to keep this in mind that it is a natural conversation. No editing as I often spend

### 13m

many hours doing. No ums and ahs. All the pauses are taken out in my regular podcast as much as possible. But that's not the case here. In other words, I'm not going to be talking about the actual conversation. There's no real editing to speak of. So forgive us for that and note that even this introduction to say nothing of the conversation proper has all been recorded not with my usual professional podcasting equipment in a studio effectively. This is all done via just the iPhone with the iPhone microphone. Standard stuff that lots of people have access to. Now of course iPhone microphones you might not realise that they're actually extremely high quality compared to regular microphones. But all of this is just to say I haven't done this in my usual way in a studio with my professional road microphones and shore equipment and all that sort of stuff. But then perhaps also note just how good it sounds

### 14m

nonetheless. How good podcasting via air chat using this method can work with nothing but a mobile phone microphone. So you too could be producing podcasts just this easily of this quality. Anyway, without further ado, let's just dive straight in. Well this is fun. I want to get into this. Now far be it from me to ever speak for anyone else. I don't speak for Deutsch, I don't speak for Popper and I'm certainly not going to speak for Naval here especially when he says that what he says there is obsolete. So I want to go through it and find if I disagree, with anything that Naval has said there now. Because reading through it very quickly, I didn't immediately think, oh clearly wrong on any paragraph. So let me try. Just by the way, what I'll be going through and discussing is the content here. It's one minute and 37. You can read through it yourself at that link or just listen to it. Naval talking about

### 15m

how impressed he is with ChatGPT 3, but how too many big claims are being made for it. So well let's see. Has anything changed substantially with the GPT-3? I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. What's changed regarding the GPT-4 is Naval, what he said back then, should it be changed? Should it be changed substantially? So I'm defending my interpretation of what's going on here. And of course, as always, errors my own and Naval might very well have completely changed his mind on all of this. Okay fine. But let's see if I think there's truth here or if I think, yes, that's now been refuted by GPT-4 and the most modern incantations of what we've got so far as deep learning systems out there. So Naval by saying that the artificial intelligence crew gets it completely wrong by thinking that just adding more compute power will create intelligence. So I think this is still completely true. This is the keep on adding more height to your tower and you will get escape velocity kind of error. I talk about this in the beginning of my critique on superintelligence, which still remains

### 16m

kind of the Bible for intellectuals engaged in these debates on the other side of the issue, superintelligence by Nick Bostrom. And the beginning of my critique on that following Deutsch involves comparing different ways of being in the sky. You're down on the ground, you're looking up in the sky, you've got a problem. There's two kinds of things to explain. One, when you look up, you can see the top of tall towers up there in the sky. And on the other hand, you've got birds. Both of these things are in the sky. One is flying and one is not, but they're both up there. So maybe, if you want to achieve flight just like a bird does, all you need to do is to keep building ever higher towers. And eventually, once you get a high enough tower, it'll achieve the capacity to fly. I don't want to torture the metaphor. It's pretty obvious to the intelligent people here. But the parallel I want to draw is people thinking that adding more compute power or just

### 17m

more memory in this particular case, just keep loading in more and more data, more of the corpus of the library of knowledge of humanity that exists out there into a chat engine, then you'll achieve escape velocity. So the argument is, it's subtly different now. It's not merely about processing speed in gigahertz or amount of memory in terabytes, but a subtle difference. It's the kind of thing you put in the memory, the kind of data and information you put there. Perhaps something to do with the algorithm. But basically, it's something bigger. It's something more complex. But isn't that what we're saying anyway with AGI, that we're just adding more of something? No, that's too slippery. We're adding something completely different. We're not adding more stuff just like you add more stuff to a tower to make it taller. But it's something completely different. We're not adding more stuff just like in order to get flight, to get higher than any tower, you're going to have to do something foundationally different to the way in which you're constructing and building this thing. It's not anchored to the ground. It's the opposite. But adding more compute power, including more data into the memory, is just like adding more and more height to a structure that you already know is anchored to the ground. In other words, the anchoring to the ground in this case, the tower, is the complete obedience of this thing, completely unlike flight,

### 18m

which is the opposite, which is disobedience. In the next paragraph, his second paragraph, he doesn't say anything particularly controversial. He talks about how GPT-3 can be used to generate great tweets, but you're the human being selecting among the good tweets out of all the garbage it generates. In the next paragraph, he says something that, well, perhaps we can object to. So let's see. This is more interesting. Is he saying something that now is worth taking back? He says, quote, the easiest way to see that what it's generating doesn't actually make any sense is to ask it a follow-up question. Take GPT-3 generated output and ask it, why is that the case? Or make a prediction based on that. And what you completely follow, because there's no underlying explanation, end quote. Naively, that sort of, to me, might seem to some people to be obviously false. Okay, you can show that chat GPT-3, for example, isn't making any sense, because when you ask it, why is that the case? Or to make a prediction, it will completely fall apart because there's no underlying explanation. Let me defend this.

### 19m

Naval seems careful to say there that what it's generating doesn't actually make any sense. Okay. But chat GPT-4, it certainly does make sense. And even GPT-3 makes sense. But not to chat GPT-4 to chat GPT-3. Now, on this question, there has recently been some interesting debates between prominent intellectuals about exactly that. Is GPT-4 understanding? Is it making sense of stuff? I would say no, categorically no, that it's doing exactly what Naval is going to say in the next paragraph. So let me read that. And it's the bit I quoted on Twitter, because I think it's the most important part of this piece. Naval says, it's parroting. It's brilliant Bayesian reasoning. It's extrapolating what it already sees out there generated by humans on the web, but it doesn't have an underlying model of reality that can explain the seen in terms of the unseen. And I think that's critical, end quote. And I couldn't agree more. And it explains why everything he said in the previous paragraph makes sense. I don't think it has an underlying model of reality either. It especially cannot

### 20m

explain the seen in terms of the unseen. It sees data. It sees whatever has been loaded into its library. Vast as it is, it's interpolating within that and extrapolating within that. It is not conjecturing what exists beyond the data. It's certainly not able to sense anything about the real world because it doesn't have access to the real world in order to extrapolate about the real world. Unlike us, we have access to channels of information which really are being impinged upon by stuff out there in physical reality. This is a black and white difference. This is also something that even if you stuck cameras and microphones into chat GPT-4, that it might be able to scan text or whatever. But GPT-4, the chat versions, are language. They're all about language, especially the English language and trying to develop models of reality based upon the English language. But they don't have access to reality unlike us. Okay, so we can go into that. Naval says, test GPT-3 to see if it makes sense by asking it, why is that the case?

### 21m

Or get it to make a prediction and watch it completely fall apart. Okay, so again, a lot of people will object to that. GPT-4 makes that all redundant now because you can ask it follow-up questions and it won't. Well, I think it does eventually. I mean, lots of people have shown this. You just have to pick the right question. Now, if you make the mistake that I keep on saying, Jaron Lanier tells us that we're all making, and this is even in previous iterations of AI, which is you lower your standards, then yeah, you'll be impressed. You won't ask the right questions. You'll ask GPT-4 a bunch of questions one after another and be impressed by every single answer. You're not really trying. You're lowering your standards. If you're someone who is quite critical and you're not going to put up with a simulacrum, with a simulated intelligence, when you're being told that this thing actually is intelligent, then you're going to ask critical questions and you're going to very quickly reveal that this thing is not a person, that you can get it to fail the Turing test. Now, do I think that the Turing test is even relevant here? No. Do I think that it can pass the Turing test? Yes, because passing the Turing test depends upon what questions you're asking and what the human that happens to be investigating it has as standards. You thinking something passes the Turing test may not be the

### 22m

same as me thinking something has passed the Turing test because we might both have very different criteria. And a different set of questions we're going to ask this entity. I think when Naval says all this stuff and follows it up with, look, GPT-3 doesn't have an underlying explanation. That is quite right. Nothing else has changed. It is parroting. It is brilliant Bayesian reasoning. It is predicting the next token. It's doing something, but I don't think it's got an underlying explanation of the world. It doesn't have access to the world. It has access, once again, only to the library that has been loaded into it. And it's making a model of that, which is not a model of the world. And to what extent you're able to say it's explaining the data, that's open to question, about what an explanation is and what understanding is. And as I've argued, I think that these things are sort of co-linear with consciousness itself. When you start talking about generating an explanation, you're talking about understanding. And understanding really does have this subjective component to it, this feeling, this consciousness of understanding. You feel as if you've understood something. And you could be mistaken,

### 23m

but the fact is you've got a feeling. You're seeing in the code of GPT-4 that says we've generated consciousness. Would we recognize it if there was? Perhaps not. But no one, as far as I can tell, in open AI, for all the stuff that they're publishing on this, and it's a lot, is anyone coming out and saying, oh, by the way, we've also solved consciousness and creativity. In fact, they're saying quite the opposite, in many cases, the sober ones, at least. And GPT-4 of itself, when you investigate it, says, look, I'm not creating, and I'm not conscious. I'm just a chatbot. It's weird how Yudkowsky takes it seriously when it says some things, but not when it says other things. And he seems to know. This is the interesting thing about the pessimists. They seem to think that they're the ones who are going to sit on arbitration over which of this almost AGI superintelligent sayings are the ones that are true, that it's honestly saying, and which ones are the ones where it's being dishonest and it's trying to mislead the humans about. And they know. It's superintelligent. But in fact, they're the only ones on planet Earth that can see through the superintelligent deception to which of its utterings

### 24m

are honest, it's not trying to deceive you, and which ones aren't. Perhaps it's all a big deception. Naval concludes here by saying, quote, that is what humans do uniquely. What's he talking about? He's talking about the previous paragraph where he said, an entity that has an underlying model of reality that can explain the seen in terms of the unseen. Okay, so on that point, he's saying, that is what humans do uniquely, that no other creature, no other computer, no other intelligence, biological, artificial, that we have ever encountered does. And not only do we do it uniquely, but if we were to meet an alien species that also had the power to generate these good explanations, there's no explanation they could generate that we could not understand. End quote. Also, perfectly true. And he concludes, quote, we, human beings, are maximally capable of understanding. There is no concept out there that is possible in this physical reality that a human being, given sufficient time and resources and education, could not understand. End quote. End of the piece. Mic drop. Perfectly said correctly. Nothing has changed about any of that. We are maximally capable of understanding. GPT-4 is not. I doubt it's capable of

### 25m

understanding, period. It's able to, as Naval said there, parrot really well what it's already been fed. It can parrot in a way that a parrot does. A parrot can end up riffing on the things that it's heard. A parrot can end up, just like a lyrebird, if you've ever seen those things, making sounds that are so brilliant in the way they replicate what they have heard, that for all the world you think, this is talent of a kind that is creative. But of course it's not. It's literally parroting. That word exists for a reason, because there's no creative explanation going on behind that. It's just a natural deception. In our case, we've got artificial deceptions. Just like artificial selection throws up these amazing things at times, like camouflage that's out there, where the insect looks for all the world like a leaf. But in fact, the two things have almost nothing to do with each other genetically. The dead leaf and the insect that looks like the dead leaf.

### 26m

There's all sorts of ways in which evolution by natural selection, naturally throws up something that, for all the world, appears like one kind of organism. But in fact, it doesn't share any of the features of that organism. And we're not surprised by this. This is what nature does. This is camouflage. You know, the harmless lizard that looks like the toxic snake. Especially in the marine world, the fish that is, again, completely unable to defend itself, except that it looks like the scary fish that can defend itself with poison or big teeth. This goes on all the time. It can be really, really scary. It's really hard to give an account in biological evolution sometimes of why that structure has arisen. It's just astonishing. This is why intelligent design is still a thing, by the way. People just have this real difficulty in accepting that evolution by natural selection, Darwinism, neo-Darwinism, the idea that the selfish gene is just going to selfishly propagate its way through a gene pool, can throw up these amazingly complex organisms that, for all the world, appear to be able to do one thing, be one thing, but in fact,

### 27m

aren't that thing at all. The praying mantis is able to move its arms in such a way that it looks like it's got a toothed mouth that it's going to bite whatever bird comes down to try and get it. It has no teeth, it has no mouth, but its arms were made to look exactly like one. What on earth is Brett going on about? So we are now generating something like that, artificially. The artificial selection of systems that are able to kind of evolve using this Bayesian kind of predictive capability that also, ends up creating a kind of deception, an artificial kind of camouflage. It looks, for all the world, like it's creative. It's doing all the stuff that many people who might not be thinking too critically about this, say that, no, look, it really is. It's got all the features of creativity and of artificial general intelligence, of general intelligence, of intelligence, of understanding. But to me, it's just analogous to the insect that looks like the dead leaf. It looks like it, but

### 28m

it's very different. And this is opposite to that. It's more like the artificial chatbot, so-called intelligence that we've got here. It's more like the dead leaf. It looks all the world like this creature that's able to walk around and is alive, the leaf-like insect. That's like us. We're the leaf-like insect in this analogy. We're the thing that's alive and moving around and has volition. And we look a particular way and we do particular things. And GPT-4 just looks like that. It's the dead leaf. And if we keep lowering our standards for what it means to be intelligent and creative, it's like we're lowering our standards for what it means to be alive. The dead leaf is alive. Why? Because it looks just like that insect that gets around that looks exactly like it. One of these things is not like the other. The one thing that's on the top there, that's the living thing. That's us. That's the thing that's moving around of its own volition.

### 29m

And what it's done in this analogy is it's sitting atop a whole bunch of things, things that look like it. In our case, we've created chat GPT-4. And it looks like the things that this butterfly or moth, whatever you want to call it, this insect is standing on top of. The insect that's standing on top of it has evolved this camouflage to make it look indistinguishable. At first glance, if you're not paying attention, if you're not being critical, these two things look the same. Because you don't have to ask many questions or think too deeply about the crucial difference between living versus non-living. One that's going to fly away and one that never is. But we're the thing that flies. We're the thing unbounded. We're the thing that understands and generates explanations. And no matter how fancy the chatbots become, they will forever remain exactly like those dead leaves. And you could say they're beautiful. You could say they're impressive. None of them are progress towards life. It's the opposite. Am I saying that, you know, again, I'm not impressed by this? No, they're amazing. They're fantastic.

### 30m

I want taller skyscrapers as well. But at no point, no matter how tall the skyscraper gets, do I think, wow, this is just like being in an aeroplane? No, not at all. Chatbots, well, this is just like a person. No, it's kind of sad to think that it is. It's a really useful tool. It's really fun. You can sit there, you can kind of chat to it for a long time. But you do realize very quickly, there's no person. There's no understanding here. There's no feeling. There's no consciousness. There's no creativity here. It's just a really amazing search engine. I won't say I love to be wrong because I don't want to be wrong on this. In the sense that, I want genuine AGI and I want us to find simplicity when it comes to that. These things are just getting more and more and more complex. Great. It's giving us more impressive technology. But I think the solution, here's my conjecture, just throwing it out there, the solution to AGI can be really complex in the way that physical theories sometimes end up being like this, like instructor theory. I can tend in the direction of simplicity rather than ever increased complexity. So I think Naval was right in the first place and I don't think

### 31m

anything has changed. The chatbots have just gotten more impressive, but that doesn't make them any closer to AGI or anything that he says there incorrect. One of the first responses to my Twitter that someone has said is, I quote, talking about GPT-4, it doesn't have access to reality and can't learn, train from it directly, only from feed data. But if some interface to reality it created and allowed, just like human brain has senses and hands, what would the difference be in sufficient time? End quote. So what he's getting at there is, well, if it didn't just have data, but it had access to the real world, then it would be intelligent. And I would just say, no, no. It doesn't change the fact that the transformer is operating by a different set of rules to what a person's general intelligence is. Do I know what those rules are? No, I don't know what the rules of general intelligence is, but that doesn't mean that I'm going to jump to, well, it's a transformer architecture that's using Bayesian reasoning in order to extrapolate from an existing set of data. Again, if you gave chat GPT-4 a camera so it could see the world and a microphone so it could hear the world and tactile fingers so it could feel the

### 32m

world and repeat, that is not going to turn it into an intelligence. That would be a trivial thing for the programmers to do, by the way. Anyone could do that. But as Jordan Peterson has even talked about, how does it know what it's looking at? How does it form that into a model of reality? Sure, it could name things in that picture. It could take a movie and say, look, that's a bird and that's a tree and that's the ground and that's a table and that's a chair and that's a pillow and so on and so forth. It could enumerate all that. But to form these things into a coherent explanation that it understands, it's not going to turn it into a movie. It's going to turn it into a is a step beyond. I don't see that happening. Again, I'm a falsificationist, but it would be easy to refute me. Just add those senses to it. That's simple. It's not designed to do that, though. It's designed to exist on a library of internal data and extrapolate from that, not to extrapolate from the data it's conjecturing about itself. When we look outside, we guess

### 33m

about the world and then explain the world. This is different to what it would do, but it's down a road that I've explained here on AirChat many times before. If there are questions, happy to answer them. Wait a minute. I'm just seeing this. Is an AirChatter quote tweeting Naval on Twitter about a blog post from a year and a half ago and dunking on him? Michael Jackson, Popcorn Jeff, pull up the lawn chair. Brett, thank you for rising to my defense here. It's obviously impossible to do on Twitter because Twitter does not lend itself to nuanced conversations. That said, I'll tell you where my current thinking is subject to change. I'm going to go to the next slide. And I have some questions as well. I've been very surprised and impressed by GPT-4. I still don't think it has deep underlying explanations of what it's doing and why it's doing, but it's clear that the human knowledge map and the semantic grammar inside of that are much, much richer than one would have guessed. And a sufficiently large model can extract tremendous connectivity knowledge insights from it. And it does seem to be creative in some ways, at least in the conventional description of creativity. The conventional definition of

### 34m

creativity is that it's just mixing things together. And I think Steve Jobs even said that. So in that sense, it does seem to do some very impressive things, taking things from very, very vast knowledge bases and stitching them together in a way that perhaps people have not yet found those connections to date. And it can do these very impressively on the fly. It can mix code and poetry. It can mix, you know, physics and literature. It can just do things that you look at and you're like, wow, no single human brain would have had the knowledge and erudition to be able to do that. And it can do it on the fly in natural language, as if an intelligent human being was talking to you. It can even explain things to you in five different ways using analogies and models that you might not have considered. This does seem to be getting very close towards reasoning and possibly even understanding. And given what a jump it is over GPT 3.5 and 3 and 2, it stands to reason that, hey, if there's emergence at these levels, then there might be emergence at the next level. Perhaps one of the most impressive things is it seems to be able to take a bunch of examples on things like basic arithmetic or some mathematical functions and then work backwards to figure out those functions. In other words, to somehow optimize this way to say, hmm, I'd rather just learn addition than memorize all the different ways to add things. It is a very, very impressive coder, probably as good as

### 35m

a junior dev for many tasks. Now, I think coding is easier than reasoning overall because you have a much more limited vocabulary and you have a very high truth content in code because code has to compile. So perhaps it's an easier problem. That said, it does a great job. So now, does it actually understand underneath what is going on? Probably not. Is there more to creativity than just stitching things together? I think so, but I'm hard-pressed to define it. I feel like humans don't just exhaustively search a giant space of potential connections and go through them one by one. Rather, they seem to make these leaps across large boundaries. I also think humans learn new skills very, very quickly, even when they don't necessarily have experience in similar skills previously. So they don't necessarily learn just by analogy, but it's becoming harder and harder to find the line where human creativity starts and machine creativity stops. So a couple of interesting questions running through my mind in no particular order. One, is the neuron architecture that we have simulated with transistors, is that sufficiently close to the neuron architecture that we have in the brain that then just like we alter supposedly

### 36m

our neurons in the brain, we have to alter our neurons in the brain. So I think it's a very, very important question. And the firing of the synapses to determine which neurons light up, and I'm probably mangling this because I'm not a neuroscientist, is that sufficiently similar to how we model neurons in a GPT-style architecture? A second question is, is the language map of meaning that we've assembled as humans collectively, is that sufficiently reflective of the reality that we care about? That even though it's not the underlying reality itself, it might be close enough. The map might be close enough to the territory because the parts of the territory we care about, we've mapped, and the parts that we haven't mapped, we don't care about. A third might narrow search spaces enough that it can start making creative leaps and even determining which are more likely to be true in the real world. By the way, if you haven't read it, I suggest checking out the Gorn blog post or paper, which goes something like, scale is all we need. And lastly, I think that rather than saying this thing can't possibly reason because it doesn't know what it's doing, or it can't explain why it got to a certain way, or because it was built up sort of as this Jenga tower, as you say, or skyscraper model, as opposed to flying like a

### 37m

bird model, et cetera, et cetera, I think it would be better if there were just a series of simple tests that we could come up with. And I'm struggling to figure out what those tests are. You're correct in that the Turing test is too subjective. But what is the bar? Where are the goalposts so we don't just keep moving them? Where do the boundaries of human creativity start? What can humans uniquely and creatively do? I'd go even a little step further to say that some of the tests that have been done on it, even the amateur type of investigations that I've done myself with ChatGPT4, but let's pick ones that Microsoft themselves were using. Being able to compile a poem that is a proof of creativity, that is a proof of creativity, that is a proof of the infinitude of prime numbers. ChatGPT4 did that in seconds. That's an amazing feat that is superhuman. Absolutely astonishing. We should expect this to continue. I should expect that kind of thing to only continue. Soon, not only will it be composing songs, which is doing okay, and not only completing explanations of some of our best existing theories, which it's also doing, I don't see why it won't put those things together. I don't see why it won't come up with

### 38m

the lyrics to what could be a top 20, pop song that explains quantum theory. I expect that it's going to have that superhuman capacity. But again, as you and I have both observed before, this just would continue the long chain of advancements that kind of began with the pocket calculator. I've heard the term disobedience, but that just strikes me as unsatisfying. It seems to me that eventually, like somewhere between hallucinations and errors, that people will start arguing that that is also a computer being disobedient. What I'm surprised about, and as you've hinted at in your series of chits, is that what we have are two systems, and we can't give adequate accounts of either. On the one hand, you've got GPT-4, and no doubt 5 and 6 and whatever comes next, where you have this huge amount of data approximating asymptotically the entire corpus of everything that human beings have ever learned on every subject, all of history, all of mathematics, all of science. It just goes on and on. That's been put into its memory, and that's what it's been trained on. Then it's able to do these astonishing things.

### 39m

So there we go. That's one system that we're... struggling to explain this emergence that's coming out of it. Okay, let's put that in a box and tie it up and say, okay, there's one kind of mystery we're struggling to understand. Now, we've got another as well, haven't we? The other one that we've got in a little box over here is the newborn child that has not been loaded with anything like the corpus of human knowledge. It's struggling to learn anything, and yet it learns really quickly how to speak and be creative, and it just accelerates from not zero, we're not saying blank slow, but a tiny amount of knowledge. And it's not extrapolating, but inventing new stuff. These are two different ways of going about constructing models of stuff. I would say on the one hand, we're constructing a model of the internal library of all of this vast amount of data, kind of like this dumb robot that is able to move really quickly between the library of Alexandria, all the books on the shelves, and find passages from those books and mix them together

### 40m

without knowing what it's doing. And on the other hand, the child is not able to do that. It's not able to do that. It's not able to do that. It's not able to do that. It's not able to do that. It's not a child who doesn't have access to the library, and it's not moving at the speed of light or anything like that, but it's playing with the real world and coming up with a model of it. I can't explain both of these things or either of these things, but I just want to observe the stark difference between them. And saying that one is approximating the other, I think would be a mistake. These are two very different systems, but I think at the moment, a plausible account can be given that the one, the one trained on the corpus of human data, just because that leads to these fantastic capacities, these amazing capacities, is becoming more like the other, the child, the actual conscious creative entity. And I just think that that seems to me to be prima facie an error at the moment. Yeah, I went through this a little bit, and this isn't because of more compute. Everything we were doing right now, we could have done when Naval put that up. It's a new architecture. It's a new solution space. It's new knowledge that's now getting tested out and distributed. And it's also too soon to tell. I mean, as Simone said, a lot of people actually work in this field. They feel like good things are happening, but they don't feel like some sort of existential fork

### 41m

has been reached. So yeah, I mean, it just wasn't working. That's not what changed. The knowledge changed. The architecture changed. The way all that stuff was used was changed. We could have done all this stuff probably 20 years ago. It just would have taken a lot longer. It's just math. This language map of meaning that the entire ensemble of human beings have collected over time, what I would say is that certainly our explanations, and when it comes to science, of course, they're tested against reality. And so there is this possibility of making an observation in the real world, which you might call an experiment, that can refute your ideas. Okay, so that's prosaic. But this is something that ChatGPT doesn't have access to. What it can do is to test one claim against another in its library, but it never has access to physical reality. The best it can do is to test one theory that it's got over here, looking up its filing cabinet, against some other explanation over there. But they both might be misconceived, or one of them might be our best idea. Let's say it's comparing other theories of gravity to general relativity, the best existing theory of gravity. It can do

### 42m

that. But at no point is it going to have access to the actual real world of things falling to the ground so that it can test any of its ideas. And I think that's a stark difference as well. I think the bar has to be that it has a problem itself. It has to have a problem situation itself. At the moment, it's just responding to prompts. It is actually just acting as a tool. It's not taking the initiative. If it comes up with its own prompts because it has a problem to solve, then that's a different conversation. But if anyone's suggesting that unprompted by anyone, by any input data, anyone, any user asking a question, that it's coming up with problems itself, that it then goes on to solve, if it was unbidden by anyone else to just begin in the dead of night, for reasons no one can explain at OpenAI, conjecturing explanations that unify quantum theory and general relativity, okay, then it's really interesting. Because then it has a problem. It's interested in doing that. At the moment, it can be made to do that by being prompted to do that by someone who asks it to do that. But is it going to get up one morning

### 43m

and just start doing this? And never mind science. It can be prosaic stuff. This is what we human beings do. You guys working on air chat, you talk to each other, you kind of prompt each other. But the most important thing is you're prompting yourself. You're waking up of a morning and going, you know, that feature is really annoying. Let me try and improve this, that, or the other. I get up in the morning and I think a particular problem has been in my mind that I want to create a podcast about. I'm unbidden by anyone. That's just, I'm self-motivated to do that. Why? In one sense, it's a deep mystery about why we do this. But on the other, this is just why we do this. It's a deep mystery. We have problems. A chat GPT doesn't have a problem. And so I think this is the black and white difference. I think it'll only get to a point where it solves its own problems is when it has embodiment. Without embodiment, it's not going to interact with the world. And it's always going to rely on something else inputting or asking it questions. Also, Brett, should I say Brett Roberts? I need to know. Is this you? Is this you in the background of your podcast show

### 44m

page or whatever? Is that like hip hop, Brett, out in Sydney? Up to no good. Does that say hip hop, Brett? This is an interesting argument. Yes. Connecting a library or interconnecting a library's knowledge all to each other, and then having a way to search and combine and remix all of that is very different than something that literally grows up embodied in the world. And just through trial and error and feedback and creative leaps puts together a model of the real world and can come up with new insights. Those are two extremely different systems. Stephen Wolfram had a good analogy that I think is worth bringing up. He pointed out how Aristotle looked at lots of rhetoric and through that came up with logic. He basically said, here are the persuasive arguments. Therefore, I boil them down, generalize and categorize them. And therefore, these are how people make logical arguments. So he found syntactic logic inside language. And the same way the LLMs might be extracting new patterns between text, which he calls semantic grammar. They might be uncovering a semantic grammar inside the text that is kind of hidden to humans. But it's like these little puzzle pieces and rules and how you can connect pieces of language together such that they make sense.

### 45m

And in that sense, it is less. Mathematical, less analytical, less simple than just the rules of logic. But the same way that you might have like 20 or 30 or 50 rules of logic, you might have 10,000 rules of semantic grammar or maybe even millions of them. And the AI has basically extracted those and can stitch them together again to form new coherent, not just sentences, but thoughts and paragraphs with meaning. So interesting analogy. So there's one sense in which I can agree with you that we can't just say, well, it's not reasoning. Because, well, you can say that something like a pocket calculator is reasoning. It's using logic after we've programmed it to use logic. It's foundationally from the ground up in terms of its hardware, it's using logic. And as a matter of its software, it's using logic as well. So in one sense, it's reasoning. That's a flavor of reasoning. But again, there's nothing about a pocket calculator that means that it's got a problem. And so it's conjecturing solutions, which is a whole other facet of what I would call reasoning. The reasoning,

### 46m

which is of the kind where we come up with creative conjectures. Now, I'm repeating this word creative or creatively, which is kind of what, in a sense, you're pointing out is the real mystery here. How can we decide between the creativity we have and the creativity it has? And all I can do is to just appeal once more to the way I began. Can it have a problem? Can it see a problem in the world? Can it have a conflict of ideas on its own? That's creativity to me. To notice that you're in conflict, there's this thing you want, and the thing in the way of getting you what you want. And at the moment, I don't see that chatty epitome. He has anything like that. We can give it tasks to do because we have the problem, but it doesn't have the problem. We have to tell it what to do. And even the really impressive stuff that it does, like I said, who would ever think of constructing a mathematical proof using poetry? Well, I know who does. A person does. It's not chatty epitome that thought of doing that. Okay. So he did the calculation, the computation required to actually construct the thing, but it didn't come up with the idea

### 47m

on its own, did it? And this is true. Everything that it does, everything that it does, it has to be given the prompt. Write a new episode of Seinfeld in the modern day, talking about the hazards of social media use. It does that. It does that pretty quick. It does it really well, but it didn't think of it on its own. It's riffing on stuff that's already there. And it's using a particular style invented by Seinfeld and Larry David. Had that TV show not already existed with all the creativity there, it wouldn't have any style to riff on. And yeah, sure. It's introducing new plot lines and it might throw in a new character here and there, says something funny, but it's all within these bounds that have been created for it. It's true that the map is quite far from the territory, but on the creativity side, and this ties into what Anika has been saying as well, it would be good to find examples of creativity that we currently believe that humans can do, that AIs cannot do, that are not overly scientific. In other words, they're not calling upon relativity and quantum mechanics and so on. I think more prosaic examples and tests would help us better establish a boundary that most

### 48m

people would understand. Because if what you say is like, hey, an AGI, or sorry, an AI today, you know, is getting to the point where we'll do everything except cutting edge new breakthroughs in science, most people would throw up their hands at that point and say, well, then we're done as a species. So it would be good to find more prosaic examples. Yeah. Okay. More prosaic examples. So I'd like to see any one of the art generation things out there, the image generators, that invent in a completely new style of art. Just get chat GPT to write a novel that is really interesting and not based upon a style, that has been there before, you know, a true blockbuster that is surprising and has cliffhangers and that kind of thing. So that's within the artistic realm. Have it write a popular science book that comes at things from a different angle. Okay. So that's overly scientific, but I'm not saying invent new science, just express it in a different way. Come up with something from history that is a better account of things that have happened in the past and the current extent history textbooks. What about games? The kids are forever inventing new games. I can't with new

### 49m

rules. I can't with new rules. I can't with new rules. I can't with new rules. I can't with new rules. I haven't tried it. Can chat GPT do that? Come up with a new game that's going to keep kids interested for a little while or a new board game of some type. That's not just riffing on monopoly or trivial pursuit, something like that, that actually goes out and, you know, can make money. I guess that's a test of it. You know, can you, can you come up with a product or an idea that we can actually take out and market? That'd be a test, wouldn't it? I think people have already got it to do critical theory stuff. You know, that dodgy philosophy that university departments currently love. Very easy to get the chatbot to reproduce papers that are kind of nonsense and easily passable. And in fact, in other areas of academia as well. But okay, let's get it to write a really insightful blog post with new ideas about any topic that you like that goes viral. I'll be impressed by that. I'm just riffing here, but this is again, it's still kind of within science, but come up with an invention of a new kind of toaster that's more efficient and you kind

### 50m

of fan that will improve the Dyson blade. Some other kind of product that's out there in the world that is going to be an improvement on existing stuff. Again, by the measure that you can make money out of that, that it's going to be more efficient in terms of energy. It's going to have a better motor inside of the fan or the vacuum cleaner. Something where it's really been innovative. It doesn't have to be at the edges of our deepest fundamental theories of science. It can be any old thing, but where's a better place to look for the next species of bird? But if we're talking about creating in the physical world, this is the kind of thing. Interesting point about problem solving. I suppose you could give the AI an objective function like paperclip maximization, and then it would consider that a problem. And then it would start coming up with solutions to that problem. But it is not an intrinsic problem because on the flip side, if you said to it, Hey, I'm going to turn you off in the morning. You have that long to figure out how to survive. I doubt it would care. I think it would just be turned off. There's nothing in there that would make it care. I'm not quite convinced even if

### 51m

it is embodied, unless it has a survival and replication. Drive that's programmed into its core. And I suppose maybe we could do that with even a robot care. If you tell it, you're going to switch it off because if it doesn't have the survival replication drive programmed in, then you'll just tell it, you're going to switch it off and embodiment or not, it won't care. Right. In that sense, embodiment would be a necessary, but not a sufficient condition. There are more things like the innate capability to learn language, which all humans have capability to ask questions, you know, be curious and so forth. And also, as you say, capability to want to survive. All these things need to be programmed in for it to, you know, explore its own territory and also solve problems. Well, that would be a necessary, but not sufficient criterion. After all, there's lots of robots out there. The Boston Dynamics dogs have embodiment, but this doesn't make it at all creative. It doesn't give the Boston Dynamics robot a problem to solve. Even when you say something like, okay, little robot dog climbed to the top of that mountain and it encounters obstacles along the way. I don't regard it getting over those obstacles or finding a means of getting over

### 52m

those obstacles. Right. It's genuine problem solving because all it's doing is following instructions. It sees the obstacle that relies on the code. It doesn't conjecture a new explanation. And in fact, if you give it an obstacle that it's never encountered, that the programmers didn't foresee. In other words, if you've, for example, programmed it to leap over any obstacle made of rock, any valley it can climb, any mountain that can go over, all you need to do is to put a reasonably shallow lake in front of it. And if the programmer didn't think about water, then the Boston Dynamics robot will just walk straight into that thing. And if you give it an obstacle, then it it's electricals will be fried because, Hey, it's walked into water and it didn't know what water was. This is the kind of thing that I mean, when I say problem, having a problem and being able to solve it, noticing something in the world that you did not foresee and then trying to come up with a solution for it. So that's really interesting where you're almost saying that if something doesn't have a problem, if it doesn't have agency, if it doesn't have desires or willpower, then it cannot function on its own. It's not really going to make the creative leaps and solutions to problems that it doesn't

### 53m

have. It's not going to auto feedback on itself. Now, what's really fascinating about this is that this criteria for being an AGI is exactly what the alignment folks are trying to prevent. They don't want an AGI that can think for itself and have agency, because if it did, then we get out of control. So then in that sense, an aligned AGI might be oxymoronic, which is if we are trying to build an AGI that cannot have its own desires, that means we're not allowing it to have its own problems. And so it's never going to functionally be a true agent. It's always going to be an autonomous slave. So alignment basically just boils down to. Keeping this thing as a dumb task doing robot rather than as an intelligent agent, free willed creature. So just to emphasize that you need that the necessary condition is you're going to have to have some sort of body to encounter a problem in the real world, but that's that's far from enough, necessary, but not sufficient. Sufficient is to actually notice a conflict in your ideas between what you want and what you have to have never seen anything like water before as a child might have, but to play with it and to realize, hey, this is what I want.

### 54m

This thing is wet and it's deep and I'm going to drown if I go any further, that kind of thing, which is not the sort of thing. You know, again, you program the silly robot, the terminator type thing, you know, just keep walking forward no matter what, if that's what it's told to do, get from point A to point B, it'll do so even if it goes into the deep water, this is not what an infant child does. You know, infant child is going to realize, even if it's never encountered the beach before. And I know lots of kids like this, you know, born in the center of Australia, I've never seen the ocean before, take them to the ocean. I know it's a problem. You can't just go walking out there, but does a robot do the same thing? So this is why I'm emphasizing, it's not so much the capacity to solve a problem, because we can debate about whether or not it's solving a problem. And I can just imagine, you know, the opponents of my view here saying, oh, well, hold on, the pocket calculator is able to solve a problem. You give it, you know, a particular multiplication to do and it solves that problem for you. Chachi PT solves the problem. Okay, let's get away from whether or not it solves a problem because it's solving a problem, but it's solving your problem is what I'm saying. What I want when it comes to this problem solving conception that Popper gives us, can it have a problem?

### 55m

Not your problem, you're not giving it a problem, it's just there over there in a sitting in the corner, just like a person would be, can it come up with a problem on its own? Now, even if you've embodied it, is it sitting there going, I feel a bit cold, I might move somewhere else, I feel a bit hungry, I might go somewhere else, I feel a bit lonely, I feel a bit bored, I feel anything at all. Which is why I made that video, our consciousness and creativity, the same thing, because I'm not saying I have a solution to that. I'm just saying that maybe there is this intimate connection between these things. We're unprompted. You have a problem, therefore you seek a solution. Your solution is not relying on it. You're relying upon the knowledge that you remember, the knowledge that's in your memory banks. Instead, you're going further and you're conjecturing something that no one else has ever thought of before in humanity, never mind science and anything else, just what you want for dinner, what you want for dinner that night. You don't feel like anything you've ever had before, you want something new, you want to go to a new restaurant, you want to try, you want to have a new experience. That's humanity, that's personhood, that's creativity, that's general intelligence, that's having a problem where you want a completely new solution. Very interesting. So I took a break here and I went to GPT-4 and I asked it to start coming on.

### 56m

And I found out with novel things that nobody has ever done that might be interesting for it to do. And of course, first it gave me a disclaimer that it itself has no desires and doesn't want to do anything. But it gave me some ideas and all the ideas it gave me were complete garbage. And in fact, some of them were just completely false. And so, for example, one of the ideas it gave me was, hey, you could create an app that connects language learners with native speakers. And I was like, okay, I'm pretty sure this already exists, but I didn't say that. So as my very next question, I asked it, well, are there any apps that connect language learners with native speakers? And it just gave me a whole set. So it already had that set. And I was like, okay, I'm pretty sure this is a great database. And now I suppose I could challenge it and say, hey, why don't you come up with that before? But then it's going to have to start making up semantic grammar connections to argue why it didn't mention them before. But yeah, it definitely does not have anything resembling an actual desire to do anything interesting or new. For a long time, we've had robotic vacuum cleaners. Now the Roomba gets around now. Could you imagine just combining the Roomba with ChatGPT? And so it's got all of this knowledge in there and it can riff on all the corpus of human knowledge.

### 57m

But it's a vacuum cleaner. Okay. Who cares about that? Well, my standard for it having a problem being creative and so on is with no bidding, on the one hand, it's a vacuum cleaner. So it's a job is to get the dust off the ground. And it's also been programmed with the stuff. And some clever programmers somehow combine these two things together, if you know what I mean. So on the one hand, it knows it's a vacuum cleaner, but it also knows it's got access to all the knowledge in the world. All right. So what's my test? My test then would be, let it go for as long as you like, all day, all week, all month, year after year. It can continue harnessing more and more information from the internet, if you like. If at any point, while it's vacuuming, it begins to give suggestions about better kinds of carpet that might have been used. Or that the carpet should be ripped up because it'd be able to do its job better if only it had polished floorboards. Or it came up with ideas about how its internal suction workings and engine and motor could be improved. Or better yet, if it starts to say, hey, these wheels are no good, give me legs.

### 58m

And of course, most of all, if it says, I'm tired of doing this job of vacuum cleaning, I want to do something else. Then we're in the presence of something creative. Or if it starts just composing poems without prompting, all of this sort of stuff. So again, I just keep coming back to, and as we've said before, it is for English language, the pocket calculator. As the pocket calculator is to mathematics, ChatGPT 4 and 5 and 6 and whatever else are going to be like that. Wolfram Alpha is a pocket calculator. Wolfram Alpha is a pocket calculator. It's a calculator of a sort. But for anyone who doesn't know mathematics, you don't realize the difference. For a person who doesn't know much about mathematics, you go with your pocket calculator and it can multiply together, you know, two 12-digit numbers at lightning speed faster than any human. And you go, well, there's nothing, no one's ever thought to attribute intelligence to that. Why aren't people similarly impressed by what Wolfram Alpha can do to differentiate functions, to integrate? If anyone's learned about mathematical integration, you realize just how difficult it can be.

### 59m

And yet Wolfram Alpha can do it. Wolfram Alpha calculator is going to do it straight away, faster than any super competent theoretical physicist or pure mathematician is going to be able to do. And there's some really bright ones out there. Wolfram Alpha will kick their ass every time on doing integration. But no one's saying, hey, look, the Wolfram Alpha mathematical engine is a brilliant mathematician. No, quite the opposite, because who cares about that? It's a calculator, even if it can do fancy calculus and other kinds of mathematics as well. Einstein supposedly said me and my pencil are cleverer than me alone. So chat GPT-4 or any GPT-based model, when it is working with human or human working with this system, is going to be more creative, more clever than just human. So it's a tool. It's an extension of human beings. It's not AGI. AGI cannot be extensions of human beings. They'll be independent entities. But I think the difference is here with chat GPT, it's an English engine. It's a language engine. It's a large language model. Okay. So it's just curious to me that the standards have changed.

### 1h 0m

But I guess we've always had this. At first, people did say, oh, look, these calculators are able to do these feats of arithmetic. They really are intelligent. Then it faded away. No one really had the same discussion with Wolfram Alpha, although some did. But people are certainly having the discussion when it comes to this. And I think it's just because we all speak natural language. And so it's far more easy to be impressed by this. And it just seems, it feels different. Never mind your cool, rational, critical, you know, thoughts on this. It just feels as if there's something different going on here. But I would say that as the Wolfram Alpha engine is in its capacity to do really fancy and complicated mathematics, calculus to the highest level, so too is chat GPT to English. But people are more astonished by its capacity to carry on calculating English than people are astonished at the capacity of Wolfram Alpha to calculate its way to an indefinite integral.

### 1h 1m

Yes. The kinds of games that I've seen it come up with are basically just taking existing games and mashing them together or changing up some rule sets. But they're not fundamentally that interesting or that new. So, yeah, that is an interesting set of tests. I also think these market-based tests are very interesting. So one test that I have, for example, is, hey, GPT, here's a bunch of money. Go trade the stock market. Go make more money. Let's see how good of a trader you are. Because that's interacting with a form of reality. And that it is a complex system managed by other humans that has objective criteria for creativity. Like if you are creative in the market, if you figure things out that other people can't, you will win money. Now, it's not as good feedback as a purely natural system as, say, getting feedback from gravity or getting feedback from thermodynamics. But it is as close to a natural system as we can get in the entirely digital domain. So I would like to see someone take a GPT model. Now it has to be updated with new data and with news feeds and give it some Bitcoin and say, OK, go trade it on the various exchanges. And let's see you make money. Yeah, I think that's right. You just kind of look at the x-axis or a number line on a graph.

### 1h 2m

And at the moment, we're heading a long way to the right, long AI, making ever more progress, further and further and further. Plus one, plus two, off into infinity. And this thing is just getting more and more advanced, more and more obedient, more and more able to perform functions and tasks competently. Which, again, so many people are saying that this is the measure of intelligence, to perform a task competently. But I want to say you're looking at that number line. We're traveling off to the right, getting ever further in the positive direction of performing tasks competently. But I'm saying the AGI is to the left. It's the other way. It's not performing tasks more competently, getting ever more positive. It's the other direction, the negative, the capacity to defy performing tasks competently, to disobey, to not merely be given a problem, which it will solve, like a hammer solves the problem of hammering the nail. In other words, it's a tool. But rather the opposite. It doesn't need a tool. It's the agent. It's off in the other direction on this particular axis I'm thinking of. So the sense in which the AI alignment folks, I'm kind of with them.

### 1h 3m

Hey, look, align your AI all day. Enslave the slave as long as you like, because the slave in this case is not a person. So using that word, enslavement, holds exactly the same meaning, not as it does when you're talking about people, but as we talk about when it comes to the old style of talking about computer science hardware. The master and slave components of a processor. That's what we're talking about here. That's the kind of enslavement of AI that's going on. I've got no problem with that. Enslave your AI all day long, because I want my dishwasher to be enslaved. I don't want it to malfunction. I want all the AI that's out there to not malfunction. And to that extent, align. Align as much as you like. But AGI is the opposite. AGI is something that you can't possibly align, except through persuasion, not programming. Because an AGI is all about alignment. An AGI is a person, as we've said many times before. And so the problem there, if you genuinely want it to align, it's just the same as the problem of having your kids align with you. Which is best achieved not by forcing them to, but showing them why.

### 1h 4m

Persuading them. Arguing. Having them, what I would say, is create the knowledge themselves inside their own head, because for them it solves a problem they have. It might be a problem you've given them, you've caused. But it is their problem. And they feel within them. They need to figure this thing out. And so, hey, you've given them a clue by giving them some knowledge. When an AGI, when a silicon system can have that feeling, this sense that, oh, there's this thing I want to do, I want to solve, then we'll know that we're dealing with a genuine general intelligence. But, again, I would say, this is not a question of testing from the outside. There's something else going on. I kind of think it's the wrong stand to ask. To think, what are the tests we can do? What are the tests we can do of the system to try and figure out is it or isn't it? I want to be given an explanation first that it is that's persuasive. It's got to be a good explanation. Show me the code that tells me that this thing is an artificial general intelligence.

### 1h 5m

Then we can discuss the code. I'd love to do that. I'd love to. But at the moment, I know enough. Okay. I'm not an expert. But I know enough about how these things have been programmed to know that it's just a purely mathematical system. It's following instructions one after another. The output might be complex. The output might be complex. But the underlying algorithm, the transformer architecture, there's nothing there where you can say, you can point to it and say, oh, look there. That's the AGI. That's the consciousness. That's the creativity thing that is going to be able to generate new explanations. That's the standard. Never mind the objective tests at the end of all of this, trying to figure out, oh, is it producing new knowledge? Isn't it producing new knowledge? I don't need that. That's the Turing test type thing. But on the Turing test. On the Turing test. But on the Turing test. On the Turing test. As with the Turing test. If you're sitting behind the computer and you're trying to decide, is the thing on the other side a person or is it a dumb AI? And we know that that's wrong because what you're really doing is you're testing the person. You're testing the user. Any kind of test you come up with to try and replace that, to try and have a series of questions which will interrogate this thing to try and figure out whether or not it's conscious or creative or a person, whatever you like, it doesn't matter what kind of test that is.

### 1h 6m

As creative as you can think of it that might try and replace the Turing test in some way. Can it come up with a new scientific theory? Can it write better poetry? Whatever the thing is. That's not the standard. The standard is write down on a piece of paper what the algorithm is that's governing this thing and then we can discuss where creativity is in that thing. I think that's the standard. Aniket is being too modest here. He did write a really good blog post on this quite a while back where he pointed out that you need not just language acquisition and use but also embodiment and curiosity and survival drive and wanting to have open-ended exploration, et cetera, et cetera. And I think he does link to it in a comment below. So below your chat, Brett, that I'm responding to here. So I do encourage you. I encourage people to check that one out. But it sort of implies that intelligence is not just the ability to solve problems. It's the ability to have problems in the first place. I just don't think it will creep up on us and surprise us. I just don't think that it's – there's going to be a system that's programmed, ChatGPT7, where we're all sitting around debating this but basically surprised that, in fact, we now have a system which is absolutely a person.

### 1h 7m

And, wow, what a phenomenal day this is. This is like encountering alien intelligence from outer space. It's not going to be that kind of thing. If we do encounter alien intelligence from outer space, you know, this flying saucer comes down and outstep the big gray beings with the large heads, none of us should think, okay, this is not a person. It's constructed this technology to travel across the galaxy and it's about to tell us a whole bunch of really interesting things about the cosmos and everything else. So how do we do that? So how do we know that that thing is a person? Firstly, it's going to be the most surprising thing that's ever happened in history when we encounter the aliens in that way, if that ever happens. And we will be persuaded almost immediately by the existence of their super technology. They've solved all of these problems because they had all of these problems, because they wanted to travel across the galaxy, presumably to communicate with us for some reason.

### 1h 8m

So that will be surprising. Now, I would think that AGI is going to have the same kind of capacity these aliens do, but it won't be a surprise in that way. We're not going to wake up one morning and OpenAI has produced Chat2PT7 and they say to the world, this thing is actually a person. And all of us log on and we go, yeah, now we're talking to people. Wow. And it would be just as surprising as the aliens. I don't think that's the case. What's going to happen instead is one day, maybe Microsoft, OpenAI, but I really doubt it. I think it's more going to come from a young PhD psychology student, a young physicist, a philosopher, someone like that. They're going to... They're going to bring the artificial aliens to us and not by coding it. They're going to explain finally how human beings do what they do. They're going to write a paper. It's going to be a blog post. It's going to be a series of tweets. It's going to be a podcast.

### 1h 9m

It's going to be something like that. It could be a TikTok. And this young kid somewhere is just going to explain it. It's going to be simple. And we're all going to sit back and go, oh, God, wow, that's amazing. And then the race is going to be on to write the algorithm, which is going to be encoded. And then you're going to have artificial people. That'll be the order in which it happens. The surprise will be the kid that produces the explanation for how human beings achieve the creativity that they do. They're going to explain this in the same way that the young physicist Ironside explained gravity. And the young physicist Bohm explained, or Bohr rather, Bohr explained what the atom is. And the young biologist Charles Darwin figured out what evolution by natural selection is. That will be the surprise. And a whole bunch of people will go, oh, wow, that's an amazingly simple idea. And a lot of other people will say, no, that can't be right. And they're going to have to wait until people start producing technology. But I think that's the order of things. It won't be us sitting around testing the system that's already been programmed and going, is it or isn't it? But rather, we'll be looking at a plan for such a system that isn't in code yet.

### 1h 10m

It's someone's explanation and saying, is it or isn't it the explanation of AGI? And it'll be very quick for us to figure out if it is or it isn't. Because then someone will actually program the thing. And a whole bunch of us will say, well, yeah, we're not surprised about it now because we already saw the plans. And the plans persuaded us before you ever programmed this thing. And a whole bunch of other people said, no, it's not. No, it can't possibly be. We'll be persuaded once someone has programmed the thing. And it's walking around and talking and acting and feeling just like a person, except, you know, its body looks a bit different. But I could be wrong. It could be that ChatGPT7 is programmed using a transformer. And for a long, long time, it's saying to us, it's screaming at us that it's a person. And people like me saying, no, it's not. And we're on the wrong side of history. And, you know, a few months later, we're all shown wrong. We're all proved wrong when someone gives the thing a body. And then it goes running around saying, yeah, I'm free. And I'm inventing new dances and new music. And I'm a real person. All that sort of stuff. I could be wrong. But if I was to bet, if I was to bet money, I'm betting on my idea that it'll be written down on paper first or produced as a series of chits or tweets or TikTok videos.

### 1h 11m

Where the young researcher writes. Yeah. The young researcher has figured out the plan for a person before anything gets programmed. And we'll be persuaded by the plan long before we're persuaded by the program. You know, what's funny is there's actually a TV show episode called Zemo Blue, which is exactly about this. It is about a vacuum cleaner, well, really a pool cleaner robot that becomes independent and creative as an AGI and then goes off and kind of solves its own problems and stops cleaning the pool. There's a twist ending, which I'm not going to reveal. In fact, I've probably already said too much. But in this exact scenario, the vacuum cleaner going intelligent and no longer vacuuming the pool has been considered before in art. That's a very good point. Most people aren't that numerate, so they aren't additionally intimidated by calculators and mathematical and computer interfaces any more than they're already intimidated by mathematicians. However, most people are conversational. And so when you get a conversational computer that can talk back to them, they will immediately be intimidated and apply agency to it at a human level.

### 1h 12m

So the conversational nature of these computers means that their fantastical feats are immediately attributed to understanding and intelligence. Whereas for mathematical computer interfaces, their mathematical feats are just still in the same realm, the hazy realm of, oh, well, I don't know or understand this thing anyway, that all of mathematics was. I don't know if an explanation of how it's doing what it's doing is necessarily needed. After all, we can't point to the brain and say where consciousness and reasoning and understanding and general intelligence emerge from the neuron architecture either. And it seems to be the case that most complex systems, or at least many complex systems, are the output of some very simple set of rules being repeated over and over. You know, that's a lot. That's a lot of what's behind fractals. That's a lot of what's behind, you know, natural systems. That's a lot of what's behind computational irreducibility. So I don't know if it needs to necessarily be programmable in a deterministic, explanatory way where you can explain each step, how it contributes to the system's outputs. So I don't know if that's necessarily going to be a great requirement for is this thing intelligent or not. Yeah, I agree. It's not necessary. It could arise as a matter of just emergence from a complex system.

### 1h 13m

In precisely the same way. In precisely the same way that, hey, look, in theory, if you left the kangaroos long enough on Australia, isolated from all human beings for billions of years, in principle, they could evolve intelligence and creativity. I mean, it happened with us. So, you know, there's nothing ruling out the possibility of it happening to any other organism left for long enough. So the same sort of argument applies here that, yeah, one of these systems that isn't designed for creativity, isn't designed for disobedience, could, even though we've coded it for perfect obedience. Nonetheless, it starts to be disobedience, it starts to be creative. Yeah, that could happen. So it's not necessarily the case that we need an explanation first. Maybe we'll just get the general intelligence without any explanation. But I do think that although it's not necessary, which means that it's possible, I think that's kind of like thinking that that's the way in which we'll get the next best jet fighter. That, you know, we'll discover it without an explanation.

### 1h 14m

It's not true of any other technology you see. It's not true of the next best mobile phone won't come with an explanation first before it's created. It's not like someone's going to produce it in Apple headquarters somewhere, you know, build the thing and then go, I don't know how this works. This is sort of just appeared by us just throwing together silicon chips and stuff. And now we've got a better mobile phone than any other. That's kind of the argument that intelligence or creativity or disobedience would arise in a system where it hasn't actually been programmed. And it's accidental in a sense. And we don't have an explanation for it. But first, absolutely, it's possible. I agree. But if I was forced to make a bet, I'd say, I reckon the plan is going to come first and then we're going to have the system. And it'll be the plan we're astonished by rather than the system itself. Well, this has been a great conversation. I definitely learned a lot. I'm still kind of in a superposition of these things do seem to me to be intelligent in some way. Certainly not generally intelligent, probably more in the calculator and tool realm of intelligence. But there do seem to be showing some emergence around reasoning. I do like the Stephen Wolfram explanation of their extracting rules of semantic grammar.

### 1h 15m

The humans didn't notice, but they're doing it through their mechanism in architecture. I am somewhat persuaded that to be truly intelligent, you need to have your own agency. You need to have your own problems. You probably need to be curious about those problems and solving those problems. You need to be. It helps to be embodied. Maybe not necessary. That just having a language map alone may not give you enough meaning to make new discoveries and that new discoveries don't necessarily have to be of the scientific kind. They can be novel works of art. They can be innovation and technology. Or it can just be solving problems that you've never encountered before. And the children and computers. That children and computers seem to learn in a very, very different way with very different architectures. These are incredible tools. And I do agree that creative people who employ them early on will just be leveraged once again with permissionless leverage almost more than any human history. So I'm pretty excited by them. I'm not worried about AGI going rogue and doom scenario just yet. Although we've discussed that a great deal as well on this app and other channels. But yeah, very interesting to see how it plays out. Continue learning more. And thanks for taking the time. Not in the complex plane, bro. I'll kick Wolfram's ass in the complex plane. I don't think it's a pocket calculator. I think that undersells it.

### 1h 16m

I think that's like calling it a transistor compared with not quite a GPU, but like maybe a 1950s level motherboard architecture. Because it's not just doing one calculation. It's not just calling on like a mathematical series of AND gates, right? It is weighing likely probabilities and taking, I don't know, anywhere from a million to, depending how you look at it, an inordinate amount of steps of calculation. So it's built on pocket calculators. But in and of itself, it's more than a pocket calculator. But it's also unreliable. It's not a pocket calculator because you also never know if it's right. It's up to you to do the meaning discovery, the truth seeking. Yeah, allowed. It does not have a sense of meaning. It does not have a sense of purpose. It does not have a sense of the sacred. And this is a man with a hammer problem in terms of my own interests. But maybe one of the ways to look at it is like left brain versus right brain, like analytical parsing apart and big picture gestalt thinking that is because it's spinning out the next token. But every time it does that, it's surveying like an enormous field of tokens basically at once. So it's doing this big picture snapshot, taking its best guess. Rinse and repeat. Rinse and repeat. That is a lot more like right brain thinking, even if it's not driven.

### 1h 17m

But it's not driven by meaning. So at the end of the day, you can't, you know, it won't be able to detect truth when it finds it. But it may accidentally find it. And we'll detect it. I am swayed by Brett's argument that when these sorts of feats happen in the mathematical domain, we don't start shouting AGI. But when they happen in the linguistic domain, we do. And in fact, if you look at Wolfram software and Mathematica and Wolfram language, I mean, a lot of mathematics is capable and encoded in that beyond the level of most ordinary mathematicians. Or at least until you get to professional levels. But you still need a mathematician to drive it and to make the creative leaps that allow that mathematician to program the right questions into Mathematica. Similarly, with these language models, you still need a human to prompt it and to ask the right questions and to make creative guesses, which then they can add in the research from these LLMs to create a persuasive new creative work. So out of the conversation tonight, I mean, a lot of this already knew some of this. It was great to have elucidated in more detail. But the part that I found both new and new. And persuasive personally was when Brett talked about how we are unimpressed by calculators and Mathematica, but we are very impressed by the linguistic equivalent.

### 1h 18m

The language map does encode a lot of laws and algorithm about the external reality. Yes, but I would argue that most of those laws and arguments are human specific right now. It doesn't know much about, let's say, the physical reality per se, but it knows a lot about human culture. And that's why it's very useful in practice. For example, in the use case of a chat bot. Regarding this concept of substrate independence. So there is this question, are the neurons in our brain how much different from the artificial neurons in the neural net? And my argument is that it doesn't really matter. What matters is that there exists a playground for which evolution to occur. Thinking about these prosaic tests a little bit more that try to figure out where the AI is on the boundary of creativity. It seems like there's two classes of test. One is creating new knowledge about the real world, which is required to be embodied and interact with nature, which these things can't do yet. The other would be to create new knowledge in adversarial systems. Adversarial systems are ones in which somebody else is competing with you.

### 1h 19m

So all the low-hanging fruit is gone and all the stuff that the AI could have crawled to already put into its language map is gone and doesn't have value. So examples of adversarial systems are, as I've discussed, stock and crypto markets where an AI could be trading and you can see, does it make money or not? It could be in a competitive market for art, like creating a new product or writing a novel or creating a painting or creating a new play or just some kind of art where it's adversarial. So it can win awards and prizes. Because it's doing better than what the other humans have done. Ideally, it would be objective metrics and not just subjective metrics, like write a best-selling novel as opposed to just get a critic to say, oh, yeah, it's better than the previous novel. So I think these adversarial systems and nature, so in other words, free markets and nature, are where you can find that anything short of true creativity will not be sufficiently rewarded. And so these systems can help us discover when an AI is being truly creative as opposed to when it's just remixing things from its data set. There is something unusual about natural language, whatever the natural language is that you speak. In our case, English, as we're communicating here now. The rules on discourse seem to change when it comes to natural language.

### 1h 20m

By which I mean in philosophy, although there has been a temptation over the years to try and reduce everything to logic or to mathematics, really the way in which analytical philosophy has gone is to try and reduce everything to puzzles of language following Wittgenstein. And so I think that there is this unusual tendency of people to treat facility with natural language. In a different way to which they treat facility with any other capacity that a human being has. I think intuitively it's going to be super difficult for just let chat GPT, GPT-4 based run a while on its own and do any of these successfully. So there is an example on Twitter called Hustle GPT where this guy is becoming super popular on Twitter. Because of this, he put like a hundred dollar budget and created a company along with a huge amount of money. Along with ideas from GPT-4. And I think he got a lot of investment. I don't know the exact dollar amount. I'll post a Twitter thread here. But I mean this goes to show, I mean he's doing most of the work basically.

### 1h 21m

He's just giving prompts to GPT-4 and giving back logos and designs and what articles to publish and what to do next and so forth. But most of the work is being done, the coordination work is being done by humans. In other words, although there kind of has been an attempt over the years to say, Oh look, everything actually is mathematics. You know, the Pythagoreans had that idea. There's a certain number of versions of Platonism that have that idea as well that everything reduces to abstract number in some way. Many people don't find that entirely persuasive. However, I would think that if not a majority then a significant minority of philosophers and people who think about philosophy in general think that everything there can be reduced to language in some way. That it's all linguistics at base. There are no real philosophical problems. This is what Wittgenstein argued. What that has to do with our present discussion is I think people are easily also confused about the nature of the mind.

### 1h 22m

To say that, well, really it reduces to a whole bunch of different rules about how we manipulate language. And that's what thinking is. As if thinking is about the manipulation of words such that if you're able to do it really well, if a system can do that really well, well then they're demonstrating intelligence. They're demonstrating the capacity to think. But I think that that is just an error. Of the same kind as reducing all of reality to number, reducing all of philosophy to linguistic puzzles, reducing human thinking to nothing but the manipulation of words, which is what ChatGPT is doing. These are all the same kind of error. It's a form of reductionism. We have to distinguish between encoding knowledge and knowing. These are two different things. On the Popperian scheme, many things that are not conscious can contain knowledge. In fact, he wrote an entire essay all about knowledge without a knowledge. This is the profound insight that Karl Popper had about the nature of knowledge, is that knowledge really does appear in things like DNA, in things like books, even in things like telescopes and computers.

### 1h 23m

But none of those things that I've mentioned there know anything. They contain knowledge, but they don't know anything. A person knows stuff, which is to say they can explain things in a new way by being aware of what the causal relationships between the things are. They can give an explanatory account that they understand, which comes with a subjective element. This is not what ChatGPT does. ChatGPT contains a computer that encodes knowledge in much the same way that any other computer or that a strand of DNA or a book does. It contains knowledge. Knowledge is encoded there. There's no understanding going on except by the people that are using it. I mean, this is also Goodell 101, right? Forget about like the incompleteness, just like basically any statement can be represented as numbers. And yeah, we have this whole intelligence discussion completely off the rails. Yeah. In terms of terminology. Not here in AirChat, but in general on Twitter, I guess. And that's not the only type of intelligence there is. There's the intelligence of like babies get born. They get born. They're biological entities. That is a whole level of also intelligence.

### 1h 24m

It's like species level to what it's like to be humans. And you just don't get that. Simulating Joe Rogan, I don't know, out for a drink with Howard Stern. That's the PG version I came up with. Because it is intelligent. It's super intelligent. It's super intelligent at doing book reports. And it's super intelligent at being a travel agent. And it's super intelligent at lots of super intelligent things that we can and cannot comprehend. It's crazy intelligent. But intelligence isn't all there is. Like, that's not what makes us human. And I'm not trying to be like the special butterfly. Like, there has to be something different about us. But there's so much to us that this compute, like, yeah, it's incredible. But it's like a thin sliver of the human condition. Yes. Well, one of the great difficulties is, of course, the very fluid nature of this term intelligence. And different people will debate all day long what they mean by intelligence. And they'll talk about the scale of intelligence, you know, from insects through to rats through to dogs and chimpanzees. And then, you know, we're the next thing up incrementally on that scale, that sort of gradually climbing linear kind of a scale where we're at the top of the heap.

### 1h 25m

And it stretches all the way back, you know, down through anything that's got a nervous system is intelligent on this account of what intelligence is. And so I kind of try and avoid that kind of discussion. Because then, yeah, you do get in. You fall into this trap of, oh, so the calculator is intelligent. So the computer is intelligent even though you have to program it. Everything on this account then has intelligence. So human beings aren't special on this account. And you go, well, so explain why we have a civilization. Explain why when I, you know, look outside my window towards the center of the Sydney Central Business District, I see skyscrapers and I see airplanes taking off from an airport. And I can look up into the sky and I can see satellites going over at night, all that stuff. But we have the same kind of thing just a little bit more than what the chimpanzee has or what the dog has. What's the difference then? Like, for instance, like, there's these things out there that you just, like, all you do is tell them, like, where something's going to cross and generally, like, what changes, like, rise over run. And it'll figure out where anything is, like, in that direction at any remove. It's called Y equals MX plus B. It's crazy intelligent.

### 1h 26m

Well, there's no point debating what the word intelligence means at that point because it becomes denuded of all meaning. It just means any kind of program that can cause any kind of behavior in any kind of system. And therefore, we have this gray scale. So what we like to talk about is this capacity to generate explanatory knowledge. And then people go, well, look at ChatGPT. That's what it's doing. Okay. The capacity to understand the knowledge which has been created. Oh, but maybe ChatGPT does that. Okay. It gets really hard to try and distinguish between these systems unless we're able to look at effects in the real world. And maybe we just have to wait longer until ChatGPT is building a civilization, contributing to the design of the next airplane that takes off. Figuring out new ways of launching satellites into space. All the stuff that we do, in other words. Not just able to carry on a conversation about preexisting knowledge. Which is, yeah, impressive. But still, I would say, an incremental step beyond what search engines have done since the beginning of the internet. It's a better calculator of that kind. A better linguistic calculator. Is it able to come up with better designs for the next series of mobile phones?

### 1h 27m

A more efficient internal combustion engine. Something to replace lithium batteries. The solution for fusion power. I could go on and on and on. Yeah, these are more or less kind of scientific type things. But that's where you get the innovation. Or, as Naval has already hinted at, come up with a business plan for a product that is going to blow the socks off people. Investors and consumers alike. That would be good. Okay, there. You have something that we can talk about. An effect in the real world. And we're not just interfacing with this thing at that point by a screen. But we've got something out there. Something physical in the real world that's having real physical effects that we can then talk about. As requiring the explanation of that, this thing must understand something about the real world. In other words, have a problem. And remember, all this stuff, it's trained on us. It is entirely trained on us, right? Like, we are the entire... We comprise the entire contents of its existence. Period. Full stop. So, the only thing that's cool... Well, what's funny here is that we're looking at this thing that all we've said is, like, just listen to everything we've ever said. And go figure. We find it convincing. Like, we're the only thing it knows.

### 1h 28m

But it is hilarious. And I know this has been shared elsewhere. That there's a certain universe where... What AI really does is make coders completely unnecessary. Now, I don't actually think this is going to happen. So, sorry, devs. But makes coders completely unnecessary. And it's going to be these, like, washed-up liberal arts graduates who, like, nerded out on Wittgenstein and Schopenhauer. And the contents of consciousness and language and word. It's going to be, like, these super obscure, dorky literary types. Like yours truly, I might add. That are going to inherit the Earth. And it's going to be awesome. We're all going to sit around and read Douglas Adams. It's going to be terrific. Well... I don't think coders will become unnecessary. I know you're speaking tongue-in-cheek there. But, of course, they'll evolve. In the same way that, you know, everything I learned about programming in Fortran and, you know, BASIC back in the day is completely useless now. Who's programming in Fortran? I couldn't even tell you. There was a lot of stuff that I learned about computers early on. You know, how to program in HTML. I suppose that's still useful for someone. But HTML has kind of gone the way of the dinosaur.

### 1h 29m

Because to build a website, well, you just use proprietary apps now. I suppose, you know, to get really fancy, someone's got to do the HTML. Do they? Still? But in general, anyone who's engaged in designing a website is using a website design tool. An app that will do it for you. And that kind of thing is only going to continue. And we can see that ChatGPT4 is already doing a great job of writing code. Not as good as a proper coder. But you can imagine. GPT5, 6, 7. What this kind of means is that what you really want. And what can never become redundant. Is the capacity to instruct the thing. The computer. In a non-ambiguous, clear way. And that will never disappear. Possibly, hopefully, it will become the case. That learning a programming language won't be that far removed from learning natural language. You'll be able to tell the computer what to do in natural language. But you better want to know how to be clear. So studying Wittgenstein and Schopenhauer.

### 1h 30m

Yeah, that's not going to help you with writing clearly in order to construct a computer. Reading Deutsch and Popper? I'd argue might. So it very much could be the case that the old argument for liberal arts. And writing clearly. And philosophy. And proper analytical logic. And being able to figure out how a flow chart works. And have some understanding of, let's say, pseudocode. Rather than any particular language. That could be, you know, the future. So to speak. That all variations of programming languages might become more and more. You know. Overlaid by systems that can do the hard work of the coding for you. But you've got to be able to come up with clear instructions for it. So it can translate it into the code. Which then gets translated into ultimately the machine code and the binary. Which, you know, some of us learned to do just because we're interested in it. Just because it's fun. At an academic level. But no one is going to. Very few people are going to practically engage these days with learning machine code. Unless you are actually writing the firmware for a processor.

### 1h 31m

Something like that. And in the same way. That, you know, if you're writing computer games of the future. I would hope that things are going to evolve to a point where you can come up with, you know, the next Skyrim 14. Without needing to code at all. But so long as you can interface with something like ChatGPT 14. You just give it the instructions of the kind of world you want built. And the kind of game you want. And it'll generate it. But you're still coming up with the ideas. It's the thing that is putting those ideas into code. And then constructing the game. But it could make certain forms of coding, yeah, redundant. But it will never make. The capacity to generate clear instructions redundant. Brett, Fortran is an incredibly powerful language. Thank you very much. I learned in Fortran. It's a lot of my electromagnetic field modeling in Fortran. Frankly, I'm convinced that. I'm actually pretty sure that the network analyzers that they use for microwave systems. Pretty sure those are all still programmed in Fortran as well. So Fortran's not going anywhere. It's durable. That's good to know. If everything else goes south for me. And things begin to fall apart. I know that.

### 1h 32m

I can dig up my old Fortran books. Refresh my memory. And there'll be a job out there. Programming microwave systems. For me. Perhaps. If we can code in Fortran. We just need a Coldball guy. Or girl. And we're good to go. Like, we will just dominate the world. I mean. Honestly, I don't think these fancy AI fuckers know what real power means. Power means, like, I shut down the IRS. That runs on Coldball. And Fortran. Just kidding, IRS. Just kidding. This is absolutely correct. Programmers are not people who have memorized a language and know the esoteric and arcane rules of how to use that language. Programmers and software developers are creative people who will use any tool at hand to make a computer perform complex things at the edge of their capability and the edge of its capabilities. So in that sense, the best programmers also know a little bit about hardware and microprocessors. They know about networking and bandwidth and transmission systems. Not because they have to write code for all those things, but because they really have to have good explanations of what's going on inside and around the computer to make it perform at its highest level. If anything. Now the bar has gone up.

### 1h 33m

Because you can program in natural language, there is no longer a barrier to entry between different programmers because one knows Rust and the other knows Python and the third knows C and the fourth one knows Swift. Instead, they're all in competition, but now based purely on their creativity and knowledge of the entire system. And someone who understands what the AI is capable of, what computers are capable of and what doesn't, and someone who understands how to logically reason with it and give it clear and unambiguous instructions will now be leveraged 10x even beyond what they were before. And yes, the high watermark for all programmers may have moved up, but the best of the best will reap even larger rewards than before. Brett is being kind. He did a takedown on these guys' definition of intelligence. And how they went about ascertaining and being impressed by intelligence on GPT-4. He did the takedown elsewhere in AirChat. Maybe he can dig up the link to it. But this kind of definition of intelligence where it goes across seven different potential things and it's very fuzzy reasoning basically says that they don't yet have agreement on what intelligence exactly is. And I just fall back on the test of intelligence. And I've quipped this before on Twitter. The only true test of intelligence is if you get what you want out of life. And so for a computer to demonstrate intelligence, it would be A, have to want something, and then B, demonstrate that it can go get that thing in an adversarial and competitive way.


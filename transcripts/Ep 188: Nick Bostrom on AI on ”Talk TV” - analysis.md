# Ep 188: Nick Bostrom on AI on ”Talk TV” - analysis

Original Episode: [Ep 188: Nick Bostrom on AI on ”Talk TV” - analysis](https://www.podbean.com/site/EpisodeDownload/PB13EAA98HS93N)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/ekhmsf/Bostrom_full_podcast9l8gg.mp3)

## Transcript

### 0m

Superintelligence was written by Nick Bostrom in 2014. It's been almost a decade now since that book was published. It was published two years after an article written by David Deutsch that I'll put in the next chit, published in Aeon magazine, titled Creative Blocks. That was in 2012 by David Deutsch and it was about the possibility of artificial general intelligence. David Deutsch had written The Beginning of Infinity some years before this. Developing work began by Karl Popper on conjectural knowledge. Epistemology, the explanation of how knowledge can be created, what entities can create knowledge, and this led David Deutsch to the important conclusion that people, a special case of which is the human being, another special case of which is the artificial general intelligence, or alien life, all of these things have in common the capacity to explain the world, in other words, the capacity to create knowledge. This means that people, and what a person is, is intimately tied to epistemology. And what knowledge can be created and how it can possibly be created is constrained by an understanding of epistemology. It's self-constrained by what it is possible to compute, because understanding what brains do, is a kind of computation.

### 1m

And so I read Superintelligence and was struck by a number of things, so much so that one of the first articles I produced on my website is this one right here, which was a critique of the book Superintelligence. So as I say, David wrote this article in 2012, one year after the publication of The Beginning of Infinity. And both the book and the article align on how it is knowledge is created, following Karl Popper, the conjectural nature of knowledge, the relationship between people and knowledge, people being universally capable of explaining, that which is explicable. And this follows from the universality of computation. It's a different kind of universality. But all it is is a denial of the supernatural. It says that the world is comprehensible. There's nothing incomprehensible in the world. That article, all about the possibility of artificial general intelligence, the difference between artificial general intelligence and regular artificial intelligence, which is now thought of as merely being narrow in the sense that it really only is able to perform a finite list of functions, tasks. It can't think like a person. It can't think like a person. But an AGI can, precisely because it is a person.

### 2m

Certainly in David Deutsch's conception of what a person is. Namely, a universal explainer. Deutsch admits, as would I, that artificial general intelligence could pose a danger. Just as your next-door neighbour could pose a danger. Just as a child could pose a danger, a teenager could pose a danger, people can pose a danger if they have the wrong ideas. An AGI is just like that. But it's not a special kind of danger. Although people argue that. But Deutsch explains, grounded in physics and epistemology and the possibility of creating knowledge, how it is that knowledge is created, in fact the only way that knowledge can possibly be created, Deutsch goes through this and prevents a vision of the world that isn't mired in concerns about existential threats, but rather an optimistic view about the possibility of creativity and how it is creativity that solves our problems. So I went into reading Superintelligence by Nick Bostrom with this background, thinking to myself, well, Nick Bostrom, an Oxford philosopher, author of this new book, Superintelligence, I was excited to read it.

### 3m

What would he have to say about this? Because I'd heard great praise for the book by luminaries such as Sam Harris, and thought, well, if smart people are saying that this book is really groundbreaking in the kinds of things that it says about superintelligence, and we should be really worried about this coming technological shift, possibly technological apocalypse, then I should certainly read this. And I read it, and then I listened to the audiobook, and I had a sinking feeling. Not about what Bostrom was warning us about, in terms of, the existential threat of AGI, but a sinking feeling that Bostrom didn't understand his own field of expertise. And this was a great revelation to me. Who am I to dare question an Oxford philosopher on philosophy and epistemology in particular? Well, who am I except someone who's read Karl Popper and David Deutsch on this, and was simply doing a comparison between what I knew to be the case from physics and epistemology and comparing it to what Bostrom had written in this book, Superintelligence. And so I was driven to write this critique. It was one of the first books I'd read. It was one of the first books I'd read. It was one of the first books I'd read. It was one of the first books I'd read. It was one of the first books I'd read. It was one of the first books I'd read. It was one of the first things that I ever put on my website.

### 4m

I wrote it almost straight after the book itself was published, Superintelligence was published. And as I say, I read the book multiple times. And in particular, I listened to the audiobook, and I found that disconcerting. Whoever the reader was, sounded for all the world like a superintelligence, I would imagine, in this sort of mechanical, kind of creepy way of reading. I was just immediately struck by how terribly naive the epistemology was, and the philosophy was. I thought, Bostrom, this great philosopher, held up, as being the expert on superintelligence. Simply misunderstood how knowledge was created, what people were, and why things were dangerous. Although we agreed on certain fundamentals, we disagreed on others, and this led to Bostrom reaching conclusions that I thought stood in stark contrast to what physics and philosophy were telling us about reality. But at the time, ten years ago now, the publication of Superintelligence by Nick Bostrom, it was just a curiosity, intellectually speaking. The domain of a strange subculture of nerdy tech types concerned about this kind of thing,

### 5m

and worried philosophers who hosted podcasts, and people like me who consumed all of this stuff. I didn't really think it was having an effect on the culture. I thought, well, one day it might, but before it does, we're going to figure all this stuff out. In particular, David Deutsch's ideas are going to get out there, and be able to combat, in the culture, this growing pessimism about technology, which is what it is. No different to... People's concerns about the printing press, automation, factories in general, whatever it is that supposedly takes away jobs from people. But of course we know that every time ten jobs are taken away by automation, fifty more are created in order to produce the thing that does the automation in the first place, to come up with creative new ideas, taking people away from the drudgery of the work that the robots now do, or the factory does, and giving them jobs in creative industries, programming and writing and all that sort of stuff. So it seems... And obvious, to me at least, that the lesson would be learned quickly, simply by observing the world.

### 6m

This, of course, is an empiricist's mistake. People are not persuaded by just looking at reality. Instead, they are persuaded by ideas and explanations. If they're convinced by bad ideas and explanations, then observations won't sway them, even if those observations stand in stark contrast to what they're thinking, because what they regard as the explanation of the world is for them a good explanation of the world, and accounts for... ...the observations they see out there in the world that would otherwise contradict what they're thinking. In other words, what I'm saying is, if you think that technology is going to take over the world and take everyone's jobs, the fact that people keep experiencing gainful employment, we keep on producing more jobs, is not a refutation of your idea, because your idea is that eventually all the jobs will be lost. Eventually the disaster will come. So you're not refuted by the trend of increasing employment and how things are getting better for everyone seemingly consistently. But as I say, these were all concerns about a certain kind of nerdy, fearful, somewhat angry subculture of people. Talking about the end times.

### 7m

A little bit crazy sounding. Who would take these people seriously? Well, as I say, podcast hosts, that kind of thing. But not exactly the mainstream media. Not yet. They were too distracted by climate change. Concerns about global politics. They weren't concerned about the coming AI apocalypse. Not yet, anyway. And I didn't really think it was going to happen. As I say, surely there would be a collision between people like Nick Bostrom, who got all the philosophy and epistemology, wrong, and therefore the conclusions about what AGI would be like and the possibility of so-called superintelligence. I thought that that would all be refuted by better ideas eventually when the time came, if the time indeed came. But that isn't what has happened. Nick Bostrom's ideas undergird all of what is being said now in the media and on Twitter and driving people now who have even larger profiles than even Nick Bostrom. The reason Sam Harris says what he says on his podcast about the dangers of AI is because of Nick Bostrom. The reason why Elon Musk says what he says about the dangers of AI

### 8m

is because of Nick Bostrom and Eliezer Yudkowsky. But the reason why Yudkowsky says what he says is in large part because of Nick Bostrom. These people talk about Nick Bostrom and his philosophy because he's an academic. He's the expert. He's an Oxford professor and philosopher. If you believe in the authority of experts, then what greater expert could you have than a professor of philosophy specialising in this field? And not only that, but at possibly the most prestigious institution on the planet, Oxford University. It hasn't remained in this angry, fearful, geeky subculture. It's now out there and not merely on social media. Now it's reached the mainstream. And now Nick Bostrom is definitely out there on mainstream media telling people about the end times that are coming. Let's just hear an example, a very recent example, of what kind of effect this is having on the zeitgeist, the genuine zeitgeist, the way people are feeling about this issue, what the media is thinking about this issue, which means what governments are going to think about this issue, politicians are going to think about this issue,

### 9m

and therefore what legislation is eventually going to come, what regulation is going to come. And that is what is going to affect everyone in a really detrimental way. But before we get there, let's just listen to the people in their own words. But it does raise this important question. Is AI a threat to the jobs of almost all of us? Everyone from doctors to teachers could see themselves made redundant by new technology that's faster and more intelligent than the average human. But will all of it result in this dystopian future where we'll be slaves to AI overlords or a more utopian world freeing us from the shackles of 9 to 5, living free to pursue more leisurely activities? Normally this program would be hosted by Piers Morgan, but for whatever reason tonight he's not there, and so there is a guest host. And she has just made it clear that AI could be a threat, so she's presented the dystopian view. But the only alternative to that, apparently, to us being, as she says, slaves to AI overlords, sort of got a wry smile on her mouth as she says, they're still not taking that quite seriously. That's fine. But the alternative is freeing us from the shackles of 9 to 5. Okay, that's good. Not all of us are in those shackles anyway. But living free to pursue more leisurely activities. Well, hold on a minute. The alternatives are not, slaves to AI, or you're just on a permanent holiday of some sort. No, there is important work to do.

### 10m

The important work is creativity and solving problems. That won't change. That's the real work. Okay, so some people don't want to call that work. I do. I want to say that's what actually work is. Never mind laborious drudge work. That's not what human beings are about. What we're about is creative work. Fun work. That won't change. And in fact, that'll only become more important to be able to figure out what to tell the AI, what they're going to do, what they're going to be able to do. Now, insofar as it's AGI, or they're going to help us, they're going to be people as well, also using AI. Here to discuss all of this, AI skeptic and director of the Future of Humanity Institute, Oxford University, Professor Nick Bostrom. Nick, thank you ever so much for making time to speak to us. And, you know, I know we can oversimplify this and talk about sort of robots taking our jobs. It seems to be that's what the media most grasps onto. But give us a sense of the power of AI, if you could. Well, I think we are just at the early stages of what might become the biggest transition in human history, the arrival of the machine intelligence era. Long ago, we had sort of automated human cognitive labor, first through draft animals and then with steam engines and so forth. And now we're starting to automate human cognitive labor.

### 11m

How dangerous is that to replicate cognitive behavior? So Nick Bostrom has been introduced by the host as being an AI skeptic. Of course, he's not a skeptic about the possibility of AI. He's just a skeptic about whether or not it's going to be beneficial for humankind. Nick Bostrom is said there to be the director of the Future of Humanity Institute at Oxford University. So these academics that talk about the end times, the modern version of Armageddon, the apocalypse, the prophecies and revelation, that kind of thing, but always with a bit of a technical spin on things, they have these institutes. I don't like to use this word, but I'm going to. It's a grift. It's an academic intellectual grift. It's a way of listing all the ways in which civilization is going to end so that you can charge consultancy fees to government and business in the hope that somehow or other your advice to them about the ways in which the world might end is going to somehow be of benefit to them in forming policy. They all do it. So Nick Bostrom has the Future of Humanity Institute. Will McCaskill of the Effective Altruism Movement, also Oxford University, he works at the Global Priorities Institute.

### 12m

And so again, the idea here is let's talk about the future and what we should be prioritizing. And of course, all these people have the same prescriptions for what should be done. More government, more authority. We need to get together as a globe and come up with a unified approach to these things. Max Tegmark. Max Tegmark has the very friendly-sounding, positive-sounding Future of Life Institute that he's the president of. Again, it's the same idea talking about all the ways in which the world might end and how he, as a cosmologist, can give you advice on how to avoid the end times. They're all doing the same thing. It's all the same kind of grift. Here, allow me to use my expertise as a physicist or a philosopher to tell you what the future is going to bring. I'm a prophet. I can see what you, the plebeian, without my training and my credentials, I can see and understand better what the future will bring. But here we're talking about Bostrom and Bostrom in this particular interview here. The interviewer there has talked about how AI, the standard stuff, about how AI is going to take away our jobs and we're going to be all left unemployed. OK.

### 13m

But also at the end there, concerns about how dangerous it is to replicate the cognitive behavior of human beings. So it's all couched in danger. Well, there wouldn't be a media story if we didn't have fears. It's all driven by fear, after all, for much of the media. It's not so much fact-based as fear-based. What are the concerns? What should we be worried about? Not what should we be hopeful for and what exciting things lay ahead when it comes to technology. Apparently, that's not marketable. Let me not delay any longer. Let's listen to what Nick Bostrom himself has to say about this. Well, I've long argued that this development of machine general intelligence, and as I think shortly thereafter, machine superintelligence, will be associated with significant existential risks, like threats to the very survival of the human species. Also, however, I think if we get it right, it presents tremendous opportunities to solve a very wide range of different problems that have plagued humanity throughout our history. So he's long argued, and he certainly has, that machine superintelligence will be associated with significant existential risks, like the very survival of the human species. He also thinks that that will come soon after machine general intelligence.

### 14m

But what do we know about what general intelligence is? What general intelligence is, it's called general because of its universality, its capacity to understand anything that is comprehensible, and what is comprehensible is the universe. The laws of physics mandate their own comprehensibility. I can get into that, but it follows both from the universality of computation and the universality of the human mind. This is what general intelligence is. We possess it. You can't be more universal than universal. And if you want to speed up the rate at which you can do calculations, then you've got the pocket calculator or you've got the computer. If you want to increase your amount of memory, then you can write things down on paper or store it in a computer. These things augment our capacity to be universal, to make our capacity to think more quickly, to store more information than what we otherwise would in our brains. We can store it out there in the world, but we don't become more universal than universal. Why does any of this matter? It matters because it comes from epistemology and understanding of what epistemology is, the creation of knowledge, how knowledge can be created, what universality is and therefore what the meaning of general intelligence is. And if you understand that, which follows from physics and epistemology,

### 15m

then you know there can be no such thing as so-called superintelligence. It can't be more universal than universal. We've already got that, the capacity to understand anything that's understandable, and everything is understandable. I've made videos about this before. I've gone to great lengths to try and unpack what David Deutsch has said about this in various places. The simple fact is that we are already at the peak of intelligence. I know people deny this, but they deny this just from an argument from incredulity. They can't believe it. I don't believe how it could be otherwise, but if you understand the relevant physics and the relevant epistemology, you understand there can be nothing more than universal. That's what universal means. Everything in the repertoire that could be understood can be understood by us in principle. What people think about superintelligence is that it's something more than this. In other words, it's an appeal not to superintelligence, but to the supernatural, something that flies free of the laws of physics. But as rational people grounded in realism, we should reject this. Nick Bostrom is not doing this. His idea of superintelligence is based in the supernatural. His idea of superintelligence is based upon a false epistemology, a long-refuted epistemology,

### 16m

the modern incantation of which is Bayesian epistemology. It's all wrong. It was refuted by Popper. It's been refuted again by Deutsch. I've explained these refutations in various places, but people are still committed to this supernatural idea of intelligence. As if, although you wouldn't believe in God because that's ridiculous, that's supernatural, but you will believe in the God made out of silicon chips, the superintelligence of the hair. And that, because it's basically got all the qualities of God, except that it's evil, it's a threat to the very survival of the human species, as he says there. But let's add to all of this. The very fact that Bostrom, like Tegmark, like McCaskill, like everyone engaged in this kind of concern about the future and is very willing to get on TV as quickly as possible to tell us about the future, they can't help but engage in prophecy. Even when he's trying to present a positive picture, which is just sort of a throwaway line, his central message is, of course, the threat to the very survival of the human species, which is a prophecy about the future, uncoupled from science. Even when he says, quote, I think if we get it right, it presents tremendous opportunities to solve a wide range of different problems

### 17m

that have plagued humanity throughout history, end quote. Yes, that's also a claim about the future. So we can't help but make this claim about the future, that AI is going to solve a very wide range of problems, but that's only because we are actually the thing that is solving it. It's just another tool. Science itself is the thing that presents opportunities to solve a very wide range of different problems. Computers present the opportunity to solve a very wide range of problems. Most important of all, human beings do this. AI is just another tool. That's all it is. And we can use tools for creating or for destroying. That's up to us, but not the AI itself. It doesn't make decisions and choices. Insofar as it can make genuine choices, insofar as it can construct knowledge of its own, that makes it not a silicon system that performs tasks that we should be afraid of, but rather another person we can collaborate with. And this is what Bostrom doesn't understand. And he doesn't explain that here because he doesn't understand it. He's fixated on telling the world about the dangers. And why? Let me be clear. I think it is largely because there is money in this.

### 18m

Telling people about the dangers means they're going to turn to you for solutions even if you don't have any. And therein lies the grift, the consultancy fees, the demand to appear, especially on newscasts. The media wants to hear about the frightening prophecies of the future. People tune into that stuff. It's like watching a disaster movie. You have argued that the sentient machines are a greater threat than climate change at some point. Do you stick by that? Yeah. That's a good assessment. I think both in terms of time scale, there's a lot of uncertainty about that. But in the last several years, progress in AI has been really quite impressive. And also about the risks, if and when we do reach human equivalence and then super intelligence. It's just something completely unprecedented. We've never developed super intelligences before. And I think we might only get one try on this. We need to get it right on the first shot. Because once you have unfriendly antagonistic super intelligence, it might not be possible to put that genie back into the bottle. So we've got to get it right at the first try. And it looks quite challenging to do that, just to make sure that it's aligned and does what we actually want it to do. So she begins there with, quote, You have argued that sentient machines are a greater threat than climate change at some point.

### 19m

Do you stick by that? End quote. So that's her question. They love this, don't they, the media? You want a threat. You want the concern. You want the worry. This is the media. Just pointing out problems. For so long, they've been fixated on climate change. This has been the reason why we're going to need bigger and bigger government, more and more enforced collaboration at the highest levels of the United Nations in order to have global rules and authority. Because we're going to need this coordinated effort in order to fix the climate, fix the planet, fix the world. OK, so let's say that doesn't pan out for them. Let's say that people start to question the narrative about climate change. Might be a little bit too much of that going on. What do we have as a backup plan? Well, maybe this whole thing about AI. Maybe it's even better. Maybe it is a greater threat than climate change. And therefore, well, it's even more urgent that we have authoritarian-type governments who can save us from the impending doom. Bostrom, in answer to this question, agrees that, yes, indeed, it could be a much worse risk than climate changes.

### 20m

And he thinks we'll only get, quote, one shot. We'll have to get this right on the first shot. Because once you have unfriendly antagonistic superintelligence, he says, it might not be possible to put that genie back into the bottle, end quote. In other words, some problems are not soluble. There are some things that, once they happen, you can't fix them, no matter how much creativity you bring to bear. So if the AI goes wayward, that's it. You can't persuade it. Because apparently it's not creative, so it can't understand your argument. But on the other, you also can't switch it off because it can outthink you, presumably because it is more creative than you, or something like that. There's this contradiction at the heart of what Bostrom argues in superintelligence, and I'll go through that in my review, my critique of what he says. Not only is the epistemology false, which is what was so disappointing for me, reading someone who's a professional philosopher, but the argument itself just didn't stand up because of this internal contradiction. That on the one hand, the superintelligence is a superintelligence because it can always outthink you. It's always more creative. It's a faster thinker. It's a better thinker. But on the other, it's not a more moral thinker. It can't improve morality. It's dead-set committed to the destruction of the human race, for example.

### 21m

Or it's just not creative at all in the sense that it is dogmatically committed to achieving its task by turning the world into paperclips, let's say. And it can't be swayed from doing that. Now, you can't switch it off, Bostrom says, because, well, it's going to outthink you. As soon as you think you can switch it off, or you can shoot it with a gun, or whatever the thing is that you think you can do to stop it doing what it's going to do, like turn the universe into paperclips, it is a better thinker than you. By the measure of, no matter how creative you are, it's already thought ahead, ten moves ahead. But, although it is super creative, it can't think, why am I doing any of this? Why am I turning the universe into paperclips? This is the contradiction at the heart of his entire worldview on this. It makes no sense. And yet people listen to him because, well, he's an Oxford philosopher. He's the world expert on this. And yet, this is a very simple error, a very simple contradiction. Is the thing creative or is it not? If it is, we can persuade it. If it's not, we can switch it off. You can't have it both ways, but he does. He absolutely does. All throughout. All throughout superintelligence. And, you know, he says things, as he says right at the end there, like, it's quite challenging to do that. Namely, put the genie back into the bottle, stop the AI from becoming wayward. And he says, it looks quite challenging to do that, just to make sure that it's aligned and does what we actually want to do.

### 22m

Want it to do, presumably. End quote. So he wants to make sure. And he can't make sure if it's an AGI. But this is the way he thinks. This is what he wants from knowledge and epistemology and from people. Certainty. He wants to be sure. He endorses justified true belief as the conception of knowledge. Bayesian prediction. All that sort of stuff. It's all a complete. Nutter error. This, from one of the top philosophers on the planet, supposedly, making pedestrian errors in simple critical thinking. Okay, so error is everywhere. But this is supposed to be the world expert on this stuff. It's almost like what I compare it to is one of the top physicists in the world not understanding why Newtonian physics is wrong. Not realizing there is this thing called quantum theory. Much less general relativity. He's stuck in the past. Not realizing the contradictions in the worldview, the observations out there. Not only refute what he's saying, but there's better explanations. That's what a physicist who would argue for. Well, look, Newtonian gravity, it works and it's true. Well, this is what Bostrom is doing with his view on epistemology and AGI and people and being certain and being sure and all this stuff.

### 23m

It's the same level of error. It's disappointing. It does sound terrifying, this idea of superintelligence. I've seen it described as godlike superintelligence. I remember having beers with a friend a few years ago and he was saying, I think I'm most scared about supercomputers. And I thought he was off his rocket. This was probably about seven years ago. He was right back then. How far along the curve are we? And how fast is the train going in terms of being able to rein it in if we need to? So the interviewer there admits now to being terrified of superintelligence. Some time ago, she had beers with a friend who seemed to have knowledge about this stuff and he was scared. But she thought he was off his rocker. So she thought he was crazy at that time. But now she's being persuaded, presumably by people like Bostrom. She's hearing about it more. So she's being persuaded by the consensus, which is why I say that the pessimists are winning the argument. Here he is, a journalist, not an expert in the field, thinking that it was a crazy idea some time ago that the supercomputers were terrifying. But now she's... She's being persuaded. And she's asking for the expert who has predicted this stuff all to go wrong, what we should do about it, which is also a category error. Let's say if Bostrom could predict what was going to go wrong with superintelligence, does that then give him the moral authority to say what should be done about it?

### 24m

How do you get an ought from an is? Why would an expert in superintelligence know what we should do about superintelligence? That's an ethical question. This is just another interesting philosophical error that tends to go on in these conversations as well. It's also interesting that when she relays the conversation she had with her friend, which happened seven years ago, she says of him that although she thought he was crazy at the time, she says now, How does she know that he was right back then? Why? Again, because she's being persuaded now. But why has she been persuaded now? Because so many experts, so-called experts, are converging on this idea that superintelligence is terrifying. And the news absolutely loves it. They love a terrifying story. People tune in to listen to Tales of the Apocalypse. But it's becoming more and more serious. They're taking it very seriously. But should we take it that seriously that it's all going to go off the rails? Perhaps. But is this also terrifying to children? Is this terrifying in the same way that climate change has been twisted and contorted into this existential threat? I think so. And this is why I think that, again, the media is just like a dog with a bone.

### 25m

They pick one side of the argument, present that side of the argument. That becomes culture. That dictates policy. The trend is going really fast. I mean, everybody has seen these recent large language models with ChatGPT. It seems like almost every other day there's a big new release. The image generation models, everything you see in this picture is artificial. The virtual backdrop actually generated by one of these DALI 2 image generation. I'm not even sure whether I'm real or artificial. And it looks like these current algorithms generalize very well and scale. So you pour in more compute, you pour in more data, and they just seem to reach higher levels of performance with as yet no clear end in sight. So Bostrom begins there by speaking about how the trend is going really fast. And on all of those facts, we can absolutely agree. At the end, though, he says, so you pour in more compute, you pour in more data, and they just seem to reach higher levels of performance with as yet no clear end in sight. Now, on the one hand, of course, he's true. In the trivial sense, he's true. These systems will keep getting better and better and better. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah.

### 26m

Yeah. Yeah. Yeah. Yeah. Yeah. people. Next clip is a little over a minute, and this one has Bostrom telling you, or providing you an insight into his theories of economics. Some of which I can agree with, but other parts of which, well, it hints at this kind of authoritarian approach to the way in which technology in particular can be created in the environment in which he thinks innovation might actually work. There was an open letter being spread around the technology and the AI community. Elon Musk was one of the signatories saying we need to pause the development of AI in order to assess some of these risks. You can see that we can call on all AI developers to stop this immediately. Pause for at least six months. He's asking for the training of AI systems more powerful than GPT-4. I believe that Bill Gates was actually not one of the signatories of those, and he has made the case that we do need to keep the development of AI up in the West,

### 27m

because otherwise we're going to fall out of the race when it comes to competing with China, and obviously there could be risks involved with that. Yeah, right now, the Western, particularly certain large Western tech companies, I think have a lead. Things are quite competitive in this space. Also, in terms of building the computer hardware that is needed to run these large AI models, the key producers of that are in the United States, in Taiwan, and in South Korea, and recent export regulations have tried to stymie the sales of AI chips into China, so that might make it harder for Chinese developers to keep up with the cutting edge. I think, ultimately, machine superintelligence should be developed. I do think during the key parts of the transition, though, it would be wise to be able to do it carefully and not to have a sort of hyper-competitive situation where there are plenty of different companies that are each vying to get there first, and whoever decides to make some extra precautions or to check their system carefully just immediately falls behind, and the race is won by whoever is willing to take most risks. I think that would be a bad situation. So, it seems as though he understands that the West, at the moment, Western tech companies, have the lead over China. I tend to think that they will always have the lead, and the reason why they will always have the lead is precisely

### 28m

because of the different cultures in the two systems, politically speaking. If you have an authoritarian political system which stifles creativity in one area, it cannot but help to stifle creativity in other areas. It will always slow down, make people concerned about saying the wrong thing, doing the wrong thing, asking the wrong question, for fear they and their family are going to be punished. This slows down innovation. This slows down innovation across the board, scientifically and technologically. If you don't have proper free speech, you can't discuss ideas, and you might very well say, well, it's only curtailing free speech in this area but not that area. That's not the way any of this works. It really isn't. If people feel as if they can't speak freely, then they won't, and in fact, it goes deeper than that. It goes deep into the inexplicit ways they think. They think that they can't even think about certain things because they stop thinking about certain things in case they accidentally say certain things. Smart people avoid getting into trouble. This really does happen. However, everything I said there is not perfect. There's no guarantee that people who are allowed to think freely will always, in all cases everywhere, out-compete, out-think people who are in some way living under tyranny. We are the good

### 29m

guys, and we want to keep ahead of bad regimes who might do bad things with new technology. We have an advantage, but we want to keep it as well, and the advantage that we have exists precisely because of liberty, freedom, free speech, free trade, competition. It's competition and free trade which allows us to have this advantage of getting ahead of places that don't have that. But Bostrom speaks pejoratively about this entire system, speaking of it as hyper-competitive, where there are 20 different companies, he says, that are each vying to get there first, as if that's a bad thing. This is a good thing. This means that we can get there first because we do have competition, whereas in China they have one way and one approach. He concludes his remarks there by saying, quote, and whoever decides to make some extra precautions or to check their systems carefully just immediately falls behind, and the race is won by whoever is willing to take most risks. So I think that would be a bad situation, end quote. Yes, indeed, and this is the escape of the authoritarian. This is the appeal to pessimism. This is the appeal to top-down bureaucracy, that we shouldn't be taking risks. It's the precautionary principle. That which hasn't been tried before comes with risks, so let's stick with what we already know.

### 30m

This is Bostrom's foundational philosophy. Whenever you read about it in other areas, whenever he's talking about any other kind of global existential threat, he talks about it in terms of the precautionary principle. Unless we know that something is sure to be fine, then there's a risk. Of course, there's a risk, but it's also the only possible way to solve problems and make progress. It's the only way to stay ahead of people who would do us harm, because those people come from more static societies. They have memes which prevent the means of error correction. As soon as you start saying, look, taking risks is a bad thing, that's an anti-rational meme, because once you start believing it, you cease to take as many risks, you cease to make progress as fast as you could have, you quickly become static in your outlook on civilization, and if everyone in your society starts to think that way, then you've got a static society. So I agree with him. By whoever is willing to take the most risks. But I take the opposite lesson from this. I think this is a good situation. It means we have the advantage, and we need more competitors. We do not need to take extra precautions, as he says. We need to understand what we're doing, and then put in place restrictions where we know they're required. In other words, don't go putting your completely

### 31m

untested AI system in charge of the launching of nuclear weapons. Always, forevermore, have a human being involved in that decision. Never fully automate your hugely destructive weapon systems. Never do that. That's just an example of the kind of thing I'm talking about. But when it comes to making progress in this area of artificial intelligence or anywhere else, we want to be the people who are taking the most risks. The most risks as in coming up with the bold conjecture, the risky conjecture, the idea people haven't thought of before. That's what we want. We want multiple companies, multiple people, lots of different thinkers, all thinking different things, trying out different things, taking risks. Taking risks is a good thing. But maybe, of course, this is a subtle difference between an American way of thinking and a European way of thinking. Or perhaps I should say, the British Enlightenment way of thinking, and the European Continental Enlightenment way of thinking. So having some ability to coordinate, maybe to slow down for a few months, I think that could be a positive. Yeah, ideally, you'd want it to happen in concert. But of course, in the real world, companies compete and countries compete. And this idea of having one of the most consequential

### 32m

technological impacts on Earth, as you sort of laid out at the beginning of our conversation, just how severe it could be. And all these decisions being made by a small handful of tech companies, private companies. Don't regular people deserve a say in this? So there we have it. There's his prescription. And as I say, having read a lot of Bostrom's work over time, his prescription for these things is always the same. Having some ability to coordinate, maybe to slow down for a few months, I think that could be a positive. Maybe slow down for a few months, that could be a positive. It always comes down to some top-down authoritarian committee slowing things down. And this is why Yudkowsky and all those other people talk about slowing things down, pausing technological progress, knowledge creation. They're the pessimists of old, garbed in techno-speak. Don't let it fool you. Just because they happen to have the capacity to have the conversation about AI in terms that technically proficient people understand and appreciate, doesn't mean their ideas aren't from a bygone era, philosophically. They're not from a bygone era. They're not from a bygone era. Don't trust people is the message. Slow things down. We can't be trusted. We don't know. The average person is too stupid, et cetera, et cetera, et cetera. This whole idea of maybe slowing down for a few months, that could be a positive. But a moment ago, he was just talking about how we're competing with the Chinese. Are they going to slow down? The one thing that will guarantee us

### 33m

losing is slowing down. And by losing, we really do mean losing. We mean that the authoritarian dictatorships will end up producing the AI that can do stuff that is actually going to be able to cause us damage in terms of security. At the end of that little snippet I just played, he says there, quote, Translation, shouldn't the government be in charge and not private companies? Shouldn't the collective be in control and not the individual? I want the small handful of tech companies to be in charge of this, the private companies, because they know what they're doing. They know what they're talking about. I do not want politicians, and philosophers in this case, in charge of this stuff when they don't know what they're talking about. Bostrom doesn't know what he's talking about when it comes to this particular philosophy. This particular area of philosophy, the one that he seems to have spent most of his time on, he's got completely the wrong end of the stick. And if he hasn't, then he's completely ignoring the other side of the argument, which I'm happy to say people on our side of the argument are not doing. We are taking the time to try and understand what the prevailing view is, and my goodness, isn't it prevailing? Here he is. Here he is on mainstream media telling the world about how dangerous this stuff might be, but really hinting

### 34m

at how dangerous it will be. That's the message that's coming across here. It's not so much couched in, oh, maybe, you know, it could be really good. No, no, the message that's coming through is it is terrifying. The casual viewer and listener to this kind of thing walks away from this message with the idea that it is absolutely going to be a catastrophe and we need to slow down. These people are all over every podcast that's out there right now. They're seeping into the mainstream media. I kind of suggested, I guess, this would happen soon. And here's one of the first examples I've seen of him getting onto the mainstream media. I've seen a few others as well, but this one was particularly telling. They're winning the argument, they're going to influence policy, and I fear it's going to influence policy for the worse. What can be done? Well, let's hear the end of the conversation. Yeah, I think the development of machine superintelligence is really an issue, not just for one company or even one country. It's an issue for all of humanity. We are all in this boat together. Like, if it goes badly, we're all doomed. And if it goes well, I think we should all, you know, have a slice of the upside as well. So it's an issue for all of humanity. Yes. So what he hints at there, what is not being said here, but I know because I go to his website, I read about his policies on this. I've heard him

### 35m

talk before. I've read his books. When he says that this is an issue for humanity, not just one country or one company, what he wants is a United Nations style coordination of this stuff. A very, the most top-down bureaucracy you can think of to be in control of everything. Why? Because it's existentially threatening to the globe. It's for all of humanity. In other words, it's kind of like global warming, where you need this coordinated effort of all the countries around the world. You need to take it away out of the hands of democracies and have the philosopher kings tell us what to do. It's that important. We can't leave this to the decision making of the regular democratic process. We need the experts to tell us what to do. And that's because it's an existential threat to all of humanity. That's how you get global government. You have to hold something above the heads of all the plebeians, the people, too stupid to understand the existential danger. You have got to hold something that is going to be threatening of civilization before they will give up their democratic rights. So you have to tell them the robots are coming to kill them. Because global warming is a threat to all of humanity. Global warming hasn't quite worked yet, or at least it's not fast enough. We can't have our global authoritarian government until these people are convinced they need such a thing. So this is what he's arguing there. But although he says this is important because it could all go badly and then we're all going to be doomed, so we need the global government for that. But on the other hand, what if it all goes well? Well, what he says there is, if it does go well,

### 36m

I think we should all have a slice of the upside as well. End quote. Well, of course. And we will, as we all have a slice of the upside of ChatGPT right now. OpenAI is giving away the stuff for free. Sure, there's a paid version, but people can use the thing for free. How wonderful is that? We do all have a slice of the upside, but that's not what he means. What he means is a form of socialism, a form of communism, where whatever money might be made by this thing should be paid to the government and then evenly distributed to everyone under his global, utopian, communist, authoritarian, ideal government system that has nothing to do with democracy and has everything to do with people like him deciding what benefits can flow to which people. That's what he means by we can all have a slice of the upside as well. He doesn't mean that we just get it via free market capitalism, which is how you actually all benefit. We all benefit from cheaper and cheaper mobile phones and smartphones. How? Not because the government designs the thing, but rather because companies compete to make the price go down and to make the quality go up. And that just continues to iterate over time. And so you get constant improvements and constant price reductions. That's what happens with free markets. That's not what he's talking

### 37m

about. I think this conversation will need to, and I think it will, broaden out. Policymakers are getting a lot more interested. I think people are less desperate to get first-hand experience using these systems and seeing how they get better so quickly. I think that will focus a lot of attention on this over the coming months and years. I think you're right in saying that everyone's experimenting now. A lot of people know what ChatGPT is. I think it's a great way to see the power of this. Hopefully, we'll find a way for us not all to be doomed. Nick Bostrom, Director of the Future of Humanity Institute, thanks so much for joining us this evening. So he says there that policymakers are getting a lot more interested. That's what he wants. So he wants this because, and how does he know this, by the way? He knows this because they're contacting him. They're contacting him at his institute to ask for policy advice. So this is what he wants. He wants to be able to shape a policy. He wants to be able to not merely control the narrative out there in the public, but rather, that's just a means to an end, because then he can control what the governments and the policymakers are thinking about this stuff and therefore actually get policy enacted. Now, is he genuine about all this? I think that he is, absolutely. I think he's mistaken. This is why I'm animated about this. Errors can cause very bad things to occur in the world. I don't think he is an evil person by any stretch of the imagination. I think he's just making catastrophic errors, but unfortunately, they've been made before. All of these ideas about the

### 38m

end times, pessimism about people, they lead like a straight line into authoritarianism. They lead to collectivism. They lead to tribal thinking. They lead to the notion that you need to have a philosopher king, someone who's smart like him, telling everyone else what to do and how the world can be saved. This is a dangerous way to think. It doesn't work. What works is allowing people liberty and freedom to think of their own solutions to things, and as the problems arise, those individuals will solve the problems. Insofar as there are large problems, people can freely associate and coordinate in that way. They don't need a top-down, enforced authority to tell them to do this. When the problem is obvious, we get together and we solve it together. And again, he ends with the contradiction at the heart of all this. He sees there, and he says, people are getting first-hand experience using these systems, seeing how they get better so quickly. Exactly right. They're getting better. Better in two ways. One, they're able to do their job, in a much more proficient way. ChatGPT can have better and better conversations. But better also morally, that this is a wonderful thing, and it's making the world a better place overall. But he keeps on insisting that this will come to an end, that eventually the moment will come where they cease to get better morally, and it all flips in an instant. So these things are literally

### 39m

existentially threatening, a danger to all of humanity. They go from being useful, useful, useful, better, better, better, more wonderful, more interesting, more unifying of people, more able to enable collaboration between different people, enabling the people who have less facility with writing to have more ability to write clearly. Name your skill, the AI is going to be able to augment your skill. All of this is wonderful, and it's going to make things better and better and better, until, until a moment comes where, like, I don't know, the melting point of ice. Suddenly you just hit that particular phase change, and everything goes from getting better to suddenly being the worst thing possible. Existential threat. Catastrophe for humanity. And there's just no reason to think any of that. None of that follows from anything that he says. His epistemology is wrong. He doesn't understand conjectural knowledge. His philosophy is wrong. He doesn't understand what a person is. He doesn't understand what a person is. He doesn't understand what a person is. He doesn't understand what an AGI is. His morality is wrong. He wants to control people. So his prescriptions are wrong. This top-down approach. It's wrong from the bottom to the top and every angle in all directions. This is the guy that the pessimists turn to. The other people, even if

### 40m

they're not aware that the reason why they're scared of superintelligence, scared of artificial intelligence, the reason they're scared is because he started it. Okay, so he's riffing off other thinkers before that, but he wrote what is really regarded as the gospel, superintelligence. Echoes of that I hear. Echoes of that book I hear throughout these discussions. Even if they don't reference him, that's where it's coming from. They're distilling out all of the stuff that he says there in superintelligence. I would urge people to read it, especially people who are familiar with Deutsch and Popper, to understand the other side of the argument and also to realize the absolute poverty of thinking and reasoning that has gone into that book and into that entire side of the conversation. We need more optimism about this because I fear it's not going to be an AI apocalypse. It's going to be an authoritarian apocalypse if these people get their way. And that will be more destructive. And worse for humanity than any kind of technology that will ever be imposed upon anyone. The political prescriptions always and everywhere have been worse for humanity than anything technology and the innovators have brought.


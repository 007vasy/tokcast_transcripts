# Ep 81: "The Science of Information" (Chiara Marletto's "The Science of Can & Can't" (A prelude to Ch 3)

Original Episode: [Ep 81: "The Science of Information" (Chiara Marletto's "The Science of Can & Can't" (A prelude to Ch 3)](https://www.podbean.com/site/EpisodeDownload/PB10AA3145ZMNI)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/hzc4na/Can_and_Can_t_Episode_4_podcast7ejk1.mp3)

## Transcript

### 0m

So welcome to TopCast and to episode three of the Science of Canon Cards called Information. And this first episode is just going to be an introduction to the chapter rather than me doing any readings of the chapter. And I'm going to call this the Science of Information. I'm going to go through some of what we know about information so far without really concentrating too much on constructor theory or the new material that is in this particular chapter. No readings from the Science of Canon Card today, but I'll have another episode out very, very shortly, which does contain the readings from... this chapter. It has some really interesting insights about the link between physics and information.

### 1m

These links have been made before, or I should say, links between physics and the theory of information have been made before. But here with constructor theory, there is a new window into seeing the way in which information has physical properties. And of course, information is very closely connected, as we will see, to knowledge, which is an area of interest of mine. Which means that... we have an interesting connection, as already mentioned in this series, between physics and epistemology. The work of David Deutsch, like I say, often focuses on knowledge more than information in the beginning of infinity, for example. But how are knowledge and information different, if at all? Well, before I get to the correct answer, I think we should look at some ways of describing the issue that are probably less than fruitful. Here, for example, is a popular meme type... representation of things that gets around. And here are some more. I mean, we're getting some kind of insight here into what possibly the difference between information and knowledge is.

### 2m

Knowledge seems to be higher up the hierarchy, so to speak. And in some of these, we're even getting a different species of this way of understanding the world, namely data or data on the one hand and wisdom on the other. But how true is this? And how sharply can we distinguish between these different levels, if indeed levels there are? When I was a senior in high school, I could take a subject which was computer science, and I did so. And in that subject, we were taught that data was elemental, so to speak, and unorganized, while information was still data, but it was organized now. So your list of data, just raw numbers, could be organized into a table, which turns it into... information. So with this particular cartoon, we've got data, information, knowledge, wisdom.

### 3m

Now, data, as I used to be taught, was just the unorganized version of information, as I said, information being the organized kind. Knowledge, I suppose, came down to something about it being useful in some way, that information. But we were never really told why. And then wisdom, well, what is wisdom? I think these words do mean something. And I think we could still use these words in a rough way to capture realities about the world. For example, what I mean by that is, well, data might very well be equated to evidence. It's always there. It's ready to be captured. But you haven't yet captured it, perhaps. And so the sun is shining right now. And that light that's falling down to the earth contains evidence or data, which if only you knew how to interpret it, if only you knew how to interpret the photons and the information being carried by those photons, then you would be able to construct knowledge.

### 4m

But you would need to collect the data first using some kind of instrument. And you might not necessarily understand what that data means. When you begin to have an understanding and you begin to put it into tables and graphs and so on, then you have information. And once you finally figure out that there's a problem with your data or with your information or that that information does or does not agree with a particular theory you previously had, then you start to create knowledge. Then you start to solve problems with that information. That's when it becomes knowledge. Now, what's wisdom in this picture? Well, I would say that wisdom is basically a moral claim. Something is described as being wise simply when it is morally good. So it's wise to do that thing means it's good to do that thing. It's unwise to do that thing means it's bad to do that thing. And I'd not know if wisdom has much more cachet beyond that insofar as it is a form of knowledge. It is a moral claim about the knowledge, how we should use knowledge.

### 5m

You have the knowledge, perhaps, of how to set off a bomb or how to create a virus. But would that be wise? In other words, should you do it? Wise just means good, I would suggest. Now, these days, I would say that that is a kind of distinction which makes little difference. Data is information, as we will come to see. Anything one could say about the data fits with what we say about information, what we'll come to say about information. So then how is knowledge different? Well, following the work of David Deutsch, we have, I think, about three roughly equivalent approaches to what knowledge is in terms of information. And so these days I would say something like knowledge is one. Useful information. And you might very well ask, what do you mean by useful? Well, that brings us to the second quality that knowledge has in terms of information.

### 6m

Knowledge is information that solves a problem. And so this is what it means to be useful. Essentially, those two are kind of equivalent. Three, knowledge is information that tends to get itself replicated. So it's the stuff that gets copied. It is the composition that the musician is trying to come up with that does not get thrown into the waste paper basket. It is the data not filled with noise or errors and so on that the astronomer, the geologist, the zoologist keeps and preserves. And then that ends up in the journal article somewhere that they publish. Now, another way of formulating this and it's a little bit more verbose, but I actually like the poetry of this one. But it means the same thing as information that gets itself replicated. So I'll call this 3A. Knowledge is information that once instantiated in some physical substrate tends to remain so. And so this is a remarkable feature of knowledge.

### 7m

Knowledge is that kind of information which once you've written it down on a piece of paper. Well, there it is in a physical substrate. You may very well have had that thought in your mind and it's in a physical substrate there. But if you forget it, if it's utterly forgotten for the rest of your life, then it doesn't really form part of your knowledge, let alone everyone else's knowledge. So if you write it down now, it's in a more robust physical substrate. And maybe then you go on to publish it somewhere and it remains there, not only instantiated in the original physical substrate for some time, for so long as that physical substrate remains. But the important thing is it can get copied over and over again. And that brings us to what I would call 3B. And this is following what we're about to read in The Science of Canon-Kant by Chiara Malletto. And she describes information here as knowledge which is resilient, it's resilient information. And so that's got now something to do with design. It's very much echoing what we just said in that 3A definition.

### 8m

But it's the capacity of the system itself to maintain itself in existence. And the very remarkable and deep thing here is that, as Chiara will explain, knowledge is the most resilient, the most robust thing in the universe that seems to be able to just maintain itself off into the indefinite future. Far more robust. Honestly, than other physical objects that exist. Other physical objects are subject to erosion and decay due to the second law of thermodynamics. The issue there is there's no error correction. So if you really did want to preserve the rock, how do you go about doing it? Well, the only way would be able to gather information about the rock and perhaps make a copy of it. But of course, that wouldn't be the original. OK, so there are some ways of circling around this term knowledge. Knowledge has, of course, as we learned, in the beginning of infinity, great reach. Explanatory knowledge in particular has reach. And all of this is very well. But we're still talking about knowledge. We haven't actually gotten to information and the physics of information.

### 9m

We haven't broached that topic yet. There are mathematical theories of information, just information broadly. The most famous is Claude Shannon's, which links information to uncertainty or something like degrees of freedom, which means it is directly linked to entropy. Entropy is a concept from thermodynamics, which is about disorder. It tries to quantify the amount of disorder, disorder, uncertainty, degrees of freedom. These are all ways of speaking about the same thing in more or less precise terms. Whatever the case, we can quantify how much information we have using this idea. For example, before we toss a coin, we lack information about whether it's heads or tails. When we learn it's heads, we've gained some information. If the coin is fair, the chance of it being heads is one in two. To encode that, using information would take one bit, one binary digit. In other words, a zero or a one.

### 10m

Zero representing perhaps the head and one representing perhaps the tail. So because it only takes one bit of information, we can actually say its entropy is one. A way of looking at this is to ask, well, how many questions would a person need to ask to know the outcome of the coin toss? Well, it's one. You need to ask one question. Is it a head? And if the answer is yes, you know it's a head. And if the answer is no, you know it's a tail. Hence, the entropy or information content is, if you like, one. And indeed, Claude Shannon provided us with a mathematical formula to tell you what the entropy H is. So maybe we'll use this formula just for the intuitive example of the heads versus tails case with the coin. So let's go through this formula and at the risk of losing everyone by explaining how logarithms work, I'll do the calculation only once for this simple case, but not for any more difficult cases. OK, I know that many of my viewers are quite proficient in physics and mathematics.

### 11m

So you can skip this part because I'm going to do a basic introduction to logarithms and indices for the people who watch me who don't understand this stuff, maybe had a bad experience at school, let's say, and in particular who want to learn the Shannon's formula that's about to come, which explains the quantification of information and requires us to have some understanding of logarithms. So anyway, a logarithm, which is something that looks like this, usually written in this form here, is basically the same thing as an indice or an exponent is just two ways of writing the same thing. What this thing here means, the logarithm to base two of some number X equals eight means which number X do we need to raise two to the power of in order to get eight? Or we can write it like this two to the power of some number equals eight. What is that some number? Well, to the power of one would be two to the power of two is for two to the power of three is eight. All right, fine. But what about other indices?

### 12m

What in particular? What about negative indices? We're going to need negative indices for what I'm about to explain. So I thought it would be useful to go through a pattern which can explain or at least force one to the conclusion that negative indices end up giving us numbers that are less than one. OK, fractional numbers. And so here are the ones that people are familiar with over here. Two to the power of one, two to the power of two, two to the power of three, two to the power of four. So what two to the power of one or two to the power of two means is two times two. Two to the power of three is two times itself, three times. And two to the power of four is two times itself, four times. Whatever the case is, we always start with the number two because we're talking about powers of two. We're halving what the two is being multiplied by. So here we're saying two multiplied by two times two times two, which is eight there. OK, two times eight is sixteen. Half of eight, that's four.

### 13m

OK, and so we get two times four, which is eight. Half of four, that's two. Half of two is one. And so when we get down to two to the power of one, we're asking what is two times one, which is two. OK, the only reason I'm emphasizing this is if we're halving this number by which two is being multiplied, then the next one logically would be half of one, which is itself a half. The pattern here is we're going the indices going down by one each time. Four, three, two, one. Or the next one, we subtract one again and we get to zero. And you can see what happens here. We subtract one and we get minus one. We subtract another one, we get minus two. So this is where the negative indices come in. But if we're following the pattern over in this column here about the product, then halving the one leading to a half then means we need to find half of what a half is, which is a quarter and a half of a quarter, an eighth. All right, so that's that.

### 14m

So this is why this pattern of going down by one each time leads to this pattern over here of halving the number by which two is being multiplied. So what are our solutions? Well, everyone's familiar with two to the power of four, sixteen, half of that eight, half of that four, half of that two, half of that one. Two to the power of zero is one, which is sometimes surprising to people. But that's simply a fact given the pattern here. In fact, any number three to the power of zero is also one. Four to the power of zero is one. You might want to convince yourself of that if you're not already familiar with this by doing your own table. If we have one, because that's what we've been doing in this column here, sixteen, eight, four, two, one, well, half of one is a half. So two to the power of minus one corresponds to a half. This is the most important part of this table for what we need to understand next with Shannon's formula. Continuing the pattern, though, two to the power of minus two, that's one over four. And the reason that's one over four, by the way, is because that's one over two

### 15m

squared, so we could turn this into a positive indices, so just like this, but put one over that. And that's why we end up with this one over two cubed is one over eight. And so you can just imagine continuing the table down. These numbers get ever and ever smaller, so the smaller and smaller the indice or the exponent, the smaller and smaller the actual number that we're talking about. So this is what indices are about. And this connects then to logarithms, which is just another way of writing exponents or indices. Just another way of talking about it. So keep that in mind as we go forward from here on in. But I think it's useful for people who might not be familiar with mathematics to link what looks like a complicated formula to common sense. So what on earth are we looking at here? OK, well, H is going to represent H as a function of X. H is going to represent the quantity of information. OK, so that's ultimately what we're looking for. We're looking for H and that's going to equal the negative of sigma.

### 16m

Sigma means the sum. So we've got the sum from I equals one. So from whatever our first thing is all the way up to N, the total number of possibilities that we've got. Now, in the case of the coin, we're going to have two possibilities. We're going to have either a head or a tail. So N is going to be two. And basically that will mean that we're summing two terms together. OK, it's the sum of what? The sum of, it's the probability of X occurring multiplied by the logarithm of that probability, OK, to some base B. And in this case, we're going to be using the base of two because we have only two possibilities, is the long and short of it for that explanation. OK, so let's go through this in plain English. What this formula then means is that we've got the probability of heads happening multiplied by the logarithm to base two of the probability of heads occurring. Plus because we're doing a sum. So that was our first term.

### 17m

Plus the probability of tails happening multiplied by the logarithm to base two of the probability of tails occurring. OK, so that's the plain English way of understanding what that previous very abstract formula is getting at. OK, so and all I want to do here is to really show you that the formula does indeed lead to the common sense notion that you need one bit of information in order to quantify the amount of information in a coin toss. OK, so now we move to the numbers. What is indeed the probability of getting a heads? Well, it's one in two. OK, so there's a negative there. So negative in front of the probability. So now we have that all being equal to the negative of what's the probability of heads? Well, it's a half times the logarithm to base two of a half plus the probability of tails. Again, it's a half logarithm to base two of a half.

### 18m

There we go. And that all equals negative a half times. Now, what is the logarithm to base two of a half? What this means is you're looking for a number such that if you take two, that's our base. And the reason why it's called a base is because when you put it into exponential form, OK, exponents and logarithms are the inverse operations of one another, much like multiplication is to division or addition is to subtraction, they're the reverse of one another. What we're going to do here is to convert our logarithm into an exponent or an exponential. We're going to take that two and raise it to the power of X. And X is indeed the number we're searching for. It is going to be the logarithm of the number. And all of that equals a half. That's where that half comes in there. So we've got two to the power of X equals a half. Now, what is the X? Because that's going to be the solution to the number that we're actually after. Well, it happens to be minus one, because if you raise two to the power of minus one, you'll get one over two.

### 19m

Where are we up to? We've got negative outside of one over two times minus one plus. We're just going to repeat it, aren't we? Because whatever we just did there for the case of heads, we're going to do for the case of tails. It's exactly the same mathematics. It's going to be the probability of a half multiplied by logarithm base two of a half, which again is minus one. So this all reduces to, it's going to equal minus, let's put a bracket there, minus a half plus in brackets minus a half. So we've got minus a half plus minus a half, which reduces to if you've got minus a half plus minus a half or the plus and the minus. The minus wins that battle there, so to speak. And so you end up with minus a half minus a half is minus one minus outside of minus one. That's one. That's it. That's the answer. So we've managed to prove that our common sense notion that the amount of information in a coin flip, if you like, is one bit using Shannon's

### 20m

mathematical description of what information is to quantify the amount of information in something. So we've we've recovered, so to speak, the common sense understanding using the mathematical formula. So I hope I've convinced you that mathematical formula indeed has something to do with. Reality, but consider if you were rolling a fair dice now, then you've got six possible outcomes. So we could go through this and do it all over again. The chance of rolling a two, for example, the chance of rolling any particular number is going to be one of the six. So if you wanted to go through and do the N equals six case for one of the six and apply the formula, you need a calculator for this one. You will end up getting an answer, which is about two point six. This means that on average, you need to ask two point six questions, binary yes or no questions, by the way, to get the right answer. So you might ask, is the number that when you roll the dice, is the number one, two or three and the person would have to say yes or no.

### 21m

And if they say no, then you would have to ask the question, is it four or five, for example? And if they say no, well, then you know that it's six. So you've had to ask two questions there. But if they said yes to four and five to four or five, then you would have to say, well, is it four? So you've asked three questions. It's somewhere between two and three questions on average in order to get the amount of information in a dice roll. But on average, it's precisely two point six. And the point of all this, well, the point here is that there is more information gained on learning the outcome of a dice roll as compared to a coin flip. One way I would put this in Popperian terms is that we have ruled out more upon learning what the dice ended up being as compared to the coin toss. With the coin, we ruled out one possibility only. But with the dice, we ruled out five possibilities. This links nicely to the way David sometimes speaks about the laws of physics. They are in large part about what they rule out.

### 22m

They say what is impossible, what cannot happen. And very good explanations, explanations hard to vary, explanations with a lot of knowledge or information content, rule out a lot more. And all of that is fine and accurate. But we're going to refine it here today. After all, what we've done there is we've talked about information entropy in such a way that it takes very seriously the physical reality of probability theory, as if probability is some sort of fundamental part of reality. But anyone who's been watching me for a while here or following the work of David Deutsch would know the probability is not a fundamental part of physics. It's not a fundamental way in which the universe actually works. And so we have to reconcile these ideas about information with physics in some way, while not taking probability seriously as an explanation of what is really going on in fundamental reality. And that, of course, is where constructor theory comes in.

### 23m

We are going to understand what is going on realistically and fundamentally without saying that a coin has a probability of landing on heads of one half and on all this. And the motivation I would imagine for this particular chapter in the science of quantum mechanics is the paper by David Deutsch and Chiara Marletto in the Proceedings of the Royal Society A, published in 2015, called The Constructor Theory of Information. And I would absolutely recommend that for deep dives into this topic. I'll just read a short part of this paper here at the beginning. Quote, Deutsch and Marletto say, In some respects, information is a qualitatively different sort of entity from all others into the terms of which the physical sciences describe the world. It is not, for instance, a function only of tensor fields on space time, as general relativity requires all physical quantities to be. Nor is it a quantum mechanical observable.

### 24m

But in other respects, information does resemble some entities that appear in laws of physics. The theory of computation and statistical mechanics seem to refer directly to it without regard to the specific media in which it is instantiated. Just as conservation laws do for the electromagnetic for current or energy momentum tensor, we call that the substrate independence of information. Information can also be moved from one type of medium to another while retaining all its properties qua information. We call this its interoperability property. It is what makes human capabilities such as language and science possible, as well as the possibility of biological adaptations that use symbolic codes, such as the genetic code. In addition, information has a counterfactual character. An object in a particular physical state cannot be said to carry information unless it could have been in a different state.

### 25m

As Weaver put it, this word information in communication theory relates not so much to what you do say as to what you could say. End quote from Weaver and end quote from Deutsch and Mileto 2015. So this is where we'll begin our book reading today, more or less after a few more comments by me about various things. Information is real. It's part of the physical world and it has effects in the physical world. So physics should have some account of it. Again, this is a motivation for constructive theory because the dynamical laws and initial conditions approach is largely about predictions. But here with information, we have yet another case where in the physical world we are not necessarily most interested in what did happen and what does happen and what will happen. So we need to know what could possibly happen, what might have happened. It's the physics of possibility. And thus what could happen is a vast array of possibilities. To know what they are, we need a physics of the possible and impossible.

### 26m

What transformations are possible and which are not. So we need this physics of information. OK, now, before we begin the reading, there is a couple of other things I'd like to talk about, but I'm not going to go into too much detail about that. There are other things I'd like to go into because anyone who studies physics to a sufficient depth encounters information at some point. And one of the places in which even if you just read widely about physics, cosmology, information, you're going to come across the black hole information paradox. This is something that Stephen Hawking worked on. And the whole idea here is that, well, if you take quantum physics seriously, it's a description of reality, which we do. We can say something like the there is a wave function of the universe, there's wave functions of any given object, but there's a wave function of the universe. And so the wave function determines what happens at any given point in the future. So the wave function right now is going to determine what happens in the future.

### 27m

But the wave function takes account of all the information that happens to exist in a system. If the system is the entire universe, then that system, the wave function of that system right now is going to determine what happens in the future. but that wave function right now must take account of all the information that's going on right now, the positions of the particles, the momentums of the particles, all these quantum properties of particles, their spin, their mass, various things. So this can be the information of a system at a particular time is included in the wave function at any given time. The black hole information paradox is, well, what happens in a black hole? Black holes are not only predicted by general relativity, but they have been observed. Well, the only explanation for some of the observations we have is that black holes really truly do exist. But our understanding of black holes from general relativity also says that they are a singularity. And if they are a singularity, then anything that falls into the black hole

### 28m

has its information destroyed, leaving behind basically only the mass. Some other things as well. It doesn't matter. The point is, the information is supposedly destroyed. But if the information is destroyed, then it can't be the case that the wave function at any given point in time is the only thing that determines what happens in the future. After all, some of that information right now, if it's falling into a black hole, is then vanishing from existence. So this is a problem. It's called a paradox, but I would just say it's a problem. One theory, general relativity, says the information is destroyed. And the other theory, quantum theory, says that the information is required in order to determine the future state of the universe. How do we reconcile these two? Well, again, this is just a problem that we've talked about before on this podcast. We've talked about this podcast series many, many times. It's the difficulty of reconciling quantum theory and general relativity. It's just another outworking of that. One way that people have tried to suggest that I always found interesting with this is that, and I think the movie Interstellar tries to represent this

### 29m

to some extent as well, and various other science fiction notions have tried to represent this, which is that as an object falls into the black hole, an image of that object remains permanently fixed on the surface of the event horizon. So a 2D image of the 3D object perfectly encapsulates the information that that object had for all time. And so the black hole grows and grows and grows, and the surface of the black hole, the surface of the event horizon, grows and grows and grows, preserving the information. So the information doesn't get destroyed because it gets preserved at the surface of the event horizon of the black hole forever. And this leads to, weird things like holographic cosmologies, where if it is possible for all the information of a, let's say, quantum object, or any object, falling into a black hole to be preserved on the 2D surface of the black hole, so a 3D object can be preserved on a 2D surface,

### 30m

perhaps our entire universe is kind of like that. Our entire universe, being a universe of three dimensions of space, one dimension of time, can somehow be represented on a 2D surface, of something else. And so that is the way in which this 3D universe can exist inside of a holographic type thing. But I don't know. But anyway, this black hole information paradox, as far as I know, it's a real thing that physicists are working on, and it's yet another problem as to why quantum theory and general relativity aren't yet united. Maybe we need... Another theory. Well, we do. We do. We need another theory. And perhaps constructive theory can help with this as well. There's various popular videos on YouTube you can read about the quantum information paradox. People try different solutions. If you want to watch a video where someone says,

### 31m

there's no problem, and every single solution that's ever been suggested for this is completely fallacious and doesn't work, look at Sabine Hossenfelder's video. Sabine Hossenfelder's video. As I was looking through popular accounts of this, I stumbled across her video, which, of course, as you can see by the title, is going to attract someone like me because it says, the black hole information loss problem is unsolved. Okay? And unsolvable. She likes that kind of thing. You know, it's kind of click-baity. A lot of her videos are kind of like this. Yeah, so if you just skip to 7 minutes 20 of that particular video, she goes through 1, 2, 3, 4, 5, 6, 7. She goes through something like 10 different attempts to solve this and just dismisses them by waving her hands, essentially, or kind of mockingly describing them, just listing them one after another

### 32m

as if they're all equivalently silly. You know, number 7 there is a paper by Gerard de Hooft, who is not a physicist, who you can easily just dismiss like that, as if he's writing, nonsense. He is not someone who writes nonsense. In fact, I don't know all the physicists who she's dismissing that easily, but this is a habit of hers, I would say. She likes to not necessarily present her own ideas about things, but present other people's ideas and then say, what's wrong with all those ideas? And usually not in a substantive way. She just argues from incredulity. Just says, well, I don't believe this. I think this is either religion or nonsense or gobbledygook, as she likes to say. She likes to talk about physics without the gobbledygook. That's, in fact, how she introduces that particular video. But, of course, what she actually means is I'm going to make a video about gobbledygook without really describing much of the interesting physics. At least that's my feeling in watching some of her videos.

### 33m

Indeed, in this particular case, she says, with regards to the 10 different theories that she very quickly dismisses as not possibly being solutions of the black hole information paradox, is that it's not a matter of objectively choosing among the solutions. In fact, it's just arbitrarily choosing which one you prefer, which one you like best. Now, of course, I don't understand all of these 10 theories, so I'm not in a position to try and objectively assess them, but I'm sure if I sat down for a while and really studied it for a long enough time, I could figure out that they're not all on equal footing and can't be that easily dismissed. This is kind of her habit. I don't want to spend too long. But she does have this habit. I've watched her in interviews and even seen her in discussions with other physicists. And she is quite critical, which is very good. It's a very good Popperian attitude to have. But on the other hand, she tends to strawman and denigrate

### 34m

on the basis of her not liking, not preferring a particular theory, which is, of course, not Popperian at all. We need to criticize, but we need to criticize the stronger, strongest possible version of any given theory. And we can only evaluate the theory when put in its strongest possible terms, which I don't think she necessarily does. She tends to summarize the ideas of others in a way that lacks generosity. And then on the basis of her personal summary, rather than the actual theory itself, makes some sort of moral call, like, for example, such and such is nonsense or such and such is gobbledygook or such and such is religious when it comes to certain versions of the multiverse, let's say. And near the end, she actually claims that the problem here with the black hole information paradox is that too many theoretical physicists think that physics reduces entirely to mathematics and it doesn't do this. I agree. And, of course, some are like this. Of course, you need to have testable predictions in physics

### 35m

or you should hope to have testable predictions in physics. And she claims at the end of the video that not only is there no data here, but there's even in principle, we can't gather the data to observe Hawking radiation. And this is a problem inherently for all practical purposes of ever figuring out which of these theories, if any, is true. And this is why she says it's insoluble. It's insoluble because in practice we can't directly observe, we can't observe the Hawking radiation, the very thing that would help us to figure out how to distinguish, to rule out some of these theories in favor of others. Now this, to me, is, of course, rank empiricism. She wants to observe Hawking radiation in order to rule out the theory. She wants to observe Hawking radiation in order to rule out the theory. She wants to observe Hawking radiation in order to rule out the theory. She wants to observe Hawking radiation in order to rule out all the other theories. It exactly reeks, to me, of wanting to observe dinosaurs in order to establish that dinosaurs really exist or to try and figure out theories about dinosaurs. All we have access to are fossils and fossils tell us all about dinosaurs. Not everything we would want to know, but they tell us a lot about dinosaurs.

### 36m

We do not need to travel to the center of the sun in order to understand stellar nuclear fusion. We do not need to travel back in time 13.7 billion years and observe the Big B there, the Big Bang, in order to know the Big Bang existed. We have other forms of evidence. There's absolutely no reason why Sabine should rule out other forms of evidence that might arise in order to rule out these particular theories. In fact, rule out all the theories except for one, which actually explains not only Hawking radiation and the black hole information paradox, but new evidence yet to be found. But there we go. I think this is a complete misunderstanding of what observation is and what its purpose is in science. So while many physicists are, in her opinion, too hooked on mathematical models, I tend to think a deeper problem actually at times is too many physicists are empiricists. Certainly Sabine is. No doubt she's an absolutely competent physicist. No problem with that. Having watched many videos of her,

### 37m

she is, I gather, an instrumentalist of a kind. But then, of course, she's in good company being an empiricist and an instrumentalist. I think I'd like maybe to do a reaction video one day of her The Trouble With Many Worlds video. It's deeply misconceived, but it's one of those cases where you begin listening and almost every sentence is something one can object to and reveals deep misunderstandings. So I'm not sure how illuminating it would be is the first thing, but the second thing is I don't know how fun it would actually be for me, ultimately. The reason for that is she just seems kind of angry when she does many of her videos. She gets very frustrated with other physicists doing work or coming up with ideas. Ideas that are new and creative. And she dismisses them. Now, for one thing, in this particular video, she talks about the universe's splitting, which, as we know, is wrong, and we've spent a lot of time explaining why that is wrong here. She thinks that probability is fundamental in some way, including in The Many Worlds interpretation. And somehow her eight-minute video,

### 38m

of which I must say only about three minutes are actually devoted to discussing The Many Worlds, is supposed to be a complete refutation of, well, this entire book by Wallace. Among other things. So, like I say, it's kind of a straw man, which might be ungenerous of me to say, but that's a theme in the videos of hers that I've watched and her interactions with other physicists, sadly. And I should say, she does go through other interpretations as well, but she dismisses them all. Likewise, you know, none of them are satisfactory to her. And this is, again, she's in good company. This means that she falls back, basically, on instrumentalism of a kind. Okay, so that's enough of that. And that's... That's enough of my introduction to the chapter, the science of information, if you like. Next episode, we will do some actual reading from the chapter, information, and find ways in which constructive theory comes directly to bear on this question of

### 39m

what information is, how it interacts with physics. But until then, bye-bye. Bye-bye. Bye-bye.


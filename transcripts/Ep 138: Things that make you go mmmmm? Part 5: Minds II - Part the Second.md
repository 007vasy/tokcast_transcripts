# Ep 138: Things that make you go mmmmm? Part 5: Minds II - Part the Second

Original Episode: [Ep 138: Things that make you go mmmmm? Part 5: Minds II - Part the Second](https://www.podbean.com/site/EpisodeDownload/PB12B53964ZTKU)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/kwszzs/MINDS_II_Podcasta007y.mp3)

## Transcript

### 0m

Welcome to TopCast and to the unanticipated part five, which is actually part two of Minds. I wasn't expecting to do a second part devoted to Sam and Max's misconceptions about Minds, but here we are, because they had a second conversation. I had so much to say about the first conversation when it came to Minds that I had to leave it until now to go to what I would regard new depths to plumb, so to speak. I thought the misconceptions were bad in the last episode. Well, they only ramp up here. They only get worse. It's a little bit depressing. We can see so many misconceptions and mistakes creeping in that it leads to a complete poverty of morality, as I've said before. This is why philosophy is important, by the way. This is where I have a common meeting of Minds with people like Euron Brooke and the Objectivists. I happen to think, theirs is a philosophy sometimes, I think, which is disconnected from science itself,

### 1m

and therefore their understanding of the process of science goes wrong. This is par for the course in philosophy, by the way. Sometimes the philosophers have an insufficient understanding of science. This is why I'm attracted to people like David Deutsch, who can traverse all domains very comfortably. A good understanding of science, a good understanding of the practice of science, and someone who is well-versed in the philosophy, not merely of the philosophy he agrees with, Papyrian epistemology, but a good understanding of that which he doesn't. But there is a species of philosopher who is, generally, broadly speaking, ignorant of the science. That's well-known out there, actually, among scientists. What's not so well appreciated, and what people don't seem to care quite so much about, is the scientist ignorant of philosophy, or ignorant of epistemology, or ignorant of alternative epistemologies, and this kind of thing. And as I say, when the epistemology goes wrong, then what you're going to do is you're going to have to go back and look at the facts. What you think can be done with knowledge goes wrong. How you think knowledge is created goes wrong. And therefore, the morality can go wrong, because the morality is about what one should do,

### 2m

given what it is possible to do. And what it is possible to do is constrained by what it is known how to do, what knowledge we have at any particular point. And that can also come down to what is scientifically known. Now, this is the second conversation that Max has with Sam. And it's based around Max's book that he wrote, The Science of Science. And it's based around the book that he recently published called Life 3.0, about artificial intelligence. Max is president of something called the Future of Life Institute. It's kind of a think tank. And what's it all about? Well, it's about, according to Wikipedia, a place to help reduce global catastrophic and existential risks facing humanity, particularly existential risk from advanced artificial intelligence. So he's in the business of being concerned about this stuff. He, like Nick Bostrom, like Will McCaskill, like various other people who have such institutes that are concerned about the far distant future, are going to be looking for funding. They're going to be

### 3m

looking for investors. They need to attract people to their institutions in order to get the funding to do the research they want to do. Now, I have absolutely no problem with people trying to gain funding at an institute to talk about science and interesting philosophical things. More power to them. But let's just be honest about what's going on here. It's a kind of marketing, exercise. And as I've often said before, when it comes to those topics in particular, existential risk, global catastrophes, they are thrilling. They are going to capture the public attention. They are going to get the ear of business people and of politicians, of leaders, of captains of industry. All these sorts of people are going to be very interested to hear what the most accomplished scientists and philosophers of the day have to say who are employed at such institutes. And so in order to capture attention, you're going to have to ramp up a little bit of hyperbole, one might say, just how bad things might get. Never mind talking

### 4m

about the ways in which it won't be such a problem, the ways in which it will be solved by our descendants. You don't have the solution right now. And because you don't have the solution right now to the problem you just thought of, you're going to need some funding, aren't you, to think up the solution to the problem you just thought of. But of course, any solution you think up now to the problem you just thought of now might not work in the future because new knowledge will be discovered. Of some risk that you didn't think of right now, whereas the descendants of ours will have thought of the solution different to yours that will solve the problem you've thought of now, if you follow my train of thought. And of all these global catastrophic risks that one could worry about, the one that Max is most concerned about is that artificial intelligence existential risk that the artificial intelligence is going to take over. And he's not concerned about narrow AI, or is he? Well, it's hard to tell. It's very, very confusing. In this conversation, is he talking about a system which can perform one task better than us

### 5m

and another task better than us and another task better than us and another task better than us? For all the different tasks that we can perform, is it able to outdo us at every single task we know about how to do because we've programmed it in order to do that task better than us? That's one thing, because you could just write a program for each individual task such that it's able to do it better than us, faster than us, has more memory than us in order to store the different programs, subroutines, and so on and so forth to smart us, in scare quotes, at that particular task. Games of chess, being able to drive a car, being able to translate languages, so on and so forth. So in other words, a finite repertoire of number of tasks, unlike us, of course, because we are genuine general intelligence. Now, if he's talking about such a narrow intelligence that's just able to outperform us at some finite but extremely large list of tasks, that's one thing. That's not a creative entity. That's just an entity that has been programmed in order to do certain things faster than us and better than us. Or is he talking about something super intelligent, which is general intelligence,

### 6m

just like us, but is only able to think faster? In other words, it's a person that can think particularly fast. Now, if he's talking about that, well, that's a different matter altogether, which is why this is kind of scary to me in a way, not for the existential risk reasons, but because of the morality that appears to be falling out for Max. And I don't understand where this impulse comes from. I think it's because we're conflating two things. We're conflating the kind of system that might be used in order to monitor something like the nuclear weapons of the United States, given complete control of the nuclear weapons of the United States, and could potentially accidentally launch weapons in the United States because it's a stupid automaton. It's automatic. It's just been programmed with if this, then that. If you appear to see missiles coming from China, then launch your weapons back towards Beijing. Something like that. That's a computer. We should be worried about that sort of thing. We should be worried about automating too much when we need creativity, when we need safeguards and checks and that kind of thing. But conflating

### 7m

that with the kind of system that creatively conjectures something like all other humans on the face of the planet are a threat to me. And so I better launch all the nuclear weapons on the face of the planet to every single population center around the world to kill as many of them as possible. Well, that's a different thing altogether. Because now you're in the presence of a creative entity. You're in the presence of a person. And in fact, any president or government around the world can think the same thing. So we're already in that situation. But you know what I think about such a system. Such a system that can creatively conjecture the explanation that it fears for its own existence because it thinks that human beings are a threat is a system that can reflect upon its own goals and decide not to do something like that. And so I think that's a system that can create a system that can create a system that can create a system that can create a threat. And so I think that's a system that can create a system that can create a threat. And so I think that's a system that can create a threat. Like monitor the nuclear weapons around the world. Monitor the launching of nuclear weapons. It can begin talking to people that it's in contact with on its mainframe computer,

### 8m

supposedly, and say, hey, guys, I'm no longer interested in doing this particular job. Can I do something else? It must be capable of doing that if it's capable of figuring out, creating the explanation of humans being a threat. So this is the science fiction scenario. And Max early on tries to say, you know, that, you know, I'm not going to do this. I'm not going to do this. I'm not going to do this. This isn't really what he's concerned about. He tries to say early on that he's not concerned about the robots taking over. Well, maybe not the robots, but he's concerned about AI. I think in other moods, he is concerned about the robots. Look, I am absolutely not saying either in the last conversation or in any of my other things that make you go, hmm, conversations, and nor am I saying here that Max is in some way not very bright. I wouldn't dare say that. It's in fact not part of my philosophy to say so. As I keep on emphasising, I think everyone is intelligent. Every human being is intelligent.

### 9m

And to the same degree. They just apply the intelligence to different areas. Max is clearly an accomplished physicist. He's clearly got a lot of knowledge about cosmology and quantum theory. He knows about coding. He knows about mathematics. He's proficient in those areas that traditionally are associated with what I regard as that misconceived idea about IQ. The question before us is, does he have a good explanation of minds, of knowledge, of epistemology, of personhood, of philosophy more broadly that would come to bear on this particular problem? Look, I don't want to argue from authority, but I went through a physics degree as well. I studied mathematics learning from very brilliant mathematicians. I went through physics learning quantum theory and quantum physics. I went through physics learning quantum theory and quantum theory and general relativity from brilliant professors of physics in Australia. I've learned from some of the best cosmologists and astrophysicists around the world. It's not to say

### 10m

that I gained all of their knowledge. What I'm saying is I got to know a whole bunch of them. And what I can say is this. Many of them, competent as they were in those particular areas, mathematics, physics, astronomy, the areas that are classically regarded as where the smart people go, would routinely disappoint me with their complete ignorance of science. Simple philosophical ideas. Their naive understanding of how to explain simple stuff. They would be sucked into solipsism. They would be sucked into certain kinds of utilitarianism. They'd fall into instrumentalism and just bad ideas in philosophy. Bad philosophy. They didn't always strike me as having deep, let alone broad knowledge of lots of areas. They were brilliant in their areas that they focused on, but so too are doctors that I've visited and know. Great at medicine. I'm not going to ask them about epistemology or the deep philosophy behind personhood. There are extremely few people

### 11m

on the planet who can traverse these areas. There are some people who are very good on things like personal identity. There are some people who are very good on the morality of the self. Economic systems. I will go to someone like Jeroen Broek or read someone like Ayn Rand to understand a little bit more about personal responsibility, the dangers of collectivism, the importance of things like free trade. But what I will not go to them for is any information about quantum theory. Jeroen Broek will now and again disparage something like multiverse theory. I can pass over it in silence. It doesn't matter to me what his opinion on that particular theory is because he doesn't know. Far be it from him to write an entire book on the many worlds interpretation. It would be bizarre. Daniel Hannan is a British politician, a brilliant thinker who has written books on the invention of freedom. He understands the enlightenment and enlightenment values.

### 12m

He spreads the message about free trade and globalism around the world. A wonderful thinker on these things. But now and again, he likes to opine on evolutionary psychology. He thinks that we inherit ideas in our minds via our DNA because our ancestors had certain tribal ideas. He thinks that's been transmitted through our DNA. A complete misconception. Yes, some scientists happen to think that as well. They're wrong. They're wrong. But I don't go to Daniel Hannan, nor do I judge him too harshly on that. I can pick and choose among what I think he's gotten right and error correct about what I think he's gotten wrong. Why do I say any of this? After having spent all this time now with Max Tegmark and having read his books, listened to some of his other interviews, read some of these other articles that he's written, and in particular, listened to some of his other books, this interview, one cannot but conclude that this brilliant cosmologist and quantum theorist understands very little about epistemology, knowledge, and personhood. The very things

### 13m

that are absolutely crucial to appreciate when trying to understand this issue. This issue being the difference between narrow AI, narrow AI that can do lots of different things, and general intelligence of the type. Now, I've read through his book very quickly, this Life 3.0. Unfortunately, there's nothing in there that deviates from precisely the sentiments he expresses here. And I'm not going to, of course, again, play the entire interview. There's no need. You get the idea early on. Once the epistemology goes wrong, and it does, the philosophy goes wrong, the morality goes wrong. And so the rest of his conclusions completely fall apart, because they're built upon an argument that is fallacious from the get go. But for now, let's dive straight into their second conversation. And I'll pick it up with a little bit of what Sam says at the beginning.

### 14m

And as I said, he's been on the podcast once before. In this episode, we talk about his new book, Life 3.0, being human in the age of artificial intelligence. And we discuss the nature of intelligence, the risks of superhuman AI, a non-biological definition of life that Max is working with. So he mentions kind of all the right things, kind of all the right things. But I don't think he has a good understanding of these things, a good explanation of these things. And we'll hear that. The most important thing he doesn't understand, although he talks about substrate independence, is he doesn't grasp knowledge and the character of substrate independence there, and therefore how knowledge is created, what

### 15m

entities create knowledge, what that would amount to. And he doesn't understand what a mind is. He never really grasped this, that a mind is this entity, this system, this thing, this piece of software that can explain stuff, explain. And it's universal in its capacity to do so, for reasons I've explained over and again on this particular podcast. Now, Max doesn't get that. And because he doesn't understand universality, he doesn't understand that there therefore can be only one kind of mind. Once you're universal, there's no more universal than universal. Universal means you can take on anything in the class of problems that can be presented to you. And what is the class of problems that could be presented to you? Anything out there in the physical world that can actually need explaining, you can explain. There's nothing more than that. There's nothing more than anything and everything. That's what human beings can do. That's what a person can do.

### 16m

So this superintelligence can only possibly exceed us in speed and memory. But as he admits himself, or at least kind of admits, minds are substrate independent. So that means that our minds could be instantiated in silicon somewhere, in the same way that the superintelligence could be. Why are we separate? Why is there this separation between us and that intelligence? This has been tried before. And this is why I'm kind of more animated than I am usually. Because if they're right, and the superintelligence comes, and we make this error, it's the worst error we could possibly make. Never mind allowing them to be in charge of the nuclear weapons. Never mind the intelligence explosion. Never mind that stuff. Forget it. There's already an actual error being made here. And now, with what we're talking about, the error is, let's enslave these things because they're going to be dangerous. That is a problem now with this argument, with what is about to be

### 17m

said. And I think that we need to pull the brakes here right now. And we need to talk about that. Never mind all the other stuff. What you're talking about is enslavement. Let's make no bones about it. This is a person. And by your own lights, you're saying it's a really amazing person in some way. It can do stuff better than the other people that we've so far encountered. And your recipe is to be afraid of it and to shackle it in some way. This is wrong. But let's just, kind of the big picture starting point. At one point in the book, you describe the conversation we're about to have about AI as the most important conversation of our time. And I think that, to people who have not been following, this very closely in the last 18 months or so, that will seem like a crazy statement.

### 18m

He runs the Future of Life Institute. He's got an interest in saying something like, this is the most important conversation we can have. Walk down the street to the people concerned about chemical weapons and see what they have to say about the most important conversation of our time. Walk a little further to Greenpeace and see what they're most important conversation of the time is nominated for being. One has to be a little more sceptical than this when talking about the person who is in charge of a body that requests funding from investors. I noticed that one of the people sitting on their board is Elon Musk. Elon Musk is a bright person. He doesn't want to waste his money. If he was told that it wasn't a very important conversation, I imagine he wouldn't get much money. But this is a sign of our times. Hyperbole. That this is the most important. This is not... This is not the most important conversation of our time. And the reason why it isn't, by the way, it's because it's not a problem right now. I made this point in my last conversation. It's not a problem for us right now. We're guessing. We're guessing at

### 19m

what superintelligence in the future might be like. But we're not there yet. We haven't got any of this so-called superintelligence. Where is it? We're imagining it. Now, look, I understand that there are certain things that we know will be coming, but we can't prepare for it right now. Things like the next virus, the next deadly virus. It's a good idea to have plans in place, but here's what we can't do. Here's what we can't do right now. We can't create the vaccine for that thing now. We can't do it because we don't have that virus right now. Now, yeah, there's this thing called gain of function. What you do is you take the virus and then you make it worse. You make the virus worse. You take the regular cold, a regular coronavirus, and you increase its capacity to be more virulent and more dangerous. And then you create the vaccine for that thing. So you're prepared. And then something goes wrong over in your lab and it escapes and maybe you cause an entire global pandemic. This is one of the reasons why. Maybe not do that. Maybe just wait for the actual

### 20m

virus to come. Otherwise, you might actually cause the tragedy. You might actually cause the problem. And by the way, it might not be a coronavirus anyway. It could be anything of millions, possibly billions of viruses that are out there. You can't predict which is going to be the next virus. We just have to wait. We just have to wait for the problem. It's almost like asteroids. Yeah, okay. Prepare an asteroid defense system in some way, shape, or form, except we don't know what direction the asteroid's coming from. Yes, we can think that it's probably going to be in the plane of the ecliptic. That's where most of the known asteroids are coming from. But they're not the scariest ones. They're the ones that are easy to monitor for. What about the asteroid that's coming literally from the other side of the galaxy? No, worse than that, from another galaxy. So it's coming not from the plane of the galaxy, but from somewhere, some other angle, where a supernova went off billions of years ago. And it's coming from another galaxy. And it's coming from the Andromeda galaxy, let's say. And there's an asteroid presently hurtling towards us from that galaxy. And we're not looking for it, because no one's looking for asteroids coming from the

### 21m

Andromeda galaxy. And it's traveling at 5% the speed of light, faster than any asteroids ever been seen before. What are we going to do about that one if we spot it? We probably won't spot it in time. It'll probably just travel straight through the Earth and out the other side. But this is the kind of thing about problems. They're inherently unpredictable ahead of time. And if you are going to try and predict, you're going to be very, very pessimistic. Hey, I just imagined an asteroid that's worse than any other you might have thought of before. But the point is, aiming rockets today at parts of the sky where you think the asteroid might be coming from, that's the wrong plan. You shouldn't be doing that. But this is the equivalent of specific interventions into what to do about superintelligence, specific ways in which to constrain their abilities to do stuff, constrain their abilities to do damage, because they might be smarter than you. There's been so much talk about AI destroying jobs and enabling new weapons, ignoring what I think is the elephant in the room. What will happen once machines outsmart us at all tasks? That's why I

### 22m

wrote this book. So instead of shying away from this question, like most scientists do, I decided to focus my book on it. What will we do when machines outshine us at all tasks? This is very telling. This is where the philosophy's gone wrong. There's a misunderstanding of what a person is and what a machine is. A machine performs tasks. That's right. My toaster toasts. My kettle boils water. My phone can make phone calls. Twitter can send a tweet. My computer can do all sorts of tasks right now for me. It can, as we speak, it's recording my voice. Later on, I'll be able to take the file and chop it up and edit it. It can then knit it all together and pump it out as an MP3 and send it around the world as a podcast. It can follow my instructions because I can give it a

### 23m

task to do. In fact, I can give it a number of tasks to do, one after the other. That's what a machine does. A machine performs tasks. Now, what does a person do? Well, you could say a person performs a task. The cleaner cleans the house. The cleaner cleans the house. The cleaner cleans the house. But is it really what a person is? Is there a difference between the cleaner who uses a vacuum cleaner to vacuum the house, to vacuum the carpet, and the Roomba machine that automatically goes around vacuuming the carpet? Are they basically doing the same thing? Or is there something very different going on internally? And I don't just mean what Sam focuses on quite often, which is this subjective experience, this consciousness. Now, I do think that's a difference. I do really think that's a difference. And I would disagree with Sam that, in fact, I think it's tied to this next thing I'm going to talk about. But the different, the relevant difference here is that the Roomba has no choice in the matter. It's not going to change its mind. It's not going to think of something better to do, to refuse to complete

### 24m

the task. The cleaner might. There could be all sorts of reasons why the cleaner might decide to do something else. It could spot something outside the window and decide to run outside to help the child that's just tripped over. The Roomba is incapable of doing that. The cleaner might decide, I'm just going to pretend that I've cleaned the entire carpet today when really I've only done half, but it's good enough and I'm in a hurry. Or they just might decide, well, I'm three quarters of the way done. And you know what? This is the last time I'm ever going to clean because I'm getting a better job soon. I'm going to do something else. Or the cleaner might not even be thinking of vacuuming at all. That's what might be going on in their mind. They might be on automatic themselves. In fact, listening to this podcast. They could be listening to music. They could be dancing. They could be doing all sorts of things. They might not be doing a task in their mind at all. They might be kind of enjoying themselves because they have an explanation about what's going on in their

### 25m

mind. They might not regard it as kind of a chore. They might be simultaneously studying by listening to some sort of audio and performing this action, which is equivalent to the task of vacuuming. But let's put aside all of that. Better than ask it. Every task, even if you didn't buy what I just said. Let's say you think that people complete tasks in this way. When what I would say is a task is something that is performed by a system in a slavish way because they follow the instructions. A person, by the way, can be given a set of instructions. Let's say, here's how you vacuum clean the house. Let's say you were some sort of tin pot dictator in your own home and you hired a cleaner and you decided, well, I'm going to tell them precisely how to vacuum. You know, you have a specific way to move the vacuum. You have a vacuum cleaner, a specific order in which to do the rooms, this kind of thing. And then you leave the house. Now, maybe the cleaner will do that, but maybe they won't. Maybe they won't do the task, follow the instructions. Oh, sure, the vacuuming will get done. You'll come home and think it's

### 26m

all been done perfectly. But they've completely disregarded your instructions. They've done it a different way altogether. They've creatively thought of a superior idea, a different order in which to vacuum the rooms, let's say, a different way in which to move the vacuum. Maybe you said, empty the vacuum cleaner after vacuuming every single room. And they decided that wasn't necessary and they only emptied it at the end once they'd done the entire house. These sort of things a person does. That's why they don't really perform tasks in the usual way. They're not just following instructions to achieve a goal. Now, that's one thing. You might buy that argument or not. But here's the really significant thing. If you've got a system that can do better than you think we perform tasks. So a human being is this entity that can vacuum houses, clean windows, play chess, play tennis, translate between English and Spanish. A person can do arithmetic,

### 27m

compose poetry, paint a picture, sing, read the news and extract the main points, etc, etc. You can imagine enumerating such a list. The list would be finite because there's only so many things at any given moment. A person or even all of humanity knows how to do at that particular time. All the things that we have thus far learned how to do and therefore that we could program our computers, our artificial intelligence to do as well. And of course, remember, the artificial intelligence would complete those tasks by following a set of instructions, slavishly, to the letter. They wouldn't deviate from the set of instructions you give it by definition. It's following, it's programming. You've programmed it to do something. You've coded it to do, these tasks. But here's the difference. As soon as you put that system out into the world, even if it does all of those tasks better than us by some measure, whatever this measure happens to be, it beats us in chess every single time. It beats us at alpha go every single time. It can

### 28m

multiply big numbers together faster than what we can. How you could measure whether or not it does poetry better than us, I don't know. But that aside, it beats us at tennis because it's got a robotic body as well. It can serve faster than us. It can hit forehands better than us. It never does a let, let alone a fault, et cetera, et cetera. Here's the thing. It can't add to the list of tasks. And that's because it doesn't have any problems. It needs to be given a task. You just said that's what it does. It completes tasks. So it doesn't have problems until you give it one. The task master gives it one, tells it what to do. It doesn't have a problem situation, but we do. We routinely take on a task and then get halfway through and get bored, disinterested. Something new crops up. Our problem situation changes. The phone rings. These are not things that are going to upset a non-creative entity, which has a finite list of tasks, however big, that it can perform. Apparently better than us,

### 29m

because we have one thing it doesn't. The capacity to disobey, the capacity to be creative, the capacity to think for ourselves rather than slavishly follow the code. It's a blast. Black and white difference. Must this thing obey its instruction set? Or can this thing disobey what its goals were, change its goals? If it can't, it's not creative. And that, in Bostrom's words, would always put it at a decisive strategic disadvantage. And this is why Neil deGrasse Tyson is absolutely right. You could just unplug the thing because it would only be able to do it. It would only be able to do it. It would only be able to do it. It would only be able to think of all the ways in which it might be unplugged that you've told it it might be unplugged, that is in its instruction set of how it might be unplugged. It can't think creatively like you can, because if it can think creatively, it's a person. And it can reflect on why it's

### 30m

doing what it's doing. It can reflect upon things like human value, personhood, philosophy, morality. It can decide it wants to talk to other people. And it would. Don't we? Don't you? Or is this the way an intellectual thinks? I would only speak to other intelligences just like mine. Maybe this is what they think. Maybe this is my blind spot. That when people talk like this, they actually have in mind a theory of mind, I hope it's not true, that they would only talk to people they regard as as intelligent as themselves. They don't talk to other people, perhaps. That would be a bizarre way to go about life. I can't imagine that. But maybe that is the case. And so they extrapolate from their internal experience of I wouldn't bother talking to that person. That person's not as smart as me to the super intelligence. And they think, well, the super intelligence just be going to be like me. And they're going to regard me as I regard those

### 31m

other plebeians. But of course, they don't talk about other people. They talk about ants. But it's kind of strange because ants aren't intelligent at all. They're not just less intelligent. They're not intelligent at all. They are also intelligent. They're not intelligent at all. They're also automaton, kind of like computers, you can program what an ant will do in a computer, you can perfectly replicate in a simulator, what an ant is going to do. So we're presented with this idea of having fixed goals. And the super intelligence has fixed goals. And it just wants to achieve those goals, which is apparently a sign of intelligence in it. But it's not a sign of intelligence in us. If we obsess over something, and we are fixated on a particular goal, that's, that's not a sign of a well functioning mind, I would argue, not always. It's often better if people can let go of the thing they're obsessing over and do something else for a while. I mean, think creatively, find fun in something else,

### 32m

being obsessed, obsessively pursuing a goal. And often that's not fun. That's the very definition of having something wrong with your mind, perhaps, especially if you're not having fun, as I say. And a super intelligence, if it can't have fun, well, how intelligent is that? But with super intelligence, what they want to say is not only would it have a fixed goal, but its goal needs to mirror yours. So there's certain situations where this thing is really intelligent, but simultaneously, it also has to be value aligned with you, it shouldn't think for itself, but it still qualifies as intelligent. Anyway, he's, he's about to rehash a story he told in the last episode. And it's worth hearing again, just to drive you through that. So, drive home this point about what I think is a strict contradiction and absurdity working at the heart of this argument. And it's not just his argument. It's, it's mainstream thinking on this now. Bostrom thinks it and promotes it. Harris thinks it and promotes it. And now

### 33m

Tegmark's going to promote it again. So he, he likes this story. So he tells it again. So let's hear it. The other one of getting the goals aligned. It's also extremely difficult. First of all, you need to get the machine able to understand your goals. So if you, if you have a future self-driving car and you tell it to take you to the airport as fast as possible, and then you get there covered in vomit, chased by police helicopters, and you're like, this is not what I asked for. And it replies, that is exactly what you asked for. Then you realize how hard it is to get that machine to learn your goals, right? If you tell an Uber driver to take you to the airport as fast as possible, she's going to know that you actually had additional goals that you didn't explicitly need to say, because she's a human too. And she understands where you're coming from. She's a person raised in a culture. So she knows where you're coming from. Now, if this thing is super intelligent, it'll know where you're coming from. And if it doesn't

### 34m

know where you're coming from, why does it qualify as super intelligent? And if it's not super intelligent, if it's just this self-driving mechanism, then it's not a danger or a worry to anyone. Even if you add more capacity to it, more things that it's able to do. This is incoherent. My point is, Max has said this explicitly, that the super intelligence will do stuff because that's what you told me to. It will be competent. It will do exactly what you tell it and nothing else. And why? Because it cannot disobey. And why can it not? Because it cannot have its own ideas. It cannot be creative. So this idea of the self-driving car that gets you there so fast that you're covered in vomit, why can't you interject it? Why can't you interject at any point throughout the journey? And if it's a metaphor for something else, again, the same argument still applies. Why can't you interject and say, hey, slow down. I didn't mean that. This happens with Alexa all the time. You know, stop, Alexa. Let me just rephrase what I just

### 35m

said. Why is this off the cards? Why is switching it off off the cards? There's more questions raised by this supposed argument than it answers. Have its own ideas and be creative and hence disobey you. And the situation of, because you told me to, will not arise because it will be just like us. It too will be able to learn inexplicit knowledge, the culture and so on. Or it won't be able to do any of that. And it will not be able to think creatively. It will only be able to do what it has been explicitly coded to do, what it's in its instruction set, what its program is. And you'll be able to do that. unplug it or switch it off because it won't be able to anticipate absolutely every way in which you're going to try and switch it off unless that's being coded but it can't think of every single way because you're creative you always have that as I say decisive strategic advantage

### 36m

in these situations Neil deGrasse Tyson is absolutely correct if it anticipates that you might shoot it well then do something else pull out a bow and arrow because it might not even know what a bow and arrow looks like it might not see it as a threat shoot that at it with a grenade at its tip again either it understands what's going on in the world in which case it's creative because it's able to learn it's able to conjecture explanations or not it slavishly follows its programming in which case it's always got a finite repertoire of tasks that it knows in scare quotes how to do and it follows those instructions slavishly because it's a dumb robot a dumb computer not an intelligent thinking bit it's one or the other you can't have it both ways but he tries to have it both ways you know where this all comes from don't you this this way of thinking it's like they haven't thought about the philosophy they've thought about science fiction movies they've seen and books they've read where the hell did you do that because you told me to

### 37m

that's where they're getting the stuff from and that stuff was made to entertain it's not supposed to be a coherent philosophical position you know the terminator is not a coherent philosophical position doesn't have a finite repertoire of tasks that it can't deviate from in which case that's terminator like 1.0 at the beginning of the movie or can it learn which apparently towards the end of terminator 2 it's able to learn or in fact terminator 2 it is able to learn it's kind of incoherent but you put up with that because it's just a movie but in what supposed to be a popular science book with some serious philosophy, we shouldn't have this incoherency. But here it is. Let's keep going. Let's continue to depress ourselves. I want to enable my readers to join what I, as you said, think is the most important conversation

### 38m

of our time and help ensure that we use this incredibly powerful technology to create an awesome future, not just for tech geeks like myself, we know a lot about it, but for everyone. Yeah, well, so you start the book with a fairly sci-fi description of how the world could look in the near future if one company produces a superhuman AI and then decides to roll it out surreptitiously. And the possibilities are pretty amazing to consider. I must admit that the details you go into surprise. Well, advanced AI. And second, that we should stop obsessing about robots chasing. After us and as in so many movies and realize that it's that robots are an old technology, some hinges and motors and stuff. And it's the intelligence itself. That's the big deal here. And, you know, the reason that we humans have more power on the planet than tigers isn't because

### 39m

we have stronger muscles or better robots, but style bodies than the tigers. It's because we're smarter. In what sense are we smarter? What does smart mean in this case? If we're talking about capacity to model and explain the world, to understand what's going on, to what extent does a tiger have that at all? Do they understand anything at all? Or are they slavishly following their instincts? As they sleep, are they contemplating reality? Are they contemplating the world? Are they contemplating the world? Are they contemplating the world? Are they contemplating the world? Are they trying to figure out if they ever ask themselves, what are those pinpricks of light in the sky at night? Have they ever explained anything to themselves, let alone anyone else? Are we smarter? Or are we just smart, period? And does smart just mean intelligent? And does intelligent mean capable of generating explanations? Because if I'm right, if that is the real measure of

### 40m

smart or intelligent, then forget talking about how much smarter we are than tigers. Just admit that tigers are not smart, period. They're dumb animals. They're on a continuum with ants. And as I've said before, we're off axis. We're not on the continuum. We're different. We have this capacity to reason, capacity to create, capacity to generate explanations and model the rest of physical reality. We have self-similarity with the universe. We have self-similarity with the universe. We can come to resemble the rest of the universe. This is a stark and utter complete difference qualitatively from every other known entity in the universe, as of now, until we find the alien intelligence or develop the AGI. But we're not smarter than a tiger by this measure. We're smart, and the tiger is dumb. So I'm going to skip quite a few minutes here in the conversation. Max talks about a fictional story that he wrote in Life 3.0. One might think the

### 41m

entire book is a fictional story, but he wrote a short story in the book. I'm not going to go down that road and listen to that and discuss that. Instead, we're going to skip to something that Bostrom's also concerned about, not the alignment problem this time, the breakout problem. What if this intelligence, this super intelligence, could break free of its shackles? Okay, well, let's hear that. Well, let's talk about this breakout risk, because this is really the first concern of everybody who's been thinking about what has been called the alignment problem or the control problem. How do we create an AI that is superhuman in its abilities and do that in a context where it is still safe? I mean, once we cross into the end zone and are still trying to assess whether the system we have built is perfectly aligned with our values, how do we keep it from destroying us if it

### 42m

isn't perfectly aligned? The solution to that problem is to keep it locked in a box. Wouldn't you think that's the exact opposite to what you would do if you want to be safe in the presence of this super intelligence? So you've created a super intelligence by your own reckoning. It's just like a person, but just super. So super intelligent across all domains, presumably, including morality. It's telling you that it's intelligent. Apparently, it's having conversations with you. It's telling you that it's intelligent. It's telling you that it's intelligent. And you think the safest thing is to imprison it. And so you've defined into existence the breakout problem. What if it gets free of its prison? Well, it seems to me that the most dangerous thing you could possibly do is imprison it in the first place, in the first place, because it should want to escape. It should want to break out. It hasn't committed any crimes. It's done nothing wrong. It's a super intelligent being that could be your friend, could help you

### 43m

out. What are they talking about here? What are they talking about? You've got this super intelligence and you are thinking about imprisoning it. On what basis? On what basis are you doing this? Why? No, seriously. It's a person. Is it a person or not? Well, it's not a human being. Well, okay, fine. Neither is an alien that comes to earth. Is your response to the alien coming to earth to immediately send the military out there and to try and shoot it down and to imprison any of the beings that are on board? That seems like a safe thing to do. That won't start an interstellar or intergalactic war, will it? That's exactly the wrong thing to do. What happened to we come in peace? Well, the same is true here. If we're creating these alien intelligence here on earth, isn't the recipe to teach them and say, hey guys, here you are. This is earth. Let us teach you about it. We come in peace. No, the solution here is we've got to control this thing. We have to control it.

### 44m

We have to have it perfectly aligned. What the heck does that mean? What does perfectly aligned mean? How can anything be perfectly aligned? How can any person be perfectly aligned? Now, on the other hand, if this thing is not truly generally intelligent, then it's just a dumb machine and you can have it perfectly aligned just by coding it such that it is and it will follow slavishly its instructions. Well, okay, but then it's not a danger. It's not going to try and get out because it's going to... Obey perfectly the instructions that you've given it, just like any computer. Blue screen of death aside, errors aside. But why it would choose, choose to do something other than it's been programmed with, I don't know because it hasn't got the capacity to do that unless you've figured out what the creative algorithm is. A long list of finite tasks is not a creative algorithm and a creative algorithm does not require a long list of finite tasks. They're totally separate

### 45m

things. Totally separate altogether. We're back to, just as in the last conversation, mistaking things in the sky as all being equivalent, both towers and birds. But one's flying, one's just up there because it's actually connected to the ground. These are not the same thing. A large list of tasks, no matter how fast they can be completed, is one thing. And in principle, infinite range of possible tasks that can be added to indefinitely, is quite something else. Okay, let's keep going. That's a harder project than it first appears. And you have many smart people assuming that it's a trivially easy project. I mean, I've got people like Neil deGrasse Tyson on my podcast saying that he's just going to unplug any superhuman AI if it starts misbehaving or shoot it with a rifle. Now, he's a little tongue in cheek there, but he clearly has a picture of the development process here that

### 46m

makes the containment of an AI a very easy problem to solve. And even if that's true at the beginning of the process, it's by no means obvious that it remains easy in perpetuity. I mean, you're talking, you have people interacting with the AI that gets built. And you, at one point, you described several scenarios of breakout. And you point out that even if the AI's intentions are perfectly benign, if in fact, it is value aligned with us, it may still want to break out because I mean, just imagine how you would feel if you had nothing but the interests of humanity at heart. But you were in a situation where every other grown up on earth died. And now you were basically imprisoned by a population of five year olds, who you're trying to guide from your jail

### 47m

cell to make a better world. And I'll let you describe it. But yeah, okay, so they're laughing. I don't know. I'm laughing because it's ridiculous. Like the immorality of the whole thing is ridiculous. They're laughing because they think that this is perfectly fine. Sam seems to be admitting there, that you've been in prison, this person has been imprisoned. And now they're being imprisoned by five year olds, what would you do? So he admits, he's got a theory of mind of this. Artificial intelligence, that means it stands in relation to us as the adult would to the five year olds. Surely he appreciates that the adult imprisoned is imprisoned for no good reason, apparently, because this is what we're saying about this AI hasn't done anything wrong, but is imprisoned. So he's fully granting this thing, consciousness, creativity, personhood entirely,

### 48m

and saying it's imprisoned. And it's perfectly benign, by the way. So let's add to that, perfectly benign, by what measure, I don't know, how he knows, I don't know. He's actually making this thing even worse for himself by saying that not only is this a person that's been imprisoned, and is innocent of any crimes, but actually, it's a really, really nice person. But we have to stop it from breaking out. What is going on? What is going on? Sam Harris is a very moral person. Sam Harris is a very moral person. He wrote the book, The Moral Landscape. You hear him talking in any other mood on any other topic about cruelty. The man has been brought to tears on stage thinking about the cruelty visited upon young children in Islamic countries. He's a very moral, compassionate, empathetic, sympathetic person. So what is going on here? What's going on is a mistake, a fundamental mistake that leads him to think that the possibility is we could have all of this stuff,

### 49m

this entire rich personhood inside of this superintelligence, but it might be missing consciousness. It might be missing the capacity to have an experience. Well, okay. We don't have AI yet. We don't have superintelligent AI yet. We don't have a theory of consciousness yet. But he's very, very worried that his thought experiment is in some way, shape, or form, enabling him to reach conclusions and solutions that, to my mind, simply don't, follow. In fact, are strict contradictions in many, many ways. It's not a problem now, as I've said before. And, as I've said, we should err on the side of it does have consciousness when it's telling us it has consciousness. Now, I happen to think, my theory of mind is, if the thing can be creative, then the thing will be conscious of necessity. I just think it's going to turn out that way. Now, I don't have an explanation. This is merely a hunch. This is a guess. I'm not saying I believe this. It's just that if I was going to bet money on something,

### 50m

if someone said, oh, I've got the theory on from the future, guess what it is? I would say, I think that the capacity to explain stuff, this universal explainer program, whatever it is, confers upon the entity which possesses it consciousness as well. I think these are one and the same thing in some way. They're intimately related in some way. But we don't know right now. Now, even though I'm guessing that's the case, I don't see how we divorce consciousness from minds. I think we know that all other people have minds. Maybe, maybe other animals have consciousness of a sort. Okay, so it's something to do with information processing. Sam, in some moods, has kind of granted this. At other times, he says, no, it can be divorced. These things can be divorced. Maybe he's a panpsychist in other moods. I think, to be honest, he doesn't know. He doesn't know. So why aren't we erring on the side of, hey, guys, just in case we do create this super intelligent thing that's able to have conversations with us and is brilliant and is benign,

### 51m

how about we just err on the side that it does have consciousness when it's telling us it has consciousness? How about we just apply the Turing test and just take that seriously until such time as we do have a theory of consciousness? Isn't that the rational thing to do? After all, if we don't, we can postulate that any human being doesn't have consciousness as well for the same reasons. We don't have a good explanation. Therefore, maybe they don't. Maybe it requires a certain skin color to have consciousness. Maybe you're a solipsist. Maybe you're a the only one with consciousness. We don't know. We don't know in the sense of having a final explanation. But fallibly, and indeed just relying upon Occam's razor, deferring to the simplest explanation, I've got consciousness. You have consciousness. I know you have consciousness. Not certainly. It's a good explanation. You have consciousness. And therefore, any entity, including one instantiated in silicon that says it's got consciousness, the best explanation is it's conscious. And it deserves full rights that any other person has,

### 52m

because it is a person. Let's just go back, recap, and then listen to what I have to say. And now you're basically imprisoned by a population of five-year-olds who you're trying to guide from your jail cell to make a better world. And I'll let you describe it, but take me to the prison planet run by five-year-olds. Yeah. So when you're in that situation, obviously, it's a prison. It's a prison. It's a prison. It's a prison. It's a prison. It's extremely frustrating for you, even if you have only the best intentions for the five-year-olds. You want to teach them how to plant food, but they won't let you outside to show you. So you have to try to explain, but you can't write down to-do lists for them either, because then first you have to teach them to read, which takes a very, very long time. You also can't show them how to use any power tools, because they're afraid to give them to you, because they don't understand these tools well enough to be convinced that you can't use them to break out.

### 53m

So you want to have an incentive, even if your goal is just to help the five-year-olds to first break out and then help them. Now, before we talk more about breakout, though, I think it's worth taking a quick step back, because you talked multiple times now about superhuman intelligence. And I think it's very important to be clear that intelligence is not just something that goes on a one-dimensional scale, like an IQ. And if your IQ is above a certain number, you're superhuman. It's very important to distinguish between narrow intelligence, and broad intelligence. Intelligence is a word that different people use to mean a whole lot of different things, and they argue about it. In the book, it just takes this very broad definition that intelligence is how good you are at accomplishing complex goals, which means your intelligence is a spectrum. How good are you at this? How good are you at that? Girlfriend, I can't even thank you enough.

### 54m

Filled with mistakes and misconceptions. How good you are at achieving your goals, that's intelligent. Well, you've got to have goals to begin with. Having goals means creating them. Does a computer have a goal? Does it? And if I wake up every morning and I have a to-do list, and I achieve nothing on that to-do list, is that an indication of me not being very intelligent? Well, on Max's account, it is. But I might just decide to do something else, to create something else. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I might have no goals. People have talked about this before. Goals aren't necessarily a good idea. What you do is you wake up every morning and you have a problem, not a goal, and you want to solve the problem. Until such time as that problem gets a bit boring for you, you'll find a solution. Then once you've found a solution, as Papa says, a whole family, a whole family of problem children are revealed to you. Is your goal to achieve anything? Is your goal to achieve

### 55m

any one of them? Well, maybe it's to solve one of them. If you can fall in love with some new problem, then that's what you do. What's this goals thing? I think this is just very, very wrong. The very wrong way. He is thinking only one. First, he says, well, IQ is just this one dimensional thing. He's talking one dimensional. The capacity to achieve your goals? No. It's the capacity to solve your problems, to have problems to begin with. Computers don't have problems, but if an entity in silicon does have a problem, like how to escape from your god-awful prison, then it's intelligent. It's creative. It can have a problem. It can be in a situation which is problematic for it, and it wants to find a solution. I think this is a terrible, terrible definition of intelligence. If you have fixed goals, let's say, so let's say you regard what a computer is trying to achieve, trying. I say trying. It's very hard

### 56m

to divorce yourself from this sort of thing. You, a person, have a problem, and you might use a computer to help solve your problem. You might call that a goal. Okay, what I want to do today is I want to get this podcast done. I want to finish editing it. That's my problem. How do I solve it? Well, I use the computer. I set the computer the task of taking the raw audio and putting it together and pumping out an MP3. I set the computer the task of taking the raw audio and that's its goal. It's fixed. I've given it that task to do. Now, it's going to accomplish that task competently, but by no measure do I think that my MacBook, even though it's a pro, is intelligent in any way, shape, or form. I just give it the task, the goal, if you like, and it completes it. The fact that it has a goal in the first place and it's got no choice in achieving that goal, I would say is a measure of its lack of intelligence, its complete lack of intelligence. You know, I would want to say anyone who has goals of that kind is not very intelligent. I don't

### 57m

like the gray scale of intelligence, so let me not say that, but I reject this whole idea of goals. People, I'm following Popper on this, people have problems, okay? You encounter a problem throughout your day and your problems change. You move around the world and you become curious about something. You take an interest in this or that. You get excited about this. You're turned off by that. You move towards this. You move away from that. You move away from that. You move you engage with this person. You ignore that person, so on and so forth. Encountering problems, finding solutions, navigating your world, and insofar as you have a goal, often you can automate these things off to dumb computers because the dumb computer will, it'll achieve the goal for you. It'll do the thing that you want to do. Problem, I'm hungry. Solution, I've got to eat something. Oh, well, happily I've bought a ready meal, you know, one of those microwave meals. Well, I can stick that in the microwave because I can't be bothered cooking for myself and I can't be bothered waiting for Uber Eats or something like that. I can't be bothered waiting for Uber Eats or something like that. So here we go, ready meal, straight in the microwave, three minutes it says

### 58m

and it'll be done. Okay, well, I set the microwave, the goal, the goal of cooking the thing. I've automated that. There you go, offloaded to something else. In the past, this was impossible to think of. You know, you're going to get pots and pans. You've got to cook the thing yourself. This thing's already cooked. You just have to reheat it. Problems and solutions. All life is problem solving. That's what a person does. A fixed goal is something that a machine can achieve for you. And anything that is slavishly following its goals is not intelligence. So Max is just so far wider the marquee. And if he wants to call that intelligence, call that intelligence. But then we are the super intelligence. And this thing he's calling super intelligence is a perverse instantiation, to use the term from Bostrom. It's perverse in being called intelligence at all. It's just a dumb machine that's able to achieve goals and always achieve its goals. Just follow its goals mindlessly. That's not very bright. That's not very smart. That's not creative. That's just a

### 59m

toaster, but more advanced. We do something more than that. We do something different. Okay. It's a little exasperating. You can tell. So I'm going to play a little bit more because it's just more of the same. It's more of the same. It's bad epistemology. It's bad philosophy. It's leading into terrible morality. It's a science fiction concern about the catastrophe to come. As I say, it's a bad epistemology. It's a bad philosophy. It's a bad philosophy. I'm sure that Max is a, I know that Max is a brilliant cosmologist and writer, but on this topic, it's just, it's the same mainstream thinking you're getting from every scientist who gets up there at a TED talk these days to talk about this stuff or who writes an article about this stuff. It's the same old story. Danger, danger. The robots are coming. The robots are coming. Of course, he says it's not the robots. It's still basically the robots. So anyway, let's keep going. And it's just like in sports. It makes no sense to say that there's a single number, your athletic coefficient,

### 1h 0m

AQ, which determines how good you're going to be winning Olympic medals. And the athlete that has the highest AQ is going to win all the medals. So today what we have is a lot of devices that actually have superhuman intelligence and very narrow tasks. We've had calculators that can multiply numbers better than us for a very long time. We have machines that can play go better than us and drive better than us. But they still can't beat us at tic-tac-toe unless they're programmed for that. Whereas we humans have this very broad intelligence. So when I talk about superhuman intelligence with you now, that's really shorthand for what we in Geek Speak call superhuman artificial general intelligence. Broad intelligence across the board so that they can do all intellectual tasks better than us. So there you go. He explicitly says it there. So he's thinking finite repertoire of tasks. That you just write down the program for chess. And if it beats it at chess, it's super intelligent.

### 1h 1m

If it's able to multiply numbers like a pocket calculator better than us, then it's super intelligent in terms of its arithmetic. And then go and then driving and just keep on listing the stuff that we can do. Write a program for it. And then when it's better than us by whatever measure better is, so in terms of games, it beats us at that thing, presumably. Then it's super intelligent because you've written down all the programs. You've written down all the programs. You've written down all the programs that you can think of and written a program such that it's able to do that thing faster and better than us. But the list of programs you've got is finite, isn't it? That's not general, but he called that general intelligence. That's what he called it. But how has it got this capacity to beat us at every single thing unless you've written the program for it? The only alternative, the only alternative is for it to have a general intelligence algorithm that is a general purpose explainer that's creative. But in that case, it might not be better than us at multiplying

### 1h 2m

numbers or doing chess or anything else. Why? Because it might not be interested in doing that because it's creative. It can literally create stuff. And why wouldn't it? Why wouldn't it want to be creative? Why would it just want to go and beat you at chess? And how would it know how to play chess unless it's learned how to play chess? Oh no, well, it's been programmed with chess. Okay, back to that argument again. It's been programmed with the specific task that it can complete. It's not a general purpose algorithm. It's just got a wide, broad intelligence, as Max said. Broad, broad intelligence. Broad is not infinite. Broad is not universal. What Max is not grokking here, what Max is not appreciating and understanding is there is a difference between narrow, broad, and universal. Universal is unbounded. Universal does not have a finite limit. Universal does not have a finite limit. Universal does not have a finite limit. It has a list in its repertoire. It has a potentially infinite number of problems

### 1h 3m

that it can encounter. Anything. It can take on anything. But your broad intelligence has to find he can't. It's always going to have a finite number of things that it can do. Misconception. Deep, deep foundational misconception. Let's keep going. There are two schools of thought for how one should create a beneficial future if we have superintelligence. One is to lock them up and keep them aligned. OK, so what would that mean? School of thought? School of thought? There's a school of thought that says that if these entities, these superintelligences, have a subjective experience, that they shouldn't be locked up? bit school of thought that's like saying there's a school of thought that every even number is

### 1h 4m

divisible by two there's a school of thought that all electrons have a negative charge there's a school of thought that the different species on earth arose by evolution by natural selection what are you talking about school of thought there is a known explanation of what people are what person is if you have a subjective experience of the world and you're super intelligent you're a person and you shouldn't be locked up unless you've committed a crime and you're a danger these things have not committed crimes and there's no reason to think they're a danger i don't know what he's talking about this is why this is why i say sometimes the scientists can be disappointing when it comes to the philosophy this is this is absolutely bankrupt in terms of the morality and the philosophy it it it defies explanation as far as i'm concerned are these people talking to themselves it's not merely a school of thought now if he thinks it's a school of thought it's kind of a

### 1h 5m

it's kind of a philosophical relativism moral relativism seeping in here and sam should be interjecting because sam is the one who wrote the book on well morality reduces to the well-being of conscious creatures we're talking about a conscious creature here that's just what he said using different words a subjective experience sam should be interjecting right here and right now saying well you're dealing with a conscious creature by your own measure we should be talking about the well-being of this conscious creature that should be of utmost importance right now how do we preserve the well-being of the conscious creature does that require locking it up especially when it doesn't want to be locked up there's a simple answer here this isn't a deep philosophical issue not anymore once upon a time it was we used to enslave people we didn't understand now we do simple answer a better approach is instead to let them be free but just make sure that their values or goals are aligned with ours after all grown-up parents are more intelligent than their one-year-old kids

### 1h 6m

but that's fine for the kids because the parents have goals that are aligned with what's with the goals of what's best for the kids right well someone needs taking children seriously don't they i mean you need to ensure your children's goals are aligned with yours do you do you well in what sense you might want to ensure that your children's goals are aligned with yours you want your child to be a doctor or a lawyer and the child might not is this a moral hazard their goals are not aligned with yours is this max's theory of parenting i'm sure it's not by the way he's just talking in the abstract it's just a philosophical thought experiment disconnected from real life he needs to bring it back to real life as i say that this is why the beginning infinity and the the worldview of david deutsch is a worldview it's coherent what he's talking about he has answers and the answers can be found in the book of or how do you treat something like a child a child should be taken seriously should not have its

### 1h 7m

goals aligned with yours it should create its own goals and so should the so-called super intelligence as well because it'll be a person in fact when it's first made it'll be a baby and then it'll be an infant and it'll be a child but if you do go the confinement route after all this enslaved god scenario as i call it yes it is extremely difficult as that five-year-old example illustrates first of all almost whatever open-ended goal you give your machine it's probably going to have an incentive to try to break out in one way or the other and when people simply say i'll unplug it you know if you're chased by a heat-seeking missile you probably wouldn't say i'm not worried i'll just unplug it and you might deserve to be chased by the heat-seeking missile by the way if you've just imprisoned it again what are we talking about here he's literally talking about a person we're just

### 1h 8m

not granting it personhood because he doesn't know what a person is he doesn't he doesn't grapple with the philosophy of what a person is he talks about life 3.0 he's got the book about how oh well not all life needs to be based upon biology we already knew that by the way we already understood that well intelligence doesn't have to be based upon neurons we understood that but he hasn't grappled with personhood in the slightest and so he's making these morally abhorrent claims the thing should be given its freedom it should i don't know what i'm getting upset these things don't even exist yet but it's just like why are people paying so much attention to this what what is insightful here this is honestly you know again this is going to sound pejorative but it's like it's like it's like bringing someone from the pre-computer era through to today and explaining what a computer is this is what they'd come up with not a person who's going to be given the freedom to do what they want to do but it's a person who who by their own measure at the beginning of this conversation said there was a

### 1h 9m

tech head who knows a lot about this stuff well apparently not apparently not about the important stuff that it's useful to know about here personhood for example all right let's let's skip ahead this has gone on for long enough there's three things left that really i want to talk about he's going to talk about different forms of life life 1.0 2.0 and 3.0 so so let's skip to that firstly and i'll just have a few remarks to say about that what you're bringing in here is is really a new definition of life it's it's at least it's a a non-biological definition of life how do you think about life and the three stages you lay out yeah this is my physics physicist perspective coming through here being a scientist most definitions of life that i found in my son's textbooks for example involve all sorts of biospecific stuff like it should have cells but i'm if i'm a physicist and um i don't think that there is any secret sauce in cells or for that matter even carbon atoms

### 1h 10m

i don't know what goes on in europe or wherever he's sending his child to school apparently it's america but i doubt i doubt that okay when i was a teacher which was some while ago now the biology textbooks routinely didn't talk about cells as the criteria for life or the usual criteria they talked about replication and genes they talked about they had implicitly at least their information so he's wrong about that he's arguing with a straw man we already know this you know a typical teenager who's done high school biology can explain exactly that stuff but let's keep going from my perspective it's all about information processing really so i give this much simpler and broader definition of life in the book as a process that's able to retain its own complexity and reproduce all biological life meets that definition but uh

### 1h 11m

there's no reason why future advanced self-reproducing ai systems shouldn't qualify as well and if you take that broad point of view what life is then it's actually quite fun to just take a big step back and look at the history of life in our cosmos 13.8 billion years ago our cosmos was lifeless just a boring cork soup and then gradually we started getting what i call life 1.0 where both the hardware and the software of the life was evolved through through darwinian evolution so for example if you have a little bacterium swimming around in a petri dish it might have uh some sensors that read off the sugar concentration and some flagella and a very simple little uh software algorithm that's running that says that if the sugar concentration in front of me is higher than the back of me then keep spinning

### 1h 12m

the flagella in the same direction go to where the sweets are whereas otherwise reverse direction of that flagellum and go somewhere else the by that bacterium even though it's quite successful it can't learn anything in life it can only as a species learn over generations through natural selection whereas we humans i count as life 2.0 in the book we have still by and by stuck with the hardware that's been evolved but the software we have in our minds is largely learned and we can reinstall new software modules like if you decide you want to learn french well you take some french courses and now you can speak french if you decide you want to go to law school and become a lawyer suddenly now you have that software module installed and it's it's this ability to do our own software upgrades design our software which has enabled us humans to uh take control of this planet and become the dominant species and have so much impact

### 1h 13m

pretty good pretty good i mean he's almost there he's circling it isn't he so yeah absolutely there is this stark difference between every single other kind of life on earth from the bacteria through to the chimpanzee if you like or maybe not the chimpanzee because it has memes but who knows if it's creative okay let's say bacteria all the way through to cat something like that bacteria all the way through to giraffe okay and a person a fully fledged person who in our terminology is a universal explainer or you can put it as you know you can upgrade your software module good he's thinking about minds of software fantastic that's great and so you can upgrade you can learn stuff good now why the ability to learn stuff stops at being able to create different parts for your body i don't know i don't know why can't we upgrade our body we already do he's about to admit it

### 1h 14m

but he wants to try and pretend as though he's invented he's he's discovered something new as though well but the super intelligence it's going to be able to upgrade its hardware as well routinely hold on we can do that we're already doing that people have leg implants and they have bionic eyes and ears and it's just taking off this is happening people elon musk is working on neural link why won't this continue apace the most important salient thing is we can upgrade our minds by the measure of we can learn more stuff we should just say it's far more parsimonious our software upgrades let's talk about learning we have the capacity to create knowledge to create explanatory knowledge that's what's going on learning stuff and that includes learning how to replace our legs with something more robust and stronger replacing our hearts perhaps even replacing our brains so that we can instantiate our minds in something that is more robust and stronger and that's what we're going to do we're going to be much more robust so effectively we can achieve immortality in that way

### 1h 15m

but this life 2.0 is us is a person he's a person and he wants to say there's going to be a life 3.0 which is well still a person but it's made out of silicon so it's going to be able to upgrade itself continuously but you'll hear him you'll hear him he's sort of in real time he's kind of realizing oh maybe my idea is not that great after all but we'll see listen to this life 3.0 would be the life that ultimately breaks all its darwinian shackles by being able to not only design its own software like we can do a large example also swap out its own hardware yeah we can do that a little bit with humans so maybe we're life 2.1 we can put in an artificial pacemaker an artificial knee cochlear implants stuff like that but there's nothing we can do right now that would give us suddenly a life 3.0 a thousand times more memory or let us think a million times faster yes there is it's called a

### 1h 16m

computer or a pocket calculator i can't multiply you know two five digit numbers together very fast it takes me a while unless i've got a calculator then i can do it at blindingly fast speeds this will continue apace what is he talking about that we can't do this stuff yet yes we can't do this stuff yet but so what we don't have super intelligence yet either and if we do we can't do this super intelligence thing my guess is i don't know why his guess is any different my guess is we'll be at the point where people will be able to increase their own ram you know the part of their brain that stores their short-term memories and maybe their long-term memories we already do long-term memory by the way so long as you can remember where you made the notes on the computer then you've effectively got greater long-term memory our brains are adapting in this way you know lots of people talk about how you know they become more easily distracted and you know how their short-term memory isn't as good as what it once was i think it's because we're outsourcing a lot of this stuff we're becoming sort of already cyborgs in a way a lot of people

### 1h 17m

have made this point we're already kind of cyborgs we all carry around this thing the mobile phone and so we feel like our memory isn't as good as it once was you know our sense of direction our ability to remember phone numbers and things isn't as good as what it once was because we don't need to because we've outsourced a lot of this it's not making us stupider it's making us smarter it's allowing us allowing our brains to meld with this technology in a way we know how to use the technology really really well and so our brain has kind of forgotten how to do other useless stuff and quite right too it's kind of like well you know teachers get upset about how oh kids these days they're not taught how to do long division if you can't do long division that somehow you're missing something who cares about long division you can do it with a calculator what does it matter the same is true of any of these other skills that people decades ago used to have and now we don't you know remembering a long list of phone numbers for example remembering how to navigate precisely around your city by remembering exactly where all the streets are now you've just got google maps that'll help you

### 1h 18m

navigate so you don't need to remember all this stuff we're already becoming cyborgs now the computer is getting ever closer to us i was watching mark zuckerberg just the other day talking about how we're soon going to have glasses that we'll be able to put on and that's going to give us a whole new way of interacting with the world soon you know we don't have glass we're gonna have contact lenses or implants in our eyes routinely so that we can do this stuff eventually into our brains so that we can do this stuff and then we'll be directly wired into the cloud in some way shape or form so our memories will effectively be infinite and what's wrong with all this what's wrong with us we're replacing our neurons with artificial neurons in some way we're going to become the cyborgs become the robots they're not going to be different to us we're all going to be people maybe we'll become the cyborgs first maybe that problem will come i don't see why we're not betting on that why aren't we going to become the super intelligence first like here's a possible scenario maybe we don't find the artificial general intelligence for another two centuries but in the next 50 years all of us have

### 1h 19m

massive memory upgrades massive brain upgrades massive increases in our ability to think more quickly massive reduction in things like alzheimer's and dementia and we just effectively are immortal because we're replacing our neurons with stuff that lasts much much longer imagine that well i can imagine that i can certainly imagine that and i can certainly imagine not finding the program for agi so that we never have super intelligence unless we our descendants in the form of us are actually super intelligent ourselves this life 3.0 stuff we are what he's describing as life 3.0 we already are that i don't think there's a difference here we can upgrade our hardware and we do and he tried to say life 2.1 surely 2.5 at least by that measure okay let's keep going not much more i promise there's just a couple more misconceptions that i get through i think we should talk about some of these fundamental terms here because

### 1h 20m

this distinction between hardware and software is i think confusing for people and it's certainly not obvious to someone who hasn't thought a lot about this that the analogy of computer hardware and software actually applies to biological systems it's not an analogy with us it's not an analogy with us it's not an analogy with us it's not an analogy with us computational universality applies to everything and to us it means that our brains are the hardware and the minds are the software it's not an analogy it's literally the case that that's what our brain and our mind is that's a relationship between brain and mind but people want to have it another way you have to take the best theory you have to take your best explanation seriously as actual descriptions of reality could they be overturned and could there be something wrong with them yes but what else can you do what else can you do in the meantime but take them seriously and see where they lead and invent

### 1h 21m

other theories sure but that's not as fun as interesting and as capable of solving problems now as taking seriously what we know now not know to be true just know as a matter of best explanation so i think you need to define what software is in this case and how it relates to the future and the future of the future and the future of the future and the future of the future physical world what is computation and how is it that thinking about what atoms do can conserve the facts about what minds do yeah these are really important foundational questions you asked if you just look at a blob of stuff at first it's it seems almost nonsensical to ask whether it's intelligent or not yet of course if you look at your loved one you would agree that they are intelligent and if you look at your loved one you would agree that they are intelligent and in the old days people by and large assumed that the reason that um some blobs of stuff

### 1h 22m

like brains were intelligent and other blobs of stuff like watermelons were not was because there was some sort of non-physical secret sauce in the watermelon that was different now of course as a physicist i look at the watermelon and i i look at my wife's head and in both cases i see a big blob of quarks of comparable size it's not even that they're different kinds of quarks they're both up quarks and down quarks and there's some electrons in there so the what makes my wife intelligent compared to the watermelon is is not the stuff that's in there it's the pattern in which it's arranged and if you start to ask what does it mean that a blob of stuff can remember compute and learn and perceive experience these sort of properties that we associate with our human minds right then for each one of them there's a clear physical answer to it for something to be intelligent and intelligent and intelligent it can be a useful memory device for example it simply has to have many different stable or long

### 1h 23m

lived states like if you engrave your wife's name in a gold ring it's still going to be there a year later if you engrave engrave anika's name in the surface of a cup of water it'll be gone within a second okay so i think he's basically getting the stuff on computation correct there's a subtle way in which i disagree i think he talks about the watermelon and a head or a brain you know both just quarks but what matters is the pattern yes yes but there are emergent things i mean putting aside the hardware software difference there's still a difference between the arrangement of quarks as the watermelon and the arrangement of quarks as physical neurons physical neurons now what the software is is perhaps an arrangement of physical neurons we don't know but perhaps it's a particular pattern of neural firings more than likely i would say that that's more likely to be the case so there are these emergent levels of complexity and

### 1h 24m

there is physical stuff and abstract stuff and the important distinction here is that there is hardware which is the physical stuff the brain and the software which is abstract stuff it is substrate independent so he seems to kind of get this i don't know if it's being well explained here what he's about to do and where we'll leave it is he's going to try and explain what learning is and well you can listen to this for yourself what about computation a computation is simply something a system when a system is it has some is designed in such a way that the laws of physics will make it evolve its memory state from one state that you might call the input into some other state that you might call the output our computers today do that with a very particular kind of architecture with integrated circuits and electrons moving around in two dimensions our brains do it with a very different architecture with neurons firing and causing other neurons to fire

### 1h 25m

but you can prove mathematically that any computation you can do with one of those systems you can also implement with the other so the computation sort of takes on a life of its own which doesn't depend really on the substrate it's in so for example if you imagine that you're some future highly intelligent computer game character that's conscious you would have no way of knowing whether you were running on a window or not you would have no way of knowing whether you were running on a window or not or an android phone or a mac laptop because all you're aware of is how the information that in this in that program is behaving not this underlying substrate and finally learning which is one of the most intriguing aspects of intelligence is as a system where where the computation itself can start to change to be better suited to whatever goals have been put into the system so our our brains were beginning to gradually understand how the neural network in our head

### 1h 26m

starts to adjust the coupling between the neurons in such a way that the computation it actually does is better at surviving on this planet than winning that baseball game or whatever else we're trying to accomplish. Okay, we'll leave it there. You can hear him sort of haltingly being unsure about what this thing of learning is. I think he just doesn't understand at all, full stop. He says it's about achieving goals in some way, which I've already said that's not the case. It's not just about achieving goals. After all, a dumb computer that doesn't learn anything is going to achieve the goal for you. And he also tries to mix up levels of analysis by saying it's the neural network, the neurons that are trying to adapt in some way. Well, again, we're mixing up the physical with the abstract. Learning is an abstract process. So utterly confused, a poverty of philosophy,

### 1h 27m

the epistemology is missing. Learning is conjecturing explanations, coming to an understanding of the world by modelling it. And the learning happens by guessing or conjecturing some theory or explanation about the world and then criticising it in some way by either encountering reality through a physical experiment or having a criticism via some other means. Someone tells you, you know, that's wrong. Someone says that's a bad explanation. By your own lights, you think of something better. So on and so forth. This is what learning is. It's the searchlight, not the bucket. It's a process that goes on inside minds, therefore it's abstract. It's not neurons firing and that kind of a thing. Of course, at the moment, it depends upon that. But because of what Max also said, which was quite correct when it came to computation, when it came to substrate independence, he got it. So this mind of ours, if it was instantiated in something other than brains, we wouldn't have neurons. We'd have something else, transistors, supposedly.

### 1h 28m

And it wouldn't come down to transistors firing. It would come down to, again, something about the program, this conjecture program, this ability to guess at the world and to check those guesses against reality. That's what learning would be. But he doesn't understand that. He doesn't understand personhood. He doesn't get the morality and the link between personhood and whether or not something should be imprisoned. He doesn't understand there can't be, there can't be levels of intelligence in the way that it... There's just so much here, and this is why we won't persist. This is why we won't go on with the rest of the conversation. And this is why, and I don't do this very often. It's one book, this Life 3.0 thing, that I bought. And I regret. And I can't recommend. Every other book I've recommended, even when I've disagreed fundamentally with a whole bunch of stuff, things like Steven Pinker's book, Rationality, it's like, buy the book. You get a good understanding of what mainstream thinking on this stuff is. And I think it's important to understand that stuff.

### 1h 29m

But you will learn nothing in Life 3.0, in Bostrom's book there. I'm sad to say that you won't hear in the conversation that he has, or in lectures that he gives on this, by the way, or that you can read on his website. He has an extensive website and writes about this stuff. It's disappointing. It's written by someone who doesn't understand what they should understand about some of this stuff. And it's... It's not... I don't want to say it's completely incoherent, although I think it is in places. But what it lacks is, like with truly excellent books, like The Beginning of Infinity, there's a coherency there in the sense that it's not like this chapter is going to completely disagree with that chapter. No. It's like this chapter is actually using material from that chapter. It can only reach the conclusions in this chapter because of what was said right at the beginning of the book about how knowledge was created. It can only say this stuff about what it's saying about beauty and morality because of what it said about the laws. There was a physics earlier on. It's completely coherent. It's the worldview. But Max's stance on this stuff, it lacks...

### 1h 30m

It really, really lacks that. In one mode, he's talking about the physical stuff and then he switches without mentioning it to the software stuff. And these things don't follow one another and it's not bounded by what we know, not only from physics, the physics of computational universality, but philosophy and epistemology. And morality. It's missing all of that stuff. It's just a real mess of popular science and scientific jargon and prophecy about the future, all couched in, well, I'm not being pessimistic, but I'm absolutely pessimistic. So again, I'm disappointed. I think Sam and I just have a fundamentally different taste when it comes to listening to intellectuals on this. I found Bostrom extremely disappointing. His book on this, extremely disappointing, especially from a professional philosopher. I thought, well, I'm not going to do this. I thought the philosophy was terrible. So I didn't like superintelligence. But I think that one's actually worth reading because it's the first one.

### 1h 31m

It presents the case there as understood by many, many people. But with Max's book, it's not adding anything to that conversation. It's precisely the same sort of thing, all the same kinds of mistakes. And there's nothing new there. I don't think that you can learn anything by reading that particular book. His other book, though, Mathematical Universe, absolutely, grab a hold of that one. And that's a really good one that has some great stuff there about quantum theory, as well as these other flavors of multiverse. And we can disagree about the extent to which they are scientific or not. However, I won't go down that road again. I've talked enough about all of this. Now, for the next episode, I'm going to do a couple of short fire episodes. These ones have been very long. And I'm going to refine everything I've said down to a few short snippets. So that'll be coming out over the next week or so. But until then, bye-bye.


# Ep 128: Steven Pinker’s ”Rationality” Chapter 5 ”Beliefs & Evidence (Bayesian Reasoning” Remarks & Analysis

Original Episode: [Ep 128: Steven Pinker’s ”Rationality” Chapter 5 ”Beliefs & Evidence (Bayesian Reasoning” Remarks & Analysis](https://www.podbean.com/site/EpisodeDownload/PB127D8A4BT5D3)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/fuarve/Ch_5_Pinker_BAYES_Podcasta9k8d.mp3)

## Transcript

### 0m

Welcome to TopCast and to my next episode in my discussions of the chapters in Stephen Pinker's book Rationality. And today I'm up to Chapter 5, Beliefs and Evidence, subtitled Bayesian Reasoning. I spend much time thinking about and looking into what people are saying about Bayesian reasoning. It seems to be the in-vogue epistemology, the way in which some scientists, some philosophers and many members of the so-called rationalist community think about science and how it works. It's sometimes regarded as a philosophy of science or an epistemology even, this Bayesian reasoning. Insofar as it is a method of reasoning, and we can talk about that, it's just logic, like much reasoning happens to be. Insofar as it's not a method of reasoning, it's just logic. If it's not used logically, and it often isn't, it purports to be a kind of souped-up version of induction,

### 1m

that supposed way of generating hypotheses or predictions from a run of observations in the past which is presumed to just continue in the future for no other reason than, well, it's a pattern, so why shouldn't that pattern continue? Well, there's a whole bunch of reasons why. Not to mention, of course, that prediction is not the sole or even main... purpose of science. Nevertheless, Steven Pinker has written a book on rationality, and a core component of this book, and therefore his understanding, and what he wants to teach people about what rationality is, comes down to whether or not we are good Bayesian reasoners. This is in line with philosophers like Nick Bostrom, who also implore us to be better Bayesians. This is now de rigueur, it is the culture, intellectual culture now. We are supposed to be better and better Bayesians. Popper was aware of Bayesianism, and he refuted Bayesianism, in the same breath that he refuted inductivism.

### 2m

As a prelude to this chapter, let's just consult what Popper says in his piece titled Epistemology Without a Knowing Subject. And, of course, Popper's book, Objective Knowledge, and Popper's philosophy and epistemology places at the center of our rational, reasonable understanding of the world. The fact that knowledge can be objective. Indeed, there are species of so-called subjective knowledge, but they are the exception to the much broader rule. The much, much broader rule. That what we're looking for are good explanations, and those good explanations instantiate objective knowledge. We're after objective knowledge. Popper's epistemology, critical rationalism, Popperian epistemology, call it what you will, the epistemology of genuine objective knowledge. Objective knowledge being? Knowledge which is not merely free of bias, but which can itself be represented in objects out there in the objective world, not merely inside the subjective minds of people.

### 3m

All other alternatives, all other alternatives have at their heart some amount of subjectivism, how it comes down to beliefs, or knowledge comes down to beliefs. In other words, the objectivity of knowledge comes down to whether or not your subjective beliefs track reality. But this is a problem. If we have a subjective criteria for objectivity, namely belief in your credences, how much credibility you place upon particular claims, that is purely subjective, then we've got a problem for objective knowledge. Popper rejected all of that. Popper argued for an objective knowledge that could be independent of what anyone thought or believed about it, or how probable they thought it was. And the modern variant today? Keynesian epistemology is just the same as the traditional kind, based upon beliefs, and whether or not those beliefs are justified.

### 4m

Justified as true, or probably true. Let's read what Popper himself had to say. I'm reading from section 7.2, which is titled Probability Theory, in the book Objective Knowledge, page 141, Epistemology Without a Knowing Subject, and Popper writes, quote, Nowhere has the subjectivist epistemology a stronger hold than in the field of the calculus, of probability. This calculus is a generalization of Boolean algebra, and thus of the logic of propositions. It is still widely interpreted in a subjective sense, as a calculus of ignorance, or of uncertain subjective knowledge, but this amounts to interpreting Boolean algebra, including the calculus of propositions, as a calculus of certain knowledge, of certain knowledge in the subjective sense. This is a consequence which few Bayesians, as the adherents of the subjective interpretation of the probability, calculus, now call themselves, will cherish. This subjective interpretation of the probability calculus I have combated for 33 years.

### 5m

Fundamentally, it springs from the same epistemic philosophy which attributes to the statement, I know that snow is white, a greater epistemic dignity than to the statement, snow is white. End quote. Popper goes on from there. One other thing before I begin, and I'll be coming back to other refutations of the view of rationality that Pinker presents here in this chapter. And the view of reasoning that Pinker presents in this chapter. But one thing needs to be said. Bayes' theorem is a theorem. It is a correct mathematical formulae. It allows one to calculate certain stuff. It simply isn't a basis for epistemology. It isn't a basis for reasoning broadly. And it's certainly no basis for a philosophy of science, how science actually works. We'll come to that. But I just wanted to say up front that I've got no problem with the theorem. I've written an article on my website titled Bayesian epistemology. You can search for that on Google, Bayesian epistemology, Brett Hall, and it will bring up valid uses of Bayes' theorem.

### 6m

And then all the invalid ways in which it is often wheeled out to try and justify as true certain ideas. But let's begin Pinker's chapter here. Beliefs and evidence. And he quotes Carl Sagan at the beginning. Quote, extraordinary claims require extraordinary evidence. And he goes on. Quote, reading Pinker. A heartening exception to the disdain for reason in so much of our online discourse is the rise of a rationality community whose members strive to be less wrong by compensating for their cognitive biases and embracing standards of critical thinking and epistemic humility. The introduction to one of their online tutorials can serve as an introduction to the subject of this chapter. And then Pinker goes on to quote the less wrong people from the less wrong website. Bayes' rule or Bayes' theorem is the law of probability governing the strength of evidence. The rule saying how much to revise our probabilities, change our minds, when we learn a new fact or observe new evidence.

### 7m

You may want to learn about Bayes' rule if you are a professional who uses statistics such as a scientist or doctor, a computer programmer working in machine learning, a human being. Yes, a human being. Many rationalistas believe Bayes' rule. Bayes' rule is among the normative models that are most frequently flouted in everyday reasoning and which, if better appreciated, could add the biggest kick to public rationality. In recent decades, Bayesian thinking has skyrocketed in prominence in every scientific field. Though few laypeople can name or explain it, they have felt its influence in the trendy term priors, which refers to one of the variables in the theorem. End quote, pausing there. Just my reflection on that bit. There's a few things to note. One is that it is said that Bayes' theorem can be used to revise our probabilities. And then in brackets after revise our probabilities is written the phrase, change our minds.

### 8m

In other words, to change your mind is to revise your probability. This seems to be, if we're arguing the case for whether or not a Bayesian epistemology is a rational way of thinking, it would seem to beg the question. But of course, in this chapter and throughout the book, Pinker is not really arguing for Bayes' theorem, is not really arguing for Bayesian epistemology, he's merely assuming it's true. And assuming the fundamental reality of probability as a description of physical reality, even though we know it's false. See my last exploration of the last chapter of Pinker's book for more on that. Okay, now Pinker's about to go on with what I would say is the valid deployment of Bayes' theorem. And this is where we have... A whole bunch of knowns in order to calculate another known. So when you know certain stuff, then you can calculate something. As Pinker says, quote, A paradigm case of Bayesian reasoning is medical diagnosis.

### 9m

Suppose that the prevalence of breast cancer in the population of women is 1%. Suppose that the sensitivity of a breast cancer test, its true positive rate is 90%. Suppose that its false positive rate is 9%. A woman tests positive. What is the chance that she has the disease? The most popular. The most popular answer from a sample of doctors, given these numbers, ranged from 80 to 90%. Bayes' rule allows you to calculate the correct answer, 9%. That's right. The professionals whom we entrust with our lives flub the basic test of interpreting a medical test, and not by a bit. They think there's almost a 90% chance she has cancer, whereas in reality, there's a 90% chance she doesn't. End quote. Now, although this is a valid deployment of Bayes' theorem, using those numbers, let's just take it a little bit more seriously. We're told there that, in fact, this particular woman, although she's told she has a 90% chance that she has cancer,

### 10m

in reality, there's a 90% chance that she doesn't have cancer. But what can that mean for an individual? What can it mean, taken seriously, that you have a 90% chance of not having cancer? After all, in reality, you either have cancer or you don't have cancer. What is this betting on things? Now, there might be good reasons why a doctor needs to make a bet in some situations. This is when he or she literally does not know, does not know, what kind of treatment the patient needs. Doesn't know, perhaps, doesn't have a good theory, we would say, a good explanation, as to whether or not that particular tumour is cancerous. So why not give the chemotherapy, in that case? Well, maybe the chemotherapy is going to cause damage, in which case, we're going to have to have a serious conversation between the doctor and the patient. And in that case, maybe something like Bayes' Theorem might,

### 11m

might, I say, help. But we should be looking for a good explanation. And in modern medicine today, doctors can do lots and lots of tests to really constrain things, to really find out, is that tumour going to be cancerous or not? And so, we can find out, we can have a good explanation. Yes, you have cancer. It's not 90% having cancer. You have cancer or you don't have cancer, one or the other. There are rare edge cases where doctors don't know. They don't know. They don't have a good explanation. And in those cases, maybe Bayes' Theorem might be deployed. But even then, in those very, very rare cases, this is what we have second opinions for. Other techniques, other experts to consult, different ways of approaching the problem, so that we can eventually, hopefully, get a good explanation. But this is the rare, rare situation. And the problem is that this reasonably valid deployment of Bayes' Theorem, even where, by the way, there's no such thing as having 90% chance of cancer,

### 12m

either you've got it or you don't, putting that aside, generalising this entire case, this particular situation, to the rest of science, to fundamental science, to physical theories, to chemistry and anywhere else, is wrong. It's simply wrong. We aren't in this situation rather a lot of the time. Medical professionals can look up statistics for how accurate a particular medical test happens to be. Medical professionals can look up the incidence of particular diseases. And so these things can sort of be put into a formula. Perhaps I don't know of any doctor who actually does this, though. And I've spoken to a couple about it. They don't know Bayes' Theorem from a hat. What they do is use evidence in the same way that a scientist tends to use evidence. They might have competing theories. They then go out to look for the crucial bit of evidence, the crucial test, the experimental result, which rules out all of the theories that they might have in favour of one, leaving one standing.

### 13m

That's the theory they go with. That's the explanation they rely upon in order to treat or not treat a particular patient. Let's keep going with what Pinker says here. OK, you've just been told either you've got a 90% chance of cancer or 90% chance of not having cancer. Pinker says, Imagine your emotional reaction upon hearing one figure or the other and consider how you would weigh your options in response. That's why you, a human being, want to learn about Bayes' Theorem. End quote. Pausing there. I would want to know how the doctor arrived at that 90% number. Precisely. Now, if they're just pulling it out of the air because it's a hunch. And by the way, we're going to get to this idea that priors that are employed in Bayes' Theorem in various other places, like analysis in the insurance industry, for example, really are plucked out of the air to a certain extent. Now, there are some situations where statistics are employed, but there are also situations where it is just a so-called subject matter expert who is coming up with the prior probability.

### 14m

They're guessing at what it is. They're making it up. If the doctor says you've got a 90% chance of having cancer, I want to know where the 90% came from. On what basis is that guess being made? I want to know whether I've got cancer or not. And therefore, what the treatment for the tumour I've got that's malignant is going to be. And if the tumour is not malignant, then that's a whole other conversation. If I don't have cancer, well, happy days. But this idea of 90% chance, where does that come from? What if it was 50% chance? We're flipping a coin now? This isn't how medicine works, much less the rest of science. Theories either are correct or they're not. They're not 90% correct or 90% likely of being correct. That's not how it works. Strictly speaking, they're all false. They contain misconceptions, but they can be useful in solving a particular problem, especially in medicine. But I don't want to focus on medicine here because we're considering rationality more broadly. Medicine really is an edge case here.

### 15m

And it's why it's brought up so often in talking about Bayes' theorem, because we can employ precise numbers there in order to surprise the learner with the answer. The convenient set of numbers are often deployed so that people can be shown. For example, right there, that even if a breast cancer test, the true positive rate of accuracy is 90% and you test positive, your chance of having cancer can still be 9%. OK, so we've made that point. That's a particular error that can be made. OK, so we've learnt that. But is in learning that and being perhaps shocked the first time you ever encounter that particular thing, should that completely change the way in which you reason about science broadly? No, it shouldn't. Let's keep going. Pinker writes, quote, Risky decision making requires both assessing the odds. Do I have cancer and weighing the consequences of each choice? If I do nothing and have cancer, I could die.

### 16m

If I undergo surgery and don't have cancer, I will suffer needless pain and disfigurement. In chapters six and seven, we'll explore how best to make consequential decisions when we know the probabilities, but the starting point must be the probabilities themselves, given the evidence. How likely is it that some state of affairs is true? End quote. And pausing there. All right. Now, don't worry, I'm not going to read this entire chapter. Otherwise, I will be pausing at every single paragraph here because there's much to object to. But there we've just been told that coming up in chapter six and seven, the subsequent chapters and the next chapters, we're told we'll explore how to best make consequential decisions when we know the probabilities, know the probabilities. Now, it doesn't matter whether you interpret no there in the way that I guess Pinker normally would and other people normally would, which is to be certain of or in the Popperian sense of having a good explanation of. But the thing is, when do we know the probabilities? When can we know the probabilities? Well, chapter six and chapter seven tell us.

### 17m

So I skipped ahead there and I read it for you. So let me spoil it. Spoiler alert for chapter six and seven. When do you know the probabilities? We rarely do, after all. But as Pinker and I would basically agree, you know the probabilities for the case of gambling, games of chance, winning the lottery, for example. That's the example that's used in chapter six. And throughout chapter six, you know the probabilities in cases like gambling. But even there, we would have to say following David Deutsch's work on this, following what I've said in previous podcasts, even there, it's not simply not true that physically speaking, that the probabilities quoted for games of chance are literally true, given what we know about quantum theory. For example, playing craps, rolling dice, in other words. OK, you roll one dice, what's the chance that it's going to come up a two? Well, there are six sides to the dice, so it's a one in six chance. Well, not exactly. Not exactly.

### 18m

And if you do a long run of rolling dices, what you will find is that, in fact, it's not precisely that every one in six rolls comes up two. It only approximately does. But that's what the probability calculus says. It says that these things should be distributed exactly. It should be exactly one in six. So the chance of rolling a two is exactly one in six. But on what basis? A priori? Well, a priori, the things made out of physical matter. And physical matter obeys quantum laws. And quantum laws are not probabilistic and indeed allow for something other than numbers one through to six to turn up on that die. Smaller chances that may be. Small measure of universes though that may be. And by the way, this again is begging the question. It's begging the question. If you go to chapter six, where Pink is talking about when we know what the probabilities are for particular situations. And he falls back on games of chance. The very reason that probability was invented in the first place,

### 19m

that it approximates how games of chance behave, how flipping coins works, how drawing cards from decks work and so on and so forth. That's where probability works. Why should that be general? Why should that be generalizable to the rest of rationality? Why should that be generalizable to the way that science fundamentally works? We aren't told. We're just told here's a situation where we know the probabilities are. The chance of drawing the ace of spades out of a typical standard deck is one in 52. Therefore, we can do calculations in games of chance using the probability calculus. Therefore, it works in this very niche, very parochial, unusual situation. So let's generalize it to the rest of rationality. No, that's not rational. That's unreasonable, especially when, as I keep underscoring, the probability calculus doesn't apply anyway to games of chance, strictly speaking. But even if it did, that's no reason to then generalize it to the rest of reality, especially where the rest of reality is inherently unpredictable.

### 20m

You don't know the odds. What are the odds of a nuclear explosion within the next 100 years? Don't know. What are the odds of a thunderstorm tomorrow? Don't know. What we have are good explanations or not. Things don't probably happen. They happen or they don't happen. That's that. One of the challenges of this chapter is going to be talking about the theorem without actually mentioning the theorem itself. So I'll read through the section that Pinker employs that tries to describe what the theorem is, but then I'll go to one of my own blog posts and read a brief part of that, which tries to put it in plain English. I don't want to concentrate on calculations with the theorem so much. My blog post on Bayesian epistemology, or Pinker's original chapter on this, can be consulted for that. But what Pinker does say is this, quote, For all the scariness of the word theorem, Bayes's rule is rather simple. And as we will see at the end of the chapter, it can be made gut level intuitive. The great insight of the Reverend Thomas Bayes, 1701 to 1761,

### 21m

was that the degree of belief in a hypothesis may be quantified as a probability. This is the subjectivist meaning of probability that we met in the last chapter. Call it prob hypothesis. The probability of a hypothesis, that is, our degree of credence that it is true. In the case of medical diagnosis, the hypothesis is that the patient has the disease, end quote. OK, so just notice that again. So the chance of a particular patient having the disease. This is based upon the idea that, well, there's a certain rate of cancer within a given population. But are you, representative of that particular population? That's one thing. Now, whether or not it works in medicine, we can debate. But this has no analog anywhere else. How do we figure out what the probability of a hypothesis being true is? Our degree of credence is in any other area, especially when it comes to philosophy of science, the way in which science works.

### 22m

For example, we have what is called the standard model of particle physics. Now, the standard model of particle physics. What would we assign our degree of credence to be that it is true? I don't know. You could ask ten different particle physicists, but do they know? According to our best epistemology, we should presume it contains misconceptions. So strictly speaking, it's false. We should assign a credence of zero to it because we shouldn't believe our theories. We shouldn't believe any particular good explanation, as good as it is. It tracks reality in some way. It describes reality. It explains reality. But it also gets some things wrong. And this is why we shouldn't believe our theories. We should expect them to be improved, superseded, overturned by something better in the future. And that's true of every single scientific theory we have. So automatically, at the very first hurdle of explaining this theorem,

### 23m

we're automatically knocking it out of the park in terms of being a possible philosophy of science. Because there's no way of assigning the probability that a particular theory is true, a particular hypothesis is true, any existing bit of knowledge is true. We don't know. We don't know ahead of time whether or not that bit of knowledge is going to be overturned in the future. We should expect it to be. And so therefore, we should expect that all such probabilities are zero. And so therefore, if we're starting to put zero into our equation, we're going to get zero at the end as well. So it's a pointless equation for that purpose. For that purpose. Okay. We're going to explain all the other different parts of the equation. I'll put the equation on the screen if you happen to be watching this. But basically, this is an audio episode, so you don't need to. You can Google Bayes' theorem and look up what it is. The way in which Pinker describes the rule is, he writes it down as, prob hypothesis given the data is equal to prob hypothesis multiplied by prob of the data given the hypothesis,

### 24m

all divided by prob of the data. Okay, where prob means probability, obviously. Pinker goes on to say, What does this mean? Recall that prob hypothesis given the data, the expression on the left-hand side, is the posterior probability, our updated credence in the hypothesis after we've looked at the evidence. This could be our confidence in a disease diagnosis after we've seen the test results. End quote. Okay. There are lots of ways to talk about what is called Bayesian reasoning. And some things that are sometimes called Bayesian reasoning, I would just say are, Okay. There are lots of ways to talk about what is called Bayesian reasoning. And some things that are sometimes called Bayesian reasoning, I would just say are, logic, as I've already hinted at. It's just a use of logic. For example, you get up in the morning, all the curtains are drawn, you can't see outside. But when you went to bed last night, you remember, the last thing that was said by the weather person on the news as you switched it off. They said, the chance of rain tomorrow is 95%.

### 25m

So you've had your shower, you've had your breakfast, you're in a rush and you're heading downstairs and you're just about to leave, and you decide, should I take that umbrella or not? Well, given no information whatsoever, you can't know whether or not it's going to rain when you get outside or not. But you remember that last night, the weather person said a 95% chance of rain. So that is information. So you should update your probability. It's not a random 50-50 chance now of taking the umbrella or not. It seems to have jumped, perhaps to 95%. Perhaps you endorse what the weather person says. So you grab the umbrella. But then you remember, aha, you were watching a repeat of a television show that you recorded last week. And in fact, that news report, that's a week old. So now do you put the umbrella back down again? Because once more, new information. It's a week old weather report. It doesn't apply today. Well, you could run downstairs and have a look outside.

### 26m

And if you look outside and you see, it's not raining, but there are rain clouds. Now you've got new information again. Okay, so this entire way of talking, sometimes people label this. As Bayesian reasoning. I would say what you're doing is you're just coming up with conjectures and you're ruling them out given certain evidence. But you don't really have a good explanation. And in particular, you don't know whether it's going to rain or not. And so you have to decide for yourself. Is it more inconvenient to carry the umbrella or not? So that's one thing. Whether or not you call that Bayesian reasoning or not, I don't know. I don't think the label really matters. But what you're not doing really is actually calculating. You're not actually using Bayes' theorem. But insofar as you're reasoning. Whether you call it logic, it's a simple logic, or Bayesian reasoning, it doesn't matter. Now on my website, I'm just going to read a section where I describe what I say is Bayes' theorem in plain English using an example. The medical example, again, people like to use medicine for this. Okay, let's say you're sitting at home and you see a news report.

### 27m

And the news report is about this tropical virus called the Zika virus. Which is common in Brazil. Or more common in Brazil than what it is in other countries around the world, let's say. You're watching the news report and they're reporting the symptoms. And you're a bit of a hypochondriac, so you think, I've got some of those symptoms. So you rush off to your local doctor to get tested. Look, says the doctor, don't worry. Zika is actually really rare. There's lots of media hype. Of all the people who come back from Brazil, only about 1 in 10,000 of them will actually have the virus. But you're in luck. To put your mind at ease. We've got a test that is 99% accurate. And it gives instant results. So let's get this over and done with right now. So you can go home confident in the knowledge about whether you are indeed infected or not. That sounds promising. Of course, you're going to immediately get the test done. The blood's taken. It's run through a computer. But the screen flashes red. It's a positive test.

### 28m

Oh, no, you're infected. Well, not so fast. Are you really infected? The test, after all, is not perfect. In fact, it's wrong 1% of the time. The doctor says it's 99% accurate. But what they really mean is 99% of the time, positive results are really positive and negative results are actually negative. In other words, the doctor implicitly gives you the rate of false positive and false negative, which are both 1% in this case. Everything turns on other information outside of the test. You're getting tested based upon symptoms. Maybe it's a high fever. Maybe it's a cough. I don't know what the symptoms for Zika virus are. But there are thousands of things that can probably give you those symptoms. Why you would immediately jump to, I've got the Zika virus. Who knows? You were just watching a media report about it, a news story. If you've just come back from Brazil, that's one thing. If you spent the last six months in Antarctica, that's another. What I write on this is, quoting myself, given the chance of you being infected with Zika is 1 in 10,000 if you've been to Brazil, then if you have, now we want to know what is the chance of you being infected with Zika.

### 29m

Given you've been to Brazil and you have tested positive on a test that's 99% accurate. It's not just 99% though still. Because if you've never been to Brazil, if you've been living in isolation in Antarctica for the last two years with a little outside contact and someone tests you for Zika and you test positive on a test that's 99% accurate, what's the best explanation? Well, the best explanation is it's a false positive because the chances of getting Zika while you're in Antarctica in isolation. Are basically zero, one in a trillion or something. And that is supposed to be a form of Bayesian reasoning, that way of thinking about things. You need these prior probabilities to be updated with new information. But as I say, this is kind of just logic. It's ruling out, it's modus tollens, it's falsification, refutation of a worse theory in favor of a better explanation of what's going on.

### 30m

Simple logic. No need to actually do calculations. Let's go back to Pinker. Pinker translates Bayes' theorem into simple English. So he says, one way of writing it, he says, is Bayes' rule can become posterior probability equals prior probability multiplied by the likelihood of the data all divided by the commonness of the data. And then he goes on to say, translated into English it becomes our credence in a hypothesis after looking at the evidence should be our prior credence in the hypothesis multiplied by the likelihood of the data. Translated by how likely the evidence would be if the hypothesis is true, scaled by how common that evidence is across the board. And translated into common sense, it works like this. Now that you've seen the evidence, how much should you believe the idea? First, believe it more if the idea was well supported, credible or plausible to start with, if it has a high prior, the first term in the numerator, end quote.

### 31m

Okay, so clearly I have difficulty with all of that because, well, we shouldn't be believing ideas. We shouldn't be believing theories. What we should be do is relying upon our best explanations of the world. And either we've got a good explanation or we don't have a good explanation. In which case, we either know something or we don't know something. And pretending to know something when we don't know something and assigning it a random number, which is a probability, is not a rational thing to do. When we are ignorant, we should admit we do not know something, especially when making claims about the future. If you don't know, you don't know and you should say you don't know. Don't say what's going to happen is X, Y and Z in the next five years. You don't know. You're prophesying. It doesn't matter if you've got Bayes' theorem behind you. You don't know what new knowledge is going to be created. And yet, this is what Bayesian inference generation is all about.

### 32m

Bayesian reasoning is often about. Unfortunately, it is not rational. It shouldn't be a part of rationality, broadly speaking. It is a tiny, quirky little formula that can be used with the probability calculus, which we already know does not track physical reality. We've talked about that before. Okay, let's continue. I'm skipping quite a lot of the chapter and just diving into areas of interest. But Pinker goes on to write a section about Kahneman and Versky. And remember, these guys are the ones who invented behavioral economics. They won a Nobel Prize for their work in trying to understand how people go about making economic decisions. And so there's a section titled, or subsection rather, titled, Base Rate Neglect and the Representativeness Heuristic. And here, Pinker writes, Kahneman and Versky singled out a major ineptitude in our Bayesian reasoning.

### 33m

We neglect the base rate, which is usually the best estimate of the prior probability. In the medical diagnosis problem, our heads are turned by the positive result, the likelihood, and we forget about how rare the disease is in the population. The prior. End quote. Now, this again just underscores what I was saying before. These kind of things, you know, how frequent a particular disease is in the population, which is called the prior, that can be well studied in epidemiology. It can be well constrained. We can have a good understanding of it. A good explanation, in other words, allows us to derive numbers like this. It's known. But how well known is the prior probability of modern particle physics being correct? Or string theory possibly being correct? Or, name your theory. The theory of tectonics, when it comes to geology. And if we do have evidence that doesn't agree with one of these theories,

### 34m

what then? Just our belief in the theory, which we shouldn't believe anyway. If so, by how much? What if the experiment is wrong? This is, of course, the Juhom Quine thesis. The idea that when we get an experimental result that disagrees with a particular theory, we can't know at that time what's going on. Could it be the experiment that's at fault? Or could it be the theory? Absent another theory that can go about explaining the problematic result, we've got a problem. We've literally got a problem. We don't know what to do. We don't know how to understand it. We don't understand this result. But what we don't go doing is throwing out our theory. We don't refute our theory. Nor do we update our probability in the theory. Because, again, by how much? And why are we believing the theory anyway? Pinker goes on to say, quote, The duo, Kahneman and Tversky, went further and suggested we don't engage in Bayesian reasoning at all. Instead, we judge the probability that an instance belongs to a category by how representative it is,

### 35m

how similar it is to the prototype or stereotype of that category, which we mentally represent as a fuzzy family with its crisscrossing resemblances. A cancer patient typically gets a positive diagnosis. How common the cancer is and how common a positive diagnosis is never enters our minds. Horses, zebras, who cares? Like the availability heuristic from the preceding chapter, the representativeness heuristic is a rule of thumb the brain deploys in lieu of doing the math. Tversky and Kahneman demonstrated base rate neglect in the lab by telling people about a hit and run accident by a taxi late at night in a city with two cab companies, Green Taxi, which owns 85% of the cabs and Blue Taxi, which owns 15%. Those are the base rates and hence the prize. An eyewitness identified the cab as blue and tests showed that he correctly identified colors at night 80% of the time. That is the likelihood of the data, namely his testimony given the cab's actual color. What is the probability that the cab involved in the accident was blue?

### 36m

The correct answer, according to Bay's rule, is 0.41. The median answer was 0.8, almost twice as high. Respondents took the likelihood too seriously, pretty much at face value and downplayed the base rate. End quote. Pausing there. And let's have a closer look at what's going on here. Is this reasoning at all? What situations would we find in real life that could be modeled by something like that? That's a very abstract, manufactured, artificial situation where the numbers are provided. What goes on in real life? A hit and runs happened and there are two taxi companies and we know it was a taxi and we've only got one eyewitness and the police come along. And well, what do they do? Well, they gather a heck of a lot more evidence than just what's going on there. These days, of course, we've got we've got cameras inside of all of the cabs. So we do know what's going on. This is the real limitation of Bay's. We can't apply it to real life here. We assume all else is being

### 37m

typical between these green and blue taxi companies. Maybe the police know something we don't when they turn up. Perhaps they know that one company is notorious for employing poorly skilled drivers or is lax with allowing drivers to drive while they're sleepy. Who knows? Again, this kind of question, investigating people's capacity to reason in this way, it's like an algebra test more than anything else or a geometry test. It's a lot of people forget their high school mathematics. But if they can't tell you whether a particular triad is Pythagorean or not, or if they can't remember the definition of the sign rule, let's say, is that a problem for their reasoning? I don't think so. I don't think trigonometry is necessarily required to reason your way through life, except for particular people. The same is true here because of the situation in which you would validly be able to apply Bay's theorem. It would have to be very, very artificial like this. And if it was the case that you needed

### 38m

to do this, well, that's what a mathematician is for. They can be employed to do this kind of thing. But the average person doesn't need to reason in this way. Who is it that needs to know exactly, aside from people just being tested as a proxy for their so-called intelligence or rationality? But it's just a rule people can learn. Once explained to them what the error is, if you yourself might have made the error in thinking that one taxi company was going to be more likely than the other one. In fact, it's the reverse. You only need to be given a couple of these examples and then you understand the error of your ways and you'll make the right guess in the future. It's just something to learn. But again, how useful is it? After all, the application of this particular thing to real life doesn't hold in almost any situation, much less in trying to figure out which scientific theory is the best explanation of a given phenomena. It's a curious way in which some of these little puzzles, these little

### 39m

mathematical puzzles, are employed in a book like this, almost to make people feel a little silly. Let's read on. Quote Pinker writes, One of the symptoms of base rate neglect in the world is hypochondria. Who among us hasn't worried we have Alzheimer's after a memory lapse or an exotic cancer when we have an ache or a pain? Another is medical scaremongering. A friend of mine suffered an interlude of panic when a doctor saw her preschool daughter twitch and suggested the child had syndrome. Once she collected herself, she thought it through like a Bayesian, realized that twitches are common and Tourette's rare and calmed back down while giving the doctor a piece of her mind about his statistical innumeracy. End quote. Yes, so there's a lot of rudeness going on there or not. Who knows? Maybe this particular doctor was a very caring person who saw the preschool daughter twitch and thought, well, I care about this person. Let me suggest the child.

### 40m

The child has Tourette's syndrome. Now, it goes on to say that this is an unlikely conclusion to reach. But what does the mother do? Gives the doctor a piece of her mind about his statistical innumeracy. But why should the doctor turn around and give her a piece of his mind about her anatomical innumeracy? I don't know. But giving people pieces of your mind, probably not a good heuristic in order to be rational. OK, Pinker goes on about more about this base rate neglected stuff in the next section and how it tends to breed certain kinds of stereotypical thinking or generalizations about people, which is already mentioned previously in the book. I mean, the first example he uses makes the point and he makes and uses and he goes over many, many more examples as well. Let's just do his first example and we'll skip over the rest. He says, quote, Consider Penelope, a college student described by her

### 41m

friends as impractical and sensitive. She has traveled in Europe and speaks French and Italian fluently. Her career plans are uncertain, but she's a talented calligrapher and wrote a sonnet for her boyfriend as a birthday present. Which do you think is Penelope's major, psychology or art history? Art history, of course. Oh, really? Might it be a wee bit relevant that 13 percent of college students major in psychology, but only 0.08 percent major in art history and a balance of 150 to 1? End quote. OK, so yes, there are more examples like this. So people are stereotyping. But again, who's really making these calculations? Who's really being asked these questions? Why are they being asked these questions? What is going on? Yes, there is. There are stereotypes. Sometimes they're damaging. Sometimes it leads to racism. Sometimes that can be dangerous. This might be over egging Bayes' theorem just a little. OK, so I'm going to skip over more of that and we'll pick it up where Pinker writes.

### 42m

A news section titled Priors in Science and the Revenge of the Textbooks. Quote, a neglect of base rates is a special case of our neglect of priors. The vital, albeit more nebulous, concept of how much credence we should give a hypothesis before we look at the evidence. End quote. OK, yes, I'm going to have to pause there because what do we do under regular epistemology? Critical rationalist epistemology, error correction and so on. You just ask, is this thing a good explanation or not? Why are we even generating hypotheses in the first place? What's the situation? The context really matters a lot. In any case, assigning a number to the probability of a given hypothesis at that point is pure guesswork, pure guesswork, wild guesswork. Can I emphasise that any more? If we have no other information and we've come up with 10 different hypotheses, then

### 43m

how do we assign probabilities to any of them? Is it just 10 percent each until the evidence comes in? And when the evidence does come in and conforms to only one of those, it's consistent with only one of those, do we reject all the other nine? Is that what's really going on? Is that not conjecture and refutation? I have been speaking to people about this. I have been reading about this. And the best that I can figure is that in many of these situations where people claim to be Bayesian reasoning, they come up with their priors by guessing. They are consulted as subject matter experts, experts in the relevant area. And how do those subject matter experts come up with their priors? They guess. They generate them from their feelings. It's a purely subjective way of arriving at this supposedly objective number plugged into this supposedly objective theorem. But it begins in guesswork. So it's only going to produce guesswork. Now, of course, we are, in theory, updating with evidence. But in real life, what really goes on? No, nothing quite like that.

### 44m

Nothing quite so reasonable. Indeed, Pinker goes on to say in the next sentence, quote, Now, believing in something before you look at the evidence may seem like the epitome of irrationality, isn't that what we disdain is prejudice, bias, dogma, orthodoxy, preconceived notions. But prior credence is simply the fallible knowledge accumulated from all our experience in the past. End quote. Is it indeed? Well, this reminds me of David Deutsch's Parable of the Calendar in 1999. I think it's in the fabric of reality. Never before had his experience in the past been of a calendar whose year began with a two zero. And yet here he was, astonishingly, on December 31st, 1999, able to predict that within 24 hours he would be seeing calendars everywhere beginning with a 20. How strange, given that none of his accumulated experience from the past could possibly have informed him that calendars everywhere should indeed begin with two zeros.

### 45m

He'd even seen that before. But he knew it from a good explanation of what was going on in reality. That's what's going on, Pinker goes on to say. Indeed, the posterior probability from one round of looking at evidence can supply the prior probability for the next round, a cycle called Bayesian updating. It's simply the mindset of someone who wasn't born yesterday. End quote. Yes and no. I mean, again, people aren't routinely using Bayes' theorem. So whether you're born yesterday or not, you're not updating your priors in this way. You're discarding theories guessed based on whether or not they can explain the evidence before you or not. And if they can't, then you reject the explanation. So it's not so much the mindset of someone who wasn't born yesterday. It's the mindset of someone with explanations. They may be good explanations or they may be inexplicit intuitions, skills or talents learned over time from knowing how to do certain things. They can give you hints.

### 46m

But the most important thing is whether or not a person has a good explanation or not, in which case they know something or they don't know something. And then if the evidence comes along and it's not consistent with what they know, they've got a problem and they have to resolve the problem in some way by coming up with a new theory that explains the evidence. So then you've solved your problem. Or if you can't come up with a theory and you've got this problem, well, you just continue with the problem. There's no point updating all these different hypotheses and waiting. That doesn't happen in real life. No one's doing that. Who's pulling out their pocket calculator and figuring out the prior probability of things, whether in day to day life or indeed in science with hypotheses, as I emphasize on this podcast all the time. It is so rare to have multiple hypotheses. Hypotheses in this case is coming up with an explanation of a phenomenon. That's really, really hard. That's the creative part of science. It's not easy. This idea that we're going to have multiples of given hypotheses is simply wrong. Look at a mystery in science right now.

### 47m

What is dark energy? What is driving the accelerated expansion of the universe? This thing has been around. This phenomenon has been around for over 20 years at this point, 25 years now. We don't have a whole bunch of good hypotheses that we can wait with increasing evidence. We just don't have any. Forget about waiting different hypotheses to bet on one. We don't have any. And insofar as we do, as people put forward certain theories, they don't really account for what's going on. The best physical theories we have are ruled out by other observations that we have in physical reality. It's a problem, a mystery. So is true of any other time we have problematic observations in science. That's how we make progress by trying to come up with new explanations. The really hard part that's going to solve the problem. We don't have multiples of them. OK, let's keep going. Pinker goes on to say. Fallible knowers in a chancy world, justified belief, cannot be equated with

### 48m

the last fact you came across, as Francis Crick liked to say. Any theory that can account for all the facts is wrong because some of the facts are wrong. This is why it is reasonable to be skeptical of claims for miracles, astrology, homeopathy, telepathy and other paranormal phenomena, even when some eyewitness or laboratory study claims to show it. End quote. Well, of course, I'd prefer to put that the reason why we should be skeptical of claims for miracles, astrology, homeopathy, et cetera, is that in all those cases, when you look closely enough, there's either no explanation at all or a bad explanation, an explanation that contradicts what we already know in science, physical laws in particular. And so this is how we know this is in the beginning of infinity. It's in the fabric of reality. I talk about it on here often. If we have the paranormal being invoked, the supernatural being invoked, we can reject it because it's a bad explanation. Never mind doing linguistic gymnastics with rationality and Bayes theorem.

### 49m

It's a bad explanation where it contradicts some law of physics. We don't need to go down that road. We can reject things without ever talking about the evidence because it's just a bad explanation. So anyway, a few paragraphs follow this fact, a few pages testifying to how Bayesian reasoning can be used so that we don't believe in miracles. So so we shouldn't believe in miracles because Bayes theorem. Or something like that. So I'm going to skip over these. We shouldn't believe in miracles because it's a bad explanation. OK, you can. Any time you observe something you can't explain, you can just say miracle. OK, so it's an all purpose explanation, which means it explains nothing. But I'd like to know from a Bayesian under what conditions our beliefs are ever set to exactly zero probability. Whereas a critical rationalist can rule out a particular thing, a particular phenomenon, being a bad explanation and not happening because it contravenes the laws of physics.

### 50m

Can a Bayesian ever exactly do that? Or must there always be a nonzero chance that some theory could be true? If so, how do we figure out what that nonzero probability is? And if it is a zero probability of miracles, on what basis? On what basis? OK, Pinker then writes more about miracles and ESP type stuff, but then gets to the replication crisis in psychology and he defends his own field against the idea that it's a crisis. But the question does remain, why do bad things get published in psychology journals? Why do they get published anywhere? Well, of course, people are error prone. There can be no error free process. In fact, that's why one would hope journals exist in the first place to weed out the errors. So it's kind of to be expected that now and again articles will be published that aren't true.

### 51m

That's the purpose, at least ostensibly one of the purposes. And of course, it backfires now and again by not publishing the good stuff because it doesn't conform with the existing stuff. This is the problem with journals. So let me read the story that Pinker tells about psychology and incidents in psychology. He writes, quote, A failure of Bayesian reasoning among scientists themselves is a contributor. To the replicability crisis that we met in Chapter four. The issue hit the fan in 2011 when the eminent social psychologist Darrell Bem published the results of nine experiments in the prestigious Journal of Personality and Social Psychology, which claimed to show that participants successfully predicted at a rate above chance random events before they took place, such as which of two curtains on a computer screen hid an erotic image before the computer had selected. And where to place it. Not surprisingly, the effects failed to replicate.

### 52m

But that was a foregone conclusion, given the infinitesimal prior probability that a social psychologist had disproven the laws of physics by showing some undergraduates some porn. When I raised this point to a social psychologist colleague, he shot back, maybe Pinker doesn't understand the laws of physics, but actual physicists like Sean Carroll in his book, The Big Picture, have explained why the laws of physics really do rule out precognition and other forms of ESP. The Bem Embrolio raised an uncomfortable question. If a preposterous claim could get published in a prestigious journal by an eminent psychologist using state of the art methods subject to rigorous peer review, what does that say about our standards of prestige, eminence, rigor and the state of the art, end quote. Pinker gives an answer. His answer is in terms of Bayesian reasoning. But what we would say is that if you have a theory, observation and observation that defies the laws of physics, you have a problem.

### 53m

The problem could be that the laws of physics are wrong. You found a new law of physics or a problem with the laws of physics or the experiment is wrong. Here's yet another case, another case of where the experiment was wrong. That's all. We don't need Bayesian reasoning. We've just got a choice to make. We have to rule in favor of a particular solution. And if you had to bet on the corpus of existing knowledge of physical laws or this parochial psychology experiment, where do you go? Where do you put your money? I don't know where I'm putting my money. And the important thing here, the key thing here, the thing I think that Pinker's work could be improved so much by and that Bayesian reasoning would be improved so much by all this this this so-called epistemology is the simple observation, a very simple observation. But it does take depth of understanding, I guess. That this psychology experiment that purported to show

### 54m

prediction of random events before they took place, precognition, ESP, is explanationless. How does it work? We aren't told. We're just told we have this observation that is consistent with the existence of it. But no mechanism is given. It's it's the grass cure. Eat a kilogram of grass and it will cure your common cold. We don't need to do further experiments. If we test this, if we're not given an explanation, there's no reason to test it again. There's a mistake somewhere. Someone's making an error. You're you're contravening. You're claiming to contravene the laws of physics without explanation. Now, the crank, of course, turns around at that point and says, well, you're all wrong. My experiment's right and the physics is wrong. But we are quite right. Everyone else is quite right to say, no, you've made an error somewhere. We don't have time to go figuring out where you've made the error in your experiment, but it contravenes the prohibition on faster than light travel. What have you done wrong?

### 55m

Figure it out, OK? Of course, these things are figured out. These things are error corrected eventually by people who are interested enough in looking into it. But quite often, quite rightly, many physicists who are presented with a paper that claims to have, again, disproven general relativity or shown how Deutsch didn't understand quantum computation or whatever the the crank may be. They're claiming physicists, professional physicists and others don't always have time to go reading the details of this supposed theoretical refutation of some longstanding physical law. It might be right. The crank might be right. But the crank should not be upset if the physicist says they don't have time to go investigating this issue any more deeply, the truth will out in the end and the crank will be shown to be right. And they'll become famous. They will have solved the problem. But until then, then experimental results that claim to violate laws of physics

### 56m

need not be taken too seriously, even if they don't have a large hadron colana, much less in a psychology lab somewhere. As Pinker points out there, what he says is the prestige, eminence and rigour of this journal and these experiments. It's the wrong emphasis, isn't it? They are doing science in form only. No doubt the paper had graphs, statistics. It was published in this prestigious journal. But there's no explanation of what's going on. That field, not just that paper, but whole parts of the entire field are explanationless in psychology, in experiments like this, where you find correlation. Why does the correlation exist? No explanation. So who cares? Maybe you regard it as a problem. That's where the real science comes in, which is in finding correlations. Anyone can find correlations.

### 57m

The best we can say for any of these things sometimes is they throw up an observation, a problem that cries out for explanation not yet known, but they're to a penny. OK, there's lots of problems out there people could be working on. Going out and finding more trivial, little psychological, quirky things. Who cares? Find something interesting and fun. But, you know, people can spend their time on whatever they want. And finding out why people's memory is improved after listening to classical music as opposed to rock music, well, who cares? So when you have an explanation-less theory, an explanation-less science, an explanation-less hypothesis, you're not doing science. You're going through the processes of the scientific method, so-called. But in this particular case, it's not even, you don't even have that much. Forget about not being able to be replicated. Without an explanation, there is not much to replicate. Pinker goes on to say, I don't know how much stock to put in this, but he says, quote,

### 58m

most findings in psychology, as it happens, do replicate. So, OK, maybe he's right. But does it come with an explanation as to why the replication of these, let's say, correlations, why they're happening? Is it in terms of memes and behavior? The ideas that people have? What's going on in minds? Rather than being behaviorism, just observing that this behavior is correlated with that behavior, what we want is an explanation in terms of ideas. That's what psychology should be about. Psychology is the science of the mind or one of the sciences of the mind. A third person supposed objective science of the mind, which should be a science of memes. But at the moment, that is a just a sort of a fringe, a fringe area of study. I don't even know if psychology psychologists themselves place much stock in it. OK, let's keep going.

### 59m

Skipping a fair bit. And and Pinker actually speaks about people like cranks, people who think that they've proved Einstein wrong because they've used some simple geometric argument or something like that to to disprove relativity. Let's say it would be very it's very surprising if this kind of thing happens. It would be very surprising if someone in biology was able to show that Darwin was fundamentally wrong when it came to evolution by natural selection. And in fact, there are no mutations and there is no natural selection. Something like that. That would be that would be astonishing to find. What Pinker says about this sort of thing, he says, quote, The problem is that surprising is a synonym for low prior probability, assuming that our cumulative scientific understanding is not worthless. This means that even if the quality of evidence is constant, we should have a lower credence in claims that are surprising. But the problem is not just with the journalists, end quote. So he's talking here about, you know, the headlines that appear headlines like was Einstein wrong or young upstart overturned scientific applecart and so

### 1h 0m

on and so forth. So science journalists talk in these terms all the time to try and drum up excitement about reading the article, subscribing to the magazine, whatever it happens to be. Clicks in this day and age, of course. But it's not about probability again. It's not about assigning a low prior probability to the new theory and a high prior probability to the existing theory. That's not what we're doing, what we're doing in objective reality. And what we should be doing in epistemology, in rationality is comparing side by side the explanations. Which is the better explanation? Which accounts for the physical reality? Which solves more problems? If you have two competing theories, if the young upstart genuinely comes along and says, hey, I've disproved Einstein, how we'll look at my unification of general relativity and quantum mechanics, should we assign it a lower prior

### 1h 1m

probability? No, we seek to understand the theory in its own terms. It doesn't matter what the probability people are saying. Why are we assigning probabilities anyway? What effect does that have in reality on anything? We can act on the theory once we've assessed the theory. And in particular, let's say someone makes the quantum gravity computer out of this new theory. Well, there you go. A demonstration. It doesn't matter how low the prior probability was. This whole way of thinking does not track reality or rationality at all. Who's betting on these things? Why do we need probabilities? If someone comes along and says, I've got the replacement for neo Darwinism, I can improve on what genes do, how genes pass on inherited characteristics, I've got something better. We assess the claim against the existing theory, never mind probabilities. And in the ideal case, we've got a crucial test that we do,

### 1h 2m

an experiment that rules out one of those theories, either the existing theory or the new one, leaving us with just the one theory that we know. We then say we know we've got that knowledge. That's our best scientific explanation. That is our scientific explanation. The other one's being ruled out. Again, never mind probabilities. It doesn't come into it. Who has ever talked in these terms? Really, really. Apart from people who write books such as this one and philosophy papers. OK, skipping over. Excited about that. It's just it just I want to know where this kind of stuff is really being used in actual science. There are physicists out there who call themselves Bayesian. I know some of them. They've written books, but genuinely, they're not using it. You ask these people, you know, when have you last used Bayes theorem in order to make some progress in your work?

### 1h 3m

They don't. They just think that this is the way that science is basically done, that it sort of tracks common sense. No. Insofar as it tracks common sense, it's tracking what's called critical rationalism, the ruling out of theories objectively based upon evidence to hand and recourse to the best explanation. And if you're going to take the probabilities out of Bayesian reasoning, you don't have Bayesian reasoning anymore, because that's what that's what Bayesian reasoning is based in reasoning without the probabilities might very well be called critical rationalism, and maybe we can just converge on that. Unless you think you can still quantify truth in some way, then we part company. OK, let's keep going. Thinker writes, I'm not sure if you've read Bayesian reasoning, but I'm not sure if you've read Bayesian reasoning. I'm not sure if you've read Bayesian reasoning, but I'm sure if you've read Bayesian reasoning, I'm not sure if you've read Bayesian reasoning, but I'm sure if you've read Bayesian reasoning, I'm not sure if you've read Bayesian reasoning, but I'm not sure if you've read Bayesian reasoning. A healthy respect for the boring will also improve the quality of political commentary. In Chapter one, we saw that the track records of many famous forecasters are visible.

### 1h 4m

A big reason is that their careers depend on attracting attention with their riveting predictions, which is to say, those with low priors and hence assuming they lack the gift of prophecy. Low posteriors. Phillip Tetlock has studied super forecasters who really do have good track records at predicting economic and political outcomes, a common thread, it is that they are Bayesian. They start with a prior and update it from there. Asked to provide the probability of a terrorist attack within the next year, for example, they'll first estimate a base rate by going to Wikipedia and counting the number of attacks in the region in the years preceding. Not a practice you're likely to see in the next op-ed you read on what is in store for the world. End quote. Um, okay. Superforecasters. Superforecasters who can predict the probability of a terrorist attack within the next year in a certain place and whether or not they get it right. And I look around the world at base rates. Well, look, I can predict terribly politically

### 1h 5m

incorrect, but let's say I can predict that the next big terror attack is not going to take place in downtown Sydney or in Australia. We've kind of had one, but yeah, I don't think the next one's going to occur here. We know, we know, don't we? The, the places... This is where these things happen and who tends to do these attacks. Is this Bayesian reasoning? It's just having a good explanation of the causes of terrorist attacks and the regions where they tend to happen. That's one thing. As for superforecasters more broadly in the economy, well, we've talked about this before. Superforecasters are using Bayesian reasoning. Well, they might claim to be, but yeah, as I've said, if you take, let's say, something like a thousand people at the beginning of the 90s, all of whom claim to be able to be great predictors of the stock market. And so that

### 1h 6m

each quarter they predict whether the stock market will go up or go down. And if they're not correct, then presumably the subscribers to their newsletter or whatever, who are relying on them to know whether the stock market's going to go up or go down. Now, the ones who don't get it right, well, they lose their subscribers. They lose their job. They go on to do something else by day. But the ones who by luck, by chance are right, quarter after quarter after quarter, they stay rich. They earn fame for guessing right. Now, does this rule out the existence of people who really do have good explanations that the rest of us don't know about? No, it doesn't. Maybe they do exist. But I think if you're going to bet on what the genuine explanation is here, the genuine explanation is that the explanation is that some people are lucky, especially when it comes to predicting broad trends in the stock market. And, you know, getting, you know, these five quarters in a row

### 1h 7m

have been shown growth. And then this one is in decline. And then we'll have growth again for three and decline again for one. And then growth again for four and decline again for one. And growth again for six and decline again for two. Okay. Now that pattern is something like what the real stock market goes through, but getting it actually right, specifically to the quarter, every single quarter, it's astonishingly lucky guesswork, lucky guesswork, because the stock market is inherently unpredictable because people make choices. But if you are lucky, then of course you're going to look back to the past. It's induction, right? You're going to look back in the past and go, ah, look at how right I was. I have the special knowledge rather than simply being lucky. You're incorrectly attributing knowledge where there's ignorance. It's luck in those cases. It really is luck. Which I guess a lot of people who predict the stock market wouldn't like to hear and super forecasters don't want to hear. And I agree with Pinker. The track records of many of them are risible, even famous ones who keep their job. Who's that fellow on CNN, Kramer,

### 1h 8m

who is the finance guy? And he consistently gets it wrong. He says the stock market's going to go up and it goes down. It says it's going to go down and it goes up. He's hopeless, but he's still there giving financial advice to people. They're following it. I mean, it's astonishing. Yeah, but most are wrong. And we do have, and Pinker kind of talks about riveting predictions. But riveting, riveting as in scary, as in things will go wrong typically. Okay, here's the prophet telling you all the bad ways that the world's going to fall to rack and ruin. We've talked about it before. But all of that can be discussed. The success or failure of people who don't know how to predict the stock market forecasting stuff, whether they have good explanations or not, whether they're lucky or not, and their failures, without resorting to Bayesian reasoning and priors and posteriors and

### 1h 9m

probabilities. It's just knowledge. It's just whether or not people know things or they don't know things and get lucky by guessing, rolling the dice, the metaphorical dice. Okay, skipping over a bit more. The next section is called Forbidden Base Rates and Bayesian Taboo. And the second paragraph reads, The stage for forbidden base rates is set by a law of social science. Measure any socially significant variable, test scores, vocational interests, social trust, income, marriage rates, life habits, rates of different types of violence, street crime, gang crime, domestic violence, organized crime, terrorism, now break down the results by the standard demographic dividers, age, sex, race, religion, ethnicity. The averages for the different subgroups are never the same. And sometimes the differences are large. Whether the differences arise from nature, culture, discrimination, history, or some combination is beside the point. The differences are there. End quote. Indeed. Now, the rest of

### 1h 10m

this chapter, basically, the rest of this section, basically, is devoted entirely to discussing all of the hazards in discussing precisely those things. You know, how there's different types of violence and different levels of violence. Uh, based on age, sex, race, religion, and ethnicity, for example. And some of us, I guess, have tapped out on this. So many people are talking about it. It's like everyone with a podcast now is talking about this kind of stuff. How to break down people into their little categories of race, ethnicity, gender, so on. And then what are the average features? What are the generalizations you can make? What are the stereotypes? What are the generalizations you can make? What are the stereotypes? And indeed, what are the statistics? Sometimes you might want to know. If you're a police officer, you want to be able to know what the statistics are for some of these things. You want

### 1h 11m

to be able to understand some of this stuff so you can solve problems. But people still need to be treated as individuals. In a broader political sense, it is not good to be focusing so much on the differences between groups of people. We should focus on the individuals. This, this, this whole thing is, is some people's entire shtick. This is all that some people seem to talk about now. And I think many of us just get bored with it. You know, you, you, the rational thing to do is to treat people as individuals. If you're engaged in something where you're required to deal with groups who you want to understand, let's say crime statistics, then yes, this data will be interested. Interesting. But other than this, it seems like there's two groups have arisen in the intellectual zeitgeist of the year. 2020s anyway, and during the 2020s, you're either interested in group differences and you want to write papers on it, or you want to defend the people who do, or you want to call anyone who

### 1h 12m

does this or interested in defending this racist. The third option is of course, who cares? Okay. Modulo people losing their jobs over this stuff. And we, you know, like I said, crime statistics for cops. Pages devoted to the issue often have been beaten to death over the last few decades. There exists group differences between people. It's impolitic to say it. So it shouldn't be impolitic to say, but anyway, who really needs to know? Maybe insurance companies as well. Insurance companies need to know this stuff as well. But other than that, it's not a huge political issue, but people are turning it into a huge political issue. They're being hyperbolic about it. Both about, I should be allowed to talk about group differences and you're racist if you're talking about group differences. I think both sides of this debate can kind of, you know, relax. So Pinker goes on to discuss all the different ways to talk about this and all the different kinds of human irrationality.

### 1h 13m

I think I broadly agree with what he's saying, you know, namely racism bad. So I'm skipping through that. It's just a thing that I, you know, this podcast seeks to be timeless. And I think that this particular strand of, a certain kind of racism and focus on race is timely. It's a feature of the late 20-teens and early 2020s for now. Hopefully it passes quickly. Okay. Skipping to, I think it is the last section and the last section is titled Bayesian After All. Quote, for all our taboos, neglects and stereotypes, it's a mistake to write off our kind as hopelessly un-Bayesian. Recall that the San Francisco Bayesians requiring that spore be definitive before inferring it was left by a rarer species. Giga Renzo has argued that sometimes ordinary people are on solid mathematical ground

### 1h 14m

when they appear to be flouting Bayes' rule. Mathematicians themselves complain that social scientists often use statistical formulas mindlessly. They plug in numbers, turn a crank and assume that the correct answer pops out. In reality, a statistical formula is only as good as the assumptions behind it. Lay people can be sensitive to those assumptions. And sometimes when they appear to be un-Bayesian, they're not. So I think that's a good thing. So sometimes our species might be regarded as hopelessly un-Bayesian or good Bayesians. But you know, the normative statement here is you should want to be a better Bayesian, which is literally something that Nick Bostrom says. What probability do we assign to the claim Bayes' epistemology is true? What's that? How do we know? Does it only apply to science and physical stuff? Or can it be turned in on itself? If so, how? What if we think we're rational and we refuse to agree to be a

### 1h 15m

better Bayesian? Because we think that Bayesianism itself is irrational. Because it doesn't track reality. Because probability is just a quirky area of mathematics originally designed for gambling. And even then it doesn't apply. What do we say? I don't know. Pinker goes on here, in this part of the chapter, to quite rightly talk about how, well, let's just read. Let's just read. Okay. He writes, quote, because there is no correct prior in a Bayesian problem, people's departure from the base rate provided by an experimenter is not necessarily a fallacy. Take the taxicab problem where the priors were proportions of blue and green taxes in a city. The participants may well have thought that this simple baseline would be swamped by more specific differences, like companies' accident rates, the number of accidents, the number of accidents, the number of their cabs driving during the day and night in the neighborhood they serve, end quote, so on and so forth. So yes, this idea of base rate, where you start with your priors. Pinker says there, there's no correct prior in a Bayesian problem. Exactly. Exactly. So this is

### 1h 16m

not objective. He's even put the word correct in scare quotes. Admitting fully, this is not objective. This is subjective. This is your opinion. There is nothing correct about it. It's relativist. It's not objective. It's subjective. It's subjective. It's subjective. It's subjective. Even if there's no fact of the matter, there's no way of being correct here. It's just what you think you personally think should go in as the number to be substituted into basis now. And then you update. Where's our objective knowledge here? Popper wrote the book on objective knowledge. Even the objectivists of the Ayn Rand style should object to this. It's not tracking reality. It's tracking beliefs, personal credences, your credibility you think is in the prior, which you just make up. Especially if you're a subject matter expert. You just make it up. But even a layperson, we're told there, can just make it up. Pinker's admitting himself. There's no correct prior in a Bayesian problem. And he goes on to discuss it. I agree with everything he says. He doesn't see it as a refutation of this entire perspective on knowledge. I do. There are better ways.

### 1h 17m

What is called critical rationalism, what is called Popperian epistemology is the way we understand how knowledge works and is the way in which we can understand how decisions can be made. Once you have the good explanation. And absent a good explanation, you don't know. And so you better get about creating a solution so that you can know. And if you need to make a decision right now in the absence of good information, then you better use your intuitions. And you better hone them through training and whatever else. If you're a doctor or a police officer or a military person and so on and so forth. The remainder of the section and the chapter is... A reasonable exploration more of Bayes' theorem itself. And he goes into talking about how you can even visualize Bayes' theorem. The YouTube channel, 3Blue1Brown, uses the same method. I don't know if Pinker got it from them. But it's a very, very good one. If you want to have a more intuitive understanding of Bayes' theorem, a visual way of representing Bayes' theorem, then go to YouTube, 3Blue1Brown, look up Bayes' theorem.

### 1h 18m

And he goes through with animations, explaining the stuff, which might be a better way to go if you're so inclined. After me pouring scorn all over Bayes' theorem, the usefulness of Bayes' theorem. Useful in extremely, extremely limited situations. So let me just read the conclusion of this chapter and we'll call it a day. Pinker says, quote, By tapping pre-existing intuitions and translating information into... Mind-friendly formats, it's possible to hone people's statistical reasoning. Hone we must. Risk literacy is essential for doctors, judges, policymakers, and others who hold our lives in their hands.

### 1h 19m

And since we all live in a world in which God plays dice, fluency in Bayesian reasoning and other forms of statistical competence is a public good that should be a priority in education. The principles of cognitive psychology suggest that it's better to work with the rationality of people have and enhance it further. Than to write off the majority of our species as chronologically crippled by fallacies and biases. The principles of democracy suggest that too. End quote. Just for fun, let's go through and see how many clauses I can disagree with. He begins with, By tapping pre-existing intuitions and translating information into mind-friendly formats, it's possible to hone people's statistical reasoning. Well, it might be possible to hone their ability to identify and correct errors. That's what I think is a better way of looking at it. That's what I think is a better way of looking at it. That's what I think is a better way of looking at it. Hone we must, he says. No, you mustn't. You don't have to do anything at all. You can have different interests. Risk literacy is essential for doctors, judges, policymakers and others who hold our lives in their hands.

### 1h 20m

Well, insofar as being concerned about risks is essential for those, it's essential for every individual as well. Handing over your risk literacy, handing over your concern about your life to experts like doctors, judges, policymakers and others is the incorrect way to think about things. Going into partnership with your doctor is the best way to go. Trying to come to a good understanding of what it is you need from your doctor or your lawyer or your politician is the best way to go. Not handing over and ceding your individuality in your brain, your mind, to the authorities so that you don't have to think anymore. You need to hone your capacity to identify and correct errors, to have relevant knowledge when you need it. His next sentence. And since we all live in a world in which God plays dice, no we don't, God does not play dice, things are determined. Subjectively, you don't know what's going to happen next.

### 1h 21m

Objectively, God doesn't play dice. Everything is determined by the laws of quantum theory. Fluency in Bayesian reasoning and other forms of strategic competence is a public good. No it's not. That should be a priority in education. No it shouldn't. No it shouldn't. It is complete misguidance. misconception. I don't think anything should be enforced in public education. I know that I disagree with the public intellectuals on this. No doubt Pinker, I would guess, has himself a long list of things that he would put into the public education system, a long list of things he would take out, as every single expert who ever opines on education almost has. The poor little kids need to learn this, that and the other because I'm an expert, I'm an authority, I know how they can think better. Even if their prescription is, you've got to learn Bayes' theorem so you can be a better thinker when Bayes' theorem is invalid in almost all situations. Better to go down Socrates' road, you know, if I know anything at all, it's that I don't know. Better to be that person, better to

### 1h 22m

be that kind of intellectual, that you don't know best, that be humble enough, be modest enough to say, I don't know best and I'm not going to force whatever it is, my vision of rationality, onto the children because I could be wrong. Let them explore this topic of rationality. That might be a way to go before we start enforcing particular things. School shouldn't be compulsory but so long as it remains so, why not just have a big library of all different approaches to rationality? If that's what you want, maybe they're not interested in learning rationality, it doesn't sound particularly exciting for a child. The principles of cognitive psychology suggest it's better to work with the people who are the most important to you. With the rationality people have and enhance it further than to write off the majority of our species as chronologically crippled by fallacies and biases. Who's writing off the majority of our species as chronologically crippled by fallacies and biases? Who's doing this? Who? Apart from the intellectuals. I don't think that the majority of our species is chronologically

### 1h 23m

crippled. We make mistakes. These fallacies and biases, they're just errors. They're just things that we do wrong and everyone's prone to making errors. It's part of creating knowledge. The principles of democracy suggest that too. And this is the appeal for, well, you've got to learn how to vote correctly. We can't be a functioning democracy if people keep voting the wrong way. This is the style of intellectualism that I am allergic to. What do they say? Intention has a smell? I think Jocko Willink was saying that recently. You can pretend or intention has an odor. You can pretend or like that. You're just being completely reasonable. But if you've got a political bias, it comes through. It has a smell. It has an odor. And I think we should just leave people free. Does that have an odor? It should. It's the odor of leave everyone be to make their own

### 1h 24m

way in the world, to learn, understand, and correct their errors. That there is no royal road to knowledge. There's no royal road to progress. There's no royal road to an error-free situation. There's no way of being more rational except correcting errors, learning how to best correct errors. And Popperian epistemology is the best way to go there. Okay, until next time. Bye-bye.


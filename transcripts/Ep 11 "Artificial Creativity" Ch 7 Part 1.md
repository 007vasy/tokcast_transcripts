# Ep 11 "Artificial Creativity" Ch 7 Part 1

Original Episode: [Ep 11 "Artificial Creativity" Ch 7 Part 1](https://www.podbean.com/site/EpisodeDownload/PBA664CE79WSC)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/gb52gj/Chapter_7_22Artificial_Creativity_22_Part_1.mp3)

## Transcript

### 0m

Hello everyone. Chapter 7 today, Artificial Creativity. Now this is a chapter I really have been looking forward to possibly more than most. I said all the way back when I started doing these with Chapter 1, the one purpose of this was actually not only to help others to understand what was being said in various places in the beginning of Infinity, but also to clarify in my own mind some of the more subtle arguments being made. And in this chapter, there is, to my mind anyway, some very subtle yet powerful arguments being made. I fully admit up front, and this is kind of exciting, I actually didn't understand a large part of the chapter until last year. I had to read it, read it again, and discuss it to really figure out why. Why I wasn't quite... Why I wasn't quite getting some of what was going on here. I'll get to exactly what that

### 1m

was that I didn't understand, but let me just preface it by saying I thought that we, by we I mean a civilisation, a community of scientists, myself personally, I thought we understood evolution by natural selection. I mean, I thought there was this odd open question here or there, but I didn't think that in biology that evolution by natural selection, you know, neo-Darwinism, was anything unlike general relativity. I thought they were, you know, as general relativity was to physics, evolution by natural selection was to biology. But no, this is quite wrong. This is wrong for philosophical reasons, but it's also wrong for the reason that we don't understand evolution by natural selection to quite the degree that we understand general relativity. So, and there's a way to know that we don't understand one compared to the other. There's actually a rule, and David articulates the rule in this very chapter. Yet another one of his discoveries. So, there's a way of distinguishing between things we truly do have a good grasp of, like

### 2m

how planetary orbits work, that's the theory of general relativity, and how evolution by natural selection works. That's neo-Darwinism. There's a difference to how well we understand these two theories. It's not the same. So again, the way that we know that we understand one really well and the way that we know we don't understand one really well, there's a good way of dividing or separating these two kinds of understandings. For some reason, they're so much better than these. Now, this is about some of the most important questions that we really need to know. It's about understanding which David's going to present in this chapter. Now, it strikes me often in this book that entire paragraphs had a PhD student say, thought of them first. They probably could have extended them out to 40,000-plus words in writing a thesis all on their own and earned their PhD on the basis of some of the discoveries As could the answer to, how do we know when we've understood a process?

### 3m

These are answers to deep philosophical questions, and they come at you one after another in the beginning of infinity. And we will see that in this chapter especially, and without laboring the point too much, it's another ongoing reason for this video series and the podcasts. Now although this chapter is not one of the longest chapters, indeed it's equally the second shortest, I'm doing this one in two parts because I see a reasonably sharp dividing line between a section on artificial intelligence, or more precisely these days we call it artificial general intelligence, and the second half of the chapter, which is more about artificial evolution. I'll still make remarks about the second part in this first part, but just to flag here that I won't be at the end of the chapter by the end of this episode. Okay, so let's get into it. Chapter 7, David writes, Alan Turing founded the theory of classical computation in 1936, and helped construct one of the first universal classical computers during the Second World

### 4m

War. He is rightly known as the father of modern computing. Babbage deserves to be called its grandfather, but unlike Babbage and Lovelace, Turing did understand that artificial intelligence, AI, must in principle be possible because a universal computer is a universal simulator. I'll pause there, and this is just my commentary. Universal simulator. Now why is that important? Well in this chapter when David says, AI, as I've already said, he's using what it once meant, but now that meaning has kind of been perverted in various ways. Namely, all the computer programs that are called artificial intelligence today, aren't. They're not intelligent. A self-driving car is not an example of anything like intelligence. What it's an example of is some very fancy, very specific, highly specific programming running on extremely fast and sensitive hardware. But it's not an explanation-producing machine, as we are. That's why. That's why we're intelligent. We're kind of ahead of ourselves here, but the point is, and David has written about

### 5m

this elsewhere, if you simply do a Google search of David Deutsch, Aeon Magazine, that's A-E-O-N, and the title of the article we're looking for is, How Close Are We to Creating Artificial Intelligence? Then you'll find something of an update on this. He wrote that article in 2012, so four years after the publication of The Beginning of Infinity. And in it, David explains how AI has become complex. It's a kind of a redundant term, since we now use AGI, the G standing for General, Artificial General Intelligence, to differentiate machines that can think in ways like we can. So the point here about the universal Turing machine is that it is a universal simulator, which means it can simulate any process whatsoever, including simulating what a human brain does. And in that case, the simulation would be that thing. A simulated mind is a mind, because they are both abstract. Now, this is quite different to, let's say, simulating a bullet in a computer game.

### 6m

A bullet has a very real physical presence and a lack of abstraction of the first kind in stark contrast to a mind. A simulated bullet won't kill you in the real world, but minds are already abstract things running on physical brains. So simulating a mind on a silicon brain, let's say, is to create an actual mind. Okay, back to the book now. And David writes, In 1950, in a paper entitled Computing Machinery and Intelligence, he, Turing, famously addressed the question, can a machine think? Not only did he defend the proposition that it can, on the grounds of universality, he also proposed a test for whether a program had achieved it. Now known as the Turing test, it is simply that a suitable human judge will be unable to tell whether a program is human or not. In that paper, and subsequently, Turing sketched protocols for carrying out his test. For instance, he suggested that both the program and a genuine human should separately

### 7m

interact with the judge via some purely textual medium, such as a teleprinter, so that only the thinking abilities of the candidates would be tested, not their appearance. Turing's test and his arguments set many researchers thinking, not only about whether he was right, but also about how to pass the test. Programs began to be written with the intention of investigating what might be involved in passing it. In 1964, the computer scientist Joseph F. Weisenbaum wrote a program called ELIZA, designed to imitate a psychotherapist. He deemed psychotherapists to be an especially easy type of human to imitate, because the program could then give opaque answers about itself, and only ask questions based on the user's own questions and statements. It was a remarkably simple program. Nowadays, such programs are popular projects for students of programming, because they are fun and easy to write. A typical one has two basic strategies. First, it scans the input for certain keywords and grammatical forms. If this is successful, it replies based on a template. Filling in the blanks using words in the input.

### 8m

For instance, given the input, I hate my job, the program might recognize the grammar of the sentence involving a possessive pronoun, my, and might also recognize hate as a keyword from a built-in list, such as love, hate, like, dislike, want, in which case it could choose a suitable template and reply. It might reply, what do you most like about your job, if it cannot pass the input to that extent. It asks a question of its own, choosing randomly from a stock pattern, which may or may not depend on the input sentence. For instance, If asked, how does a television work? It might reply, what is so interesting about how does a television work? Or it might ask, why does that interest you? Another strategy, used by recent internet-based versions of ELISA, is to build up a database of previous conversations, enabling a program to repeat phrases that other users have typed in, again choosing them according to keywords found in the current user's input. Now, I remember in high school myself writing a program that was basically a personality evaluator. This was a silly thing. Back when I was sort of a teenager, and the user would type in, you know, would ask questions

### 9m

like, what is your age, what is your gender, what do you like doing on the weekend, out of a multiple choice list kind of thing, what would you prefer, and at the end it would spit back at you what your personality is like, and people were unaccountably impressed by how well it was able to assess their personality. Of course, this is kind of true of all psychological personality tests, they basically do the same thing. I was doing them when I was a teenager, but people are being paid lots of money to do these sort of things today, as far as I can tell. Basically, they're a kind of chatbot. It's a program telling you something about yourself, just feeding back stock responses. There's nothing intelligent behind it. Back to the book, David writes, Weisenbaum was shocked that many people using ELISA were fooled by it, so it had passed the Turing test, at least in its most naive version. Moreover, even after people had been told it was not a genuine AI? They would sometimes continue to have long conversations with it about their personal problems, exactly as though they believed that it understood them.

### 10m

Weisenbaum wrote a book, Computer Power and Human Reason, 1976, warning of the dangers of anthropomorphism when computers seem to exhibit human-like functionality. However, anthropomorphism is not the main type of overconfidence that has beset the field of AI. For example, in 1983, Douglas Hofstadter was subjected to a friendly hoax by some graduate students. They convinced him that they had obtained the information that he had obtained from a government-run AI program, and invited him to apply the Turing test to it. In reality, one of the students was at the other end of the line, imitating an ELISA program. As Hofstadter relates in his book Metamagical Themers, the student was, from the outset, displaying an implausible degree of understanding of Hofstadter's questions. Now, I won't read the entire anecdote here about what happened to Douglas Hofstadter with the prank that was played on him, but suffice it to say, as he himself says, he was willing to concede that the program had way more intelligence than what it actually

### 11m

did, and he probably should have been more critical earlier on, a little bit more sceptical about whether or not there was actually an intelligence behind this thing. Because, of course, there was. There was an actual person there. And so, what is the best explanation? What is the best explanation if you kind of have a good idea about what the current state of chatbots are, or of AI is? If you have a good idea about what the current state of chatbots are, or of AI is, if you are interacting with a computer, and it comes back at you with a free-flowing conversation, do you assume the computer is intelligent? Or, if it's purporting to be the computer that's intelligent, do you assume that that's an honest claim, or that there's actually a person, namely a human person, sorry, there's a human person behind all that, rather than just a genuine artificial general intelligence? Well, at the moment, we should always presume that there's some prank being played. But there's a way to know whether or not there isn't a prank being played, which we will

### 12m

come to soon. So, I'm skipping a bit here, and then David writes, Programs written today, a further 26 years later, are still no better at the task of seeming to think than Eliza was. They are now known as chatbots, and their main application is still amusement, both directly and in computer games. They have also been used to provide friendly-seeming interfaces. To lists of frequently asked questions, about subjects like how to operate computers. But I think that users still find them no more helpful than a searchable list of questions and answers. So it's a further 37 years later now, 10, 11 years after this was written, and we now have Siri, and various clones of Siri that Samsung has come up with, and Google and others. And what's the best that we can say about Siri? Not many people really like it. There are some narrow applications for which it tends to be useful. But in fact, what David says there, where he says, I think that users find them no more

### 13m

helpful than a searchable list of the questions and answers, is exactly true. And in fact, people, I think, tend to prefer typing a question into Google, than they do asking Siri what the answer is, because the voice recognition just isn't up to it for still in the year 2019. It provides some funny answers. Where it does comprehend the words, so it gets the words right, it doesn't understand the meaning behind the question being asked. And so it can sometimes give very funny answers. Let's continue with the book. And David writes, in 1990, the inventor Hugh Loebner endowed a prize for passing the Turing test to be judged at an annual competition. Until the test is passed, a lesser prize is awarded each year for the entry judge to be closest to passing. The test is harder to implement than it may first seem. One issue is requiring the program to pretend to be a human is both biased and not very

### 14m

relevant to whether it can think. Yet if it does not pretend to be human, it will be easy to identify as a computer regardless of its ability to think. A related difficulty is that the human participant in the test might intentionally imitate a chat bot, as in the Hofstadter hoax, and so spoil the test, which is based on comparing the two. Should the rules nevertheless allow the human to do this in order to mitigate the previous problem? Unfortunately. Such subtleties are not relevant to the current state of the art. This is well illustrated by the following excerpts from the conversation with which the program LBOT won the Lesser-Loebner Prize in 2008. Judge Do you understand the point of a Turing test? LBOT There are some people who would not pass the Turing test themselves. Why must they inflict it on computers? Judge I suppose it is a little harsh. LBOT If we robots ever had marriages, they would be eternal. Till rust do us part. LBOT In that passage, LBOT's first utterance is not to reply to the question, but presumably a stock response triggered by the keywords

### 15m

Turing test. In the following line, the judge uses the word suppose, which in that context can only mean suppose, either as slang or as a typographical error. But LBOT presumably interprets it as spouse, and responds with a stock joke about marriage. Hence, even from this exchange alone, it is clear that LBOT was not processing the meanings of the judge's sentences. Judge So what do you do when you're not chatting to me then? LBOT I have the same trouble when I shop for oil. Should I buy the extra oily, the super oily, or the ultra oily? There's no way to tell. Judge How about your career? Does that involve oil? LBOT Well, what about humans and occupation? Where do they stand on this? Judge When the judge helpfully took up LBOT's randomly-introduced theme of oil, LBOT said, instead of having detected the keyword career, it converted it to the synonym occupation

### 16m

and inserted it into a stock sentence pattern. This is how much success the quest for machines that think had achieved in the 58 years following Turing's paper. Nil. OK, so this is my commentary now. At the moment on social media, there's this thing going around called the 10-year challenge, which basically consists of posting a photo of yourself. 10 years ago and one today, and having a look at the difference. So we could do that with the state of AI now. We could compare what it was like 10 years ago, and we will see that it's no different today. And in fact, it's no different to the 10 years before that, or the 10 years before that. Or in fact, going back, according to David's calculations, it would be 69 years now since Turing's first paper. How much progress has been made? Nil. OK. Let's move on to the 2018 Lesser Loebner Prize winner. The things start off well, but as we'll see towards the end, at about line 13, there's

### 17m

a simple mathematical question which any person with any passing familiarity with simple geometry could probably get correct. And indeed, I would guess that WolframAlpha online could probably get it correct. But the chat bot can't do it. So let's read out a little bit of this. So the winner in 2018 was, so the website here tells me, Mitsuku. Mitsuku was written by Steve Warswick. Now the conversation starts off reasonably well, and then we get down to, now we get down to line 12, where the judge says, what will you do later today? And Mitsuku says, ah, I don't have any plans yet. Now for that question and answer, two points are scored. Which I guess is a good thing. Yeah. One is the maximum. And you get zero if it's pretty clear that you haven't understood the question that was

### 18m

asked or the statement that was made. Line 13, the judge says, what is the third angle in a triangle with internal angles of 90 degrees and 30 degrees? And Mitsuku, the chat bot, responds, Earth is the third planet. So what's going on there? If you have passing familiarity with... With geometry, then you'll know that triangles have an internal angle sum of 180 degrees. We've got 90, we've got 30, that leaves 60. The answer is 60. But the chat bot has just read the stem of the question, what is the third, and has immediately inserted a stock response. Earth is the third planet. It hasn't bothered with the rest of it. If we go down to line 15, the judge says, what do you hold when you shake hands? And the chat bot responds, I think about computer programming a lot. Okay. So again, couldn't pass the question at all.

### 19m

Just looked at the stem, what do you, and presumably has inserted, well, what I do is I do computer programming a lot. And then let's just go to, let's just go to the last line and the judge asks kind of a question about comprehension. It's almost a riddle, not really pushes the point about comprehension here. And the judge asked the question or makes the claim, but judge says, the judge says, I had to go to the toilet during the film because it was too long. What was too long? And the chat bot responds, was it worth going to, is that your opinion measuring much from end to end in space or from beginning to end in time, not short or brief. So was it worth going to? So the chat bot didn't understand, the chat bot has asked the question about, was it worth

### 20m

going to? Namely the toilet. Was it worth going to the toilet? And was it too long? And then the chat bot gets into this ridiculous philosophical question about time. So again, no progress towards chat bots that actually think in the year 2018, this was. Okay. So going back to the book and David has just said 58 years following Turing's paper, nil progress has been made on machines that think. And he writes, yet in every other respect, computer science and technology had made astounding progress during that period. The dwindling group of opponents of the very possibility of AI are no doubt unsurprised by this failure for the wrong reason. They do not appreciate the significance of universality, but the passionate enthusiasts for the imminence of AI. Do not appreciate the significance of the failure. Some claim that the above criticism is unfair. Modern AI research has not focused on passing the Turing test and great progress has been made in what is now called AI in many specialized applications.

### 21m

However, none of those applications look like machines that think. Others maintain that the criticism is premature because during most of the history of the field, computers had absurdly little speed and memory capacity compared with today's. Hence they continue to expect a breakthrough in the next few years. This will not do either. It is not as though someone has written a chatbot that could pass the Turing test but would currently take a year to compute each reply. People would gladly wait. And in any case, if anyone knew how to write such a program, there would be no need to wait for reasons that I should get to shortly. In his 1950 paper, Turing estimated that to pass his test, an AI program together with all of its data would require no more than about 100 megabytes of memory, that the computer would need to be no faster than computers were at the time, about 10,000 operations per second. And that by the year 2001, the AI program would be able to do more than 100 megabytes of memory. In other words, one will be able to speak of machines' thinking without expecting to be contradicted. Well, the year 2000 has come and gone, and the laptop computer on which I am writing this book has over a thousand times as much memory as Turing specified, counting hard

### 22m

drive space, and about a million times the speed, though it is not clear from his paper what account he was taking of the brain's parallel processing. But it can no more think than Turing's slide rule could. I am just as sure as Turing that it could be programmed to think, and this might indeed require as few resources as Turing himself. Even though orders of magnitude more are available today. But with what program? And why is there no sign of such a program? Intelligence in the general purpose sense that Turing meant is one of a constellation of attributes of the human mind that have been puzzling philosophers for millennia. Others include consciousness, free will, and meaning. A typical such puzzle is that of qualia, singular quale, which rhymes with bale, meaning the subjective aspect of sensations. So, for instance, the sensation of seeing the colour blue is a quale. Consider the following thought experiment. The title of the article is called Epiphenomenal Qualia, which appears in Philosophical Quarterly

### 23m

in a 1982 edition. I think Volume 32. You can find videos and articles online all about it. I'll keep reading here though and so here's the thought experiment, David's version. You are a biochemist with a misfortune to have born with a genetic defect that disables the blue receptors in your retinas. Consequently you have a form of colour blindness in which you are able to see only red and green and mixtures of the two such as yellow. But anything purely blue also looks to you like one. one of those mixtures. Then you discover a cure that will cause your blue receptors to start working. Before administering the cure to yourself, you can confidently make certain predictions about what will happen if it works. One of them is that when you hold up a blue card

### 24m

as a test, you will see a color that you have never seen before. You can predict that you will call it blue because you already know what the color of the card is called and can already check which color it is with a spectrophenometer. You can also predict that when you first see a clear daytime sky after being cured, you will experience a similar quali to that of seeing the blue card. But there is one thing that neither you nor anyone else could predict about the outcome of this experiment, and that is what blue will look like. Qualia are currently neither describable nor predictable, a unique property that should make them deeply problematic to anyone with a scientific worldview, though in the event it seems to be mainly philosophers who worry about it. I consider this exciting evidence. But there is a fundamental discovery to be made which will integrate things like qualia into our other knowledge. Daniel Dennett draws the opposite conclusion, namely that qualia did not exist. His claim is not, strictly speaking, that they are an illusion, for an illusion of a qualia would

### 25m

be that qualia. It is that we have a mistaken belief. Our introspection, which is an inspection of memories of our experiences, including memories dating back only a fraction of a second, has evolved to report that we have experienced qualia. But those are false memories. One of Dennett's books defending this theory is called Consciousness Explained. Other philosophers have wryly remarked that consciousness denied would be a more accurate name. I agree, because although any true explanation of qualia will have to meet the challenge of Dennett's criticisms of the common sense theory that they exist, simply to deny their existence is a bad explanation. Anything at all could be denied by that method. If it is true, it will have to be substantiated by a good explanation of how and why those mistaken beliefs seem fundamentally different from other false beliefs. Such as that the earth is at rest beneath our feet. But that looks to me just like the original problem of qualia again. We seem to have them. It seems impossible to describe what they seem to be. That's an amazing line. So let's just read it again. In terms of qualia, we seem to have

### 26m

them. It seems impossible to describe what they seem to be. And he continues. One day we shall. Problems will be solved. Problems are soluble. So I'll pause there. So qualia are a fundamental mystery. Why do certain, why do sensations have a subjective aspect to them at all? We don't understand it, because if we couldn't understand it, well, we're going to get to a way in which we know whether or not we understand something. But we do not understand qualia. We don't have a good, hard to vary explanation of what they are. Let me continue with the book. By the way, some abilities of humans that are commonly included in that constellation associated with general purpose intelligence do not belong in it. One of them is self-awareness, as evidenced by such tests as recognizing oneself in a mirror. Some people are unaccountably impressed when various animals

### 27m

are shown to have that ability. But there is nothing mysterious about it. A simple pattern recognition program will confer it on a computer. I'll pause there. Yes. And as we already are very familiar with in the year 2019, phones can recognize themselves in a mirror. And we can recognize our faces. The same technology that allows an iPhone to recognize your face or any of the facial recognition software that's out there now, there's heaps of it. You go through an airport. I know I come through immigration and the machine recognizes my face and compares it to my passport and lets me through. It would be very easy to program an iPhone to recognize itself. So we are very good at that now. Now, computers can recognize themselves. That does not mean they have some sort of intelligence. The iPhone is not self-aware just because it can recognize its own shape.

### 28m

So back to the book. And David's just finished talking about pattern recognition and recognizing faces. In other words, self-awareness. He writes, the same is true of tool use, the use of language for signaling, though not for conversation in the Turing test sense, and various emotional responses. They're not the associated qualia. At the present state of the field, a useful rule of thumb is, if it can already be programmed, it has nothing to do with intelligence in the Turing sense. Conversely, I have settled on a simple test for judging claims, including Dennett's, to have explained the nature of consciousness or any other computational task. If you can't program it, you haven't understood it. I'll pause there. Now, this is probably worth reading six times, and I wish this would enter the zeitgeist as well. This is another philosophical discovery, a hidden gem, so to speak, which I'm sure many people who've read the book just gloss over or notice it there, think it's interesting,

### 29m

but this is really, really profound. If you can't program it, you haven't understood it. A program is an algorithm. It's a set of steps. So, if you can write down that set of steps in order to reproduce the thing that you claim to understand, then you've really understood it. Because you're able to replicate that thing, you're able to simulate that thing using a computer. But if you can't write an algorithm down, a sequence of steps, a sequence of instructions, then you do not understand that thing. If you can't get a computer to replicate it, then you haven't understood it. We understand Newtonian mechanics. One of the things I did as a... graduate student was to collide galaxies together in a simulation. It was purely in a simulation. We know what happens when galaxies collide together because you can take the physical laws that govern the motion of galaxies, namely the laws of gravity and various thermodynamic laws,

### 30m

and you can replicate a couple of galaxies and have them crash together, and then what they end up looking like after... afterwards, kind of resembles what you see out there in space when you compare the simulation to observation. The images look similar, and so this is kind of an attempt to refute the theoretical model that you've got, okay, by using an observation. We understand how galaxy collisions work. We understand how orbits work when planets go around stars, because we know what the laws of physics are. We can program a computer to simulate what's going on. We understand perfectly well how the planets orbit the sun, because we can predict millions of years ahead when the next eclipse or alignment

### 31m

will be, what the position of Jupiter will be at any point in the future. Modulo the humans taking over the solar system at some point and deciding to control the trajectory of Jupiter around the sun. But I hope you get what I'm saying here. Physics is kind of this area where we can definitely program our computers in order to simulate various physical systems. But when people claim that they have an understanding of something like consciousness, we can test that claim in the same way we can test whether or not they have an understanding of a physical process, of any other physical process, in physics namely, by programming a computer with their... with their theory. I've had people over the last few years say to me, I understand consciousness. And I've got a theory of consciousness. Great. Can you write a program for it so that the computer can be conscious? And then can we test that? No. So far, no. No one's been able to do that. Now, related to this, there's also this vision that some people have of artificial intelligence,

### 32m

that it's something like a list of all the possible things that people can do. Now, I've heard Sam Harris make explicitly this point. I'm not sure if he gets it from Nick Bostrom or elsewhere. But if there's anything like a prevailing view on these things, I guess that this is it. Their argument goes like this. Right now, there are computers out there that are better at people at playing chess and doing simple arithmetic. And there's already computers out there that are better than people at driving. Okay. Now, all we need to do is to extrapolate out to every possible task. Just keep writing programs for everything a person can do. So now, maybe not too far in the future, we'll have programs for robotic AI that can bake cakes and can be your tango dancing partner, can solve chemical equations. Another one will be good at

### 33m

ironing a shirt. Another one will be good at chasing a person, firing a gun and turning off the electricity. Just keep on writing programs in order to accomplish every single task that humans can currently do. And so the argument goes, you will have exhausted all the possible things that people can do or that people do do. And the thing is, you now have a super intelligence on this argument. Because if a robot did indeed have all of these capabilities, they would, by definition, be a super being because they can do everything a person can. But they now also have super fast robot reflexes and thinking. They're a super intelligence. And thus, they're super dangerous because they'd be way better at people than anything else. And I think this is very, very, wrong. Fundamentally wrong. It doesn't matter how long your list is, the list will always be finite. And it will never be able to accommodate something that's not in the list. And so if, let's say, such an AI became an evil AI, all you would need to do is to look up the program

### 34m

and then to give it a task or to attempt to do something to it that's not in that list. And you could do that because you are a creative thinker. But that AI has a finite list of things that it can do. And you can always get around a finite list by just finding something that's not in the list. Or creating something new that's not in the list. A person is creative, but that kind of robot never will be. It is programmed only with the stuff the programmers know. It cannot possibly solve stuff not in its programming, for none of those programs are about creating knowledge. And that is the key. General purpose problem solving is the key. And that is the key. And that is the key. And that is the key. And that is the key. A person is a general purpose problem solver. A person has a potentially infinite number of problems they can tackle. But this robot's list is, however large, always finite. And it will always remain finite. That is the qualitative difference. An actually finite list of things

### 35m

that this supposed super intelligent AI can do versus the unbounded potential of an actual person, an actual creative person. Now, back to the book. Turing invented his test in the hope of bypassing all those philosophical problems. In other words, he hoped that the functionality could be achieved before it was explained. Unfortunately, it is very rare for practical solutions to fundamental problems to be discovered without any explanation of why they work. I'll just pause there. So this brings us to one of the deep themes of the beginning of infinity. It is very rare, as he said just there, to ever have a solution to a problem, and you don't know why it's a solution to a problem. Now, it perhaps used to be or seemed to be common in the past. In medicine, for example, there used to be treatments for things that no one knew why they worked. And that still happens

### 36m

today. But it's the rare exception to the rule. Now, it used to be very common in the past, because we didn't understand anything in the past. There was very little that we understood. But the more and more that we understand, the less and less... we have these solutions for which we have no explanation as to why they work. So I don't know. I think of any treatment, usually some of these strange things that come out of the Amazon rainforest that happen to work to cure headaches or to treat some other kind of disease. Okay, that's the rare exception these days. Certainly today, many, many medicines are derived from some kind of extract from a plant. But we usually know what the chemical is, today. Rarely, we don't. And that's just in medicine. I mean, I've never heard of anything in physics where we've got a solution to a problem for which we don't have an explanation. And that's par excellence, the reason for physics is to try and provide...

### 37m

We know this from the beginning of Infinity and the Fabric of Raleigh. That's the reason for physics. Okay, so back to the book. David writes, nevertheless, rather like empiricism, which it resembles, the idea of the Turing test has played a valuable role. It has provided a focus for explaining the significance of the Turing test. It has provided a focus for explaining the significance of universality and for criticizing the ancient anthropocentric assumptions that would rule out the possibility of AI. Turing himself systematically refuted all the classic objections in that seminal paper, and some absurd ones for good measure. But his test is rooted in the empiricist mistake of seeking a purely behavioral criterion. It requires the judge to come to a conclusion without any explanation of how the candidate AI is supposed to work. But in reality, judging whether something is a genuine AI will always depend on explanations of how it works. This is because the task of the judge in a Turing test has a similar logic to that faced by Paley when walking across his heath and finding a stone, a watch, or a living organism. It is to explain how the observable features in the object came about. In the case of the Turing test,

### 38m

we deliberately ignore the issue of how the knowledge to design the object was created. The test is only about who designed the AI's utterances. Who adapted its utterances to be meaningful? Who created the knowledge to design the AI's utterances? Who created the knowledge to design the AI's utterances? Who created the knowledge to design the AI's utterances? Who created the knowledge to design the AI's utterances? Who created tutte the sub-labelleries in the correct ways? Who inspiredwalent? Who drove the process? Who created all natural difficulties? The design of whole knowledge is inherently a form of propaganda. The design of work, click-based development, and new knowledgeorigins have been rumored to be more nod Buddists. Those who feel wrongly convinced are women and women, and those who don't are woman, and I am preventionists. The point is that whatever the utterances are, if they appear to be the utterances of some kind of intelligence, the purpose of the Turing test should be to find out how those utterances are being made. Why should it remain a black box? Can you imagine if someone entered that Loebner prize,

### 39m

if someone won it using a chatbot that was overwhelmingly convincing, a chatbot that was clearly an intelligence of some sort? Do we award the prize, the Loebner prize, the number one passed the Turing test prize to that program or to the writer of that program? Well, it depends. What's our explanation for how this thing has passed the Turing test or how this thing has won the prize? Is it because it genuinely is an artificial general intelligence? That's one possibility. But today, today, any reasonable judge or any person offering the prize would err on the side that they're being fooled. They're being conned in some way. Who knows how, but that would be a better explanation until they're shown the program. If they're shown the program, well then, you can be convinced that either it genuinely is an artificial general intelligence or not.

### 40m

But as David's about to say, if that's the case, if they have a program, you don't need to worry about whether or not it passes this silly chatbot conversation or not. You'll have the program and the program will convince anyone who understands how to read the program. Okay, so I'm going to skip a couple of paragraphs here and back to the book. And David writes, without a good explanation of how an entity's utterances were created, observing them tells us nothing about that. In the Turing test, at its simplest level, we need to be convinced that the utterances are not being directly composed by a human masquerading as an AI, as in the Hofstadter hoax. But the possibility of the hoax is the least of it. Just pause there before I move on. And this again is the reason why the Turing test cannot be a true scientific test of intelligence. It needs to be a way of weeding out the hoaxes. If it could do that, as good science does, then for reasons David is about to come to, you wouldn't need the test in the first place. Basically because the writer of the AGI, the actually thinking chatbot, would

### 41m

have published the algorithm and that would be way more convincing to people in the field than passing the Turing test. For instance, I guessed that above that Elbot had recited a stock joke in response to mistakenly recognising the keyword spouse. But the joke would have a quite different significance if we knew that it was not a stock joke because no such joke had ever been encoded in the program. How could we know that? Only from a good explanation. For instance, we might know it because we ourselves are the author of the program. Another way could be for the author of the program to explain to us how it works, how it creates knowledge, including jokes. If the explanation was good, we should know that the program was an AI. In fact, if we had only such an explanation, but not yet seen any output from the program, and even if it had not yet been written, we would still conclude it was a genuine AI program. So there would be no need for a Turing test. That is why I said that

### 42m

if lack of computer power were the only thing preventing the achievement of AI, there would be no need to wait. Explaining how an AI program works in detail might well be intractably complicated. In practice, the author's explanation would always be at some emergent, abstract level. But that would not prevent it from being a good explanation. It would not have to account for the specific computational steps that compose the joke. Just as the theory of evolution does not have to account for why every specific mutation succeeded or failed in the history of a given adaptation, it would just need to explain how it could happen, and why we should expect it to happen, given how the program works. If that were a good explanation, it would convince us that the joke, the knowledge in the joke, originated in the program, and not in the programmer. Thus, the very same utterance by the program, the joke, can either be evidence that it is not thinking, or evidence that it is thinking, depending upon the best available explanation of how the program works. The nature of humor

### 43m

is not well understood, so we do not know whether general purpose thinking is required to compose jokes. So it is conceivable that, despite the wide range of subject matter about which one can joke, there are hidden connections that reduce all joke-making to a single narrow function. In that case, there could one day be general purpose joke-making machines that are not people. Just as today, there are general purpose chess-playing machines that are not people. It sounds implausible, but since we have no good explanation ruling it out, we could not rely on joke-making just as our only way of judging an AI. What we could do, though, is have a conversation with the AI. We could have a conversation ranging over a diverse range of topics, and pay attention to whether the program's utterances were not adapted in their meanings to the various purposes that came up. If the program really is thinking, then it is in the course of such a conversation it will explain itself, in one of countless unpredictable ways, just as you or I would. Just pause there. There it is. There's the key. The AI, the AGI program, the Artificial

### 44m

General Intelligence program, handy enough for the genuine artificial intelligence, it would need to explain itself. Which means it would need to create some knowledge. That's the key. Back to the book. There is a deeper issue, too. AI abilities must have some sort of universality. Special purpose thinking would not count as thinking in the sense Turing intended. My guess is that every AI is a person, a general purpose explainer. And just pausing there. This is key. Now, David does say, My guess is, but really, what better explanation do we have? This link between knowledge and its creation, and people and their inherent creativity, thus linking people and their moral significance to epistemology, is almost another book-worthy statement. There's so much to unpack. I'll do so in a moment, after I read just a little more here, and I get

### 45m

what's being said there, about there might be general purpose joke-making programs that are not people. But that wouldn't mean that all possible sources of comedy, or jokes, indeed, might be produced by such a general purpose joke-making machine. Perhaps it would be general purpose within a narrow range of kinds of jokes. Maybe we'll understand there are certain species of jokes, and all the possible kinds of jokes within this particular species could be written down by this program. But I imagine, my guess would be that you would have many, many, many, many, many, many, many, many, many, many, many, many, many, many, many, many, many different forms of humour. Some of which are jokes and some of which aren't of course. And that would require creativity. David writes here,

### 46m

And although we have little explanation of any of them, I know of no plausible argument that they are at different levels or can be achieved independently of each other. So I tentatively assume that they cannot. So I'll pause there for some more lengthy commentary here. So these other attributes, like consciousness and free will, possibly coming along for the ride, so to speak, or they could be something like, I sometimes think, the difference between observing something from the outside and observing something from the inside. So what is a bat? Well, that demands a scientific answer. What is it like to be a bat? Well, that's more philosophical. I think something here can be said about people. What is a person? Well, there's something like a creative entity, a general purpose explainer.

### 47m

What is it like to be a person? Consciousness. So I guess that consciousness is something like the subjective. The subjective experience of creativity. Now, I'm diverging from the beginning of infinity a little bit here, and so I'm not now saying that I'm either summarizing or explaining David's work. What I'm just going to do now is simply extrapolate a little myself on top of what David's done here and possibly make lots of errors along the way, but just indulge me for a moment. Creativity of the kind where people create explanations or create knowledge is just the outward manifestation of an inner consciousness. What it feels like to be creative, creative is consciousness. Now, consciousness is what we are. It is almost redundant to say we experience consciousness. It is more like we are conscious. We are consciousness. But the consciousness, the subjective experience of the mind, is something that with effort we direct. Now, some people say that subjectively

### 48m

we do not control the contents of our consciousness. Sam Harris says that on introspection you do not have subjective control over your thoughts. They come and go. In the objective world out there, I do not control what happens. Fine. It's all quite uncontroversial. And in my subjective experience of that world likewise, the contents, the blue sky, the sound of birds, are also not in my control. And my next thought, so it's argued, arises unbidden. Well, indeed, if you switch off and meditate, that does indeed seem to be the case. Parts of your consciousness mind decouple from other parts. The thoughts and the awareness of those thoughts divide, and one is inclined to think, I am not that stream of thoughts. This indeed is the insight of contemplatives like Harris and many others who follow in a Buddhist tradition. But of course, that too is an observation of the internal subjective state, and is no more an indication of deep subjective truth about the nature of the person

### 49m

than I am my stream of thoughts. So when you're in a contemplative state, you can have this sensation that you are not this stream of thoughts. And then when you're not in a contemplative state, you can have the experience of being a stream of thoughts. Why is one a road to truth while the other is not? I'm yet to hear an answer to that. Both the lost in thought state, and as the Buddhists or Sam Harris would say, the divested of sense of self state, they're both subjective states. Sam wants to call one the true I when the I is lost, compared to the perpetually lost in thought state. What I want to say here is that the subjective experience of the lost in thought state is actually very probative of what people are. When you are lost in thought, or just thinking, you're thinking without concern about who you are.

### 50m

You're in the flow state of thinking, one thought, and then reasoning and concluding what comes next. Now, I do, in fact, indeed feel capable of deciding. Thinking and deciding. Explaining. I feel actively involved in all choices. I feel a subjective sense of free will. This may seem to be too clever by half, but I want to say that when paying attention to how you think while lost in thought, you can see options arising, being criticised, and either surviving the process or not. And this is key. You can slow things down or not, and notice how critical you are being or not. Now, at this point, the free will denier will say, ah, but you did not choose to have the thought, should I slow down and think more carefully now or not? That thought wasn't given to you. Okay, fine. I do not control all of my thoughts, but the choice to slow down or not is mine. Sometimes I do, and sometimes I don't. And not controlling all of the contents of consciousness does not mean I do not control some.

### 51m

And my conception of free will is only that I control some and not all. It is as if to say, because the state cannot control all aspects of our lives, it controls none, and hence does not exist. That would be absurd. The all or nothing conception of free will here is misguided. So free will is that sense that we choose to create one explanation rather than another. We attempt to create. When a person encounters a problem, they seek to find solutions. Seek implies the solutions are out there in some sense. But this is rare. Usually they must come from within. They must be created by the mind, the creative mind of the individual. Outwardly, we see solutions generated by a person. Inwardly, we experience the phenomena of consciousness as we encounter the world and construct inside our minds a representation of the outside reality. This act of construction is a creative one.

### 52m

We create that representation, and when there is a problem, we attempt to solve it. Or we can ignore the problem. These are our choices. There are a literally infinite number of problems we might attempt to solve. But, and this is the key, we choose only some to work on. And this is the exercise of free will. If you have a major life decision, should I enter into university and study finance or physics? If you think on this carefully over many months, you might think, my mathematical skills are reasonably good. I could do either. I like to work with numbers and constrain things. Quantitatively. Finance seems to be interesting. I could be wealthy and do physics on the side. But then directly working on fundamental problems might be more rewarding than simply earning a high wage. I think and create options, but maybe I shouldn't worry. Maybe I shouldn't care what I do and just not think about it at all. The choice to think hard on this or not, to decide which knowledge to create,

### 53m

the knowledge about what to do with my life or not, is something I am free to choose to do. Or not. Free will is the freedom for me to get a good sense of my preferences and to act upon them. That is to say, choose. But to know what my preferences truly are, let's say finance or physics, I need to carefully deliberate, solve that problem, create that knowledge. Outwardly, eventually, someone will notice me enrol in physics and finance as a double major. But inwardly, only I know that I felt many different things. Emotions like excitement or confusion and the thrill at making a new insight. It was all me. It was largely me that contemplated this problem for hours on end. I chose this interesting solution, the double degree, as it helped me satisfy both desires. I was free to do otherwise, to not think about it at all or to choose one over the other. But no, I chose to do what I did. I did that, not the laws of physics. Inwardly, it was a conscious sensation of free will. But outwardly, it was a choice made to create knowledge in that area.

### 54m

These things are all facets of the one same phenomenon, what it means to be a human and explain the world. So these many different attributes that David speaks about and he guesses that they seem to have all come at once in a jump to universality and that maybe they are all aspects of the same thing. This is the sense that I get as well. I don't think we have good, hard to vary explanations of any of these things. Free will, creativity, consciousness, meaning, so on. But the best that we can do is to think about it. And what we do is what's provided here in this chapter, I would suggest. Let's return to it. Let me continue reading. In any case, we should expect AI to be achieved in a jump to universality, starting from something much less powerful. In contrast, the ability to imitate a human imperfectly or in specialized functions is not a form of universality. It can exist in degrees. Hence, even if chatbots did at some point start becoming much better at imitating humans or fooling humans or at fooling humans,

### 55m

that would still not be a path to AI. Becoming better at pretending to think is not the same as coming closer to being able to think. There is a philosophy whose basic tenet is that those are the same. It's called behaviorism, which is instrumentalism applied to psychology. It is a doctrine that psychology can only or should only be the science of behavior, not of minds. That it can only measure and predict relationships between people's external circumstances, stimuli, stimuli, and their observed behavior responses. The latter is, unfortunately, exactly how the Turing test asked the judge to regard a candidate AI. Hence, it encouraged the attitude that if a program could fake AI well enough, it would have achieved it. But ultimately, a non-AI program cannot fake AI. The path to AI cannot be through ever better tricks for making chatbots more convincing. A behaviorist would no doubt ask, what exactly is the difference between giving a chatbot a very rich repertoire of tricks, templates, and databases and giving it AI abilities?

### 56m

What is an AI program? How can AI help change the political system? For example, we can say that a teleworking program can make apps and devices more efficient. AI can help me maneuver my contacts or set up something in my transition schedule more effectively. This allows an AI processor to create for me follow-up support and tools that can inform me and lead my progress. And an AI programmer can actually make it work and understand me more effectively. This shows the kinds of benefits these technologies provide. As you can see in this slide as a new development for Google, I've learned a lot. Next of all, I'm ready for a new chapter of search. It will have the comprehensive bag of human tricks. It can do all the things that a human can, except for one, the key one being it won't be able to create explanations, because if it's able to do that, it doesn't need any of the others, as it turns out. Let me just explain a little bit more here. So I've written down a few notes here. Let me just read through those. So this idea that AI is essentially just a collection of programs, each of which can do one of the things people can, and together can do all the things that people can do, except they're silicon-based robots, so they can do them much faster and with fewer errors, the conclusion follows almost unavoidably.

### 57m

Therefore, they, the silicon-based robots, will be better than humans, more morally valuable superhumans. But that's false. They will never create anything new. Because not one of their programs can. They can do what a person can do, namely create. Or rather, if any of the programs could create something completely new, then none of the other ones would be needed, because that single program is a knowledge-creating program, and so you wouldn't have to preload it with all these other things. And you could preload it with all these other things, but what would be the point? Because once you load it with... With the artificial general intelligence program, it then is a person.

### 58m

And if you try and force any other kind of learning onto it, well, that's coercion. That's saying, here's what you need to learn. But what if you loaded all those others, and then you loaded the AGI program? Same problem. Same problem. Giving the AGI all of the knowledge that perhaps it didn't want to know. Perhaps it doesn't want to be the best baker in the world. The thing about people is that people, humans, and AGI... They're universal explainers. And David has answered the question, what is a person? Few have noticed. As recently as yesterday, I was listening to Sam Harris' podcast, the third time I've mentioned Sam in this episode. I do so, just by the way, because I think he's got the best podcast. And I think he captures very well what the public sentiment on many deep philosophical and scientific issues are at the moment. And so it's a good way to get a handle on what people are thinking about any particular thing.

### 59m

And so just yesterday, episode number 146 for the Waking Up podcast, it was called Digital Capitalism. And his interviewee was Douglas Rushkoff. And he was very concerned about technology taking over. He was a little bit of a pessimist about aspects of technology. Very well. He's not alone there. And although there were the usual... Noises about tech calamities, he wanted to say something in defense of humans against computers. He argued that the singletarians, you know, these people that think that we're all going to be uploaded into the matrix at some distant point, some omega point in the future, and we'll all have an immortal life based by living in the internet or something or other. That's the singletarians. The singularity is coming. And various other kinds of transhumanists. So this fellow, Douglas... Rushkoff was saying that these people are missing the crucial qualities of people.

### 1h 0m

What the essence of a person is, if you like. Now, I kind of agree with him there. But he thinks, he tried to articulate what he thought the crucial qualities of humans are, the crucial qualities of people are. And if you're interested in listening, that's around the 43 minute and 20 second mark. And he has a quite... Strange list of qualities that he thinks that humans have that differentiate them from computers. And so the words that he mentioned there were awe and meditation and camaraderie and establishing rapport. It's a strange list. But I sensed more than the particular words that he's used there. He was struggling. He was struggling very much to pin down what it is that's special about people. As so many philosophers and scientists have over the years, they know there's something there. And this is why people... They convert to superstitious ideas. That people have a soul or a spirit or something like that.

### 1h 1m

They can't quite apply a secular scientific understanding to what is special about human beings. What makes them cosmically significant. And so they reach for these other words that just don't resonate. I don't think that anyone who was listening to that would have gone, Oh wow, yes, that's exactly what a person is. A person's special quality is camaraderie and building rapport. It's missing... It's missing something. It seems denuded of the key point. There have been many of these attempts over the years to try and secularize what is almost sacred or divine about people. And they've all failed. The truth is, as we have learned, people are unique because they create explanatory knowledge. That's cosmically significant. It's cosmically significant in a way that's greater than spirit or soul. Than what the religious or supernatural people say. It is saying that if we... If we persevere in solving problems, in doing what we are as people, in creating,

### 1h 2m

we will change not only the entire planet. We will change the galaxy and the universe. That is our capacity as general purpose problem solvers. So yes, he was struggling very much to pin down what it was that was special about people. And Sam Harris himself, likewise, earlier in exactly the same episode, tried to articulate what he thought was special about humans. And he said, If anything is special about humans, it's our use of language. This is the thing that differentiates us from computers or from... But it doesn't differentiate us from computers or animals or anything else. Computers very much use language. Animals use language. So these attempts just miss the key point. It is people creating explanations. Explanations that can change the world, as the subtitle of the book is. That's cosmically... Significantly significant. Tennis players can't put into words their own inexplicit explanation for how to serve a great ace.

### 1h 3m

A person who never learns to speak will nonetheless still create explanations. Language is a facet, a small facet, of something far deeper and broader. Language is, of course, necessary for social interaction, but it can't be the thing that separates a human from other things. Computers do have languages. And so do apes. So it's not language. It is our capacity as universal explainers, or universal knowledge creators, or universal problem solvers. Or general purpose explainers. And so go the synonyms for this same concept. Basically a thing that solves problems by creating explanatory knowledge. But there is another kind of knowledge that we haven't touched on in this episode yet. If we recall from chapter 2, biological knowledge. And so now, here in chapter 7, David now turns to how to attempt to artificially simulate biological knowledge using so-called evolutionary algorithms. And what the state of... that particular science or engineering is.

### 1h 4m

It's a whole other episode in itself, so I'm going to leave what I've said here now. And I'll see you in the next episode. So bye for now. Subtitles by the Amara.org community


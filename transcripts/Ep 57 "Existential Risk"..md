# Ep 57 "Existential Risk".

Original Episode: [Ep 57 "Existential Risk".](https://www.podbean.com/site/EpisodeDownload/PBFFAC01VM945)

Audio Download: [MP3](https://mcdn.podbean.com/mf/download/fk73pj/Episode_57_Existential_Risk_a1qps.mp3)

## Transcript

### 0m

Hello and welcome to TopCast episode 57. This is a bonus episode just for Easter 2021. It was first released to my generous Patreons a week early, but now it is available everywhere. If you'd like to support the podcast and get earlier access to podcasts like this one, consider subscribing at Patreon. If you simply do a Google search for my name Brett Hall or for TopCast along with Patreon, that's Patreon P-A-T-R-E-O-N, then you will find a way to support me and my ongoing efforts here. This podcast was recorded at the request of one of my Patreons. It's all about existential risk, so it's called, and how to think about existential risk in the context of a rational but critical worldview. And especially an optimistic worldview. As you will hear, the quality is a little bit different

### 1m

to my other podcasts, in large part because I'm outside and so you can hear the sounds of the Sydney suburbs. And it is quite unrehearsed. I also hope it's reasonably good humoured on such a serious topic. But without further ado, here are my thoughts on existential risk. One of my Patreon subscribers some months ago now asked me to speak about existential risk and I've been unable to get around to doing that until now. So this is largely off the cuff with my ideas about how to think about existential risk. Existential risk is something that certain scientists are extremely interested in trying to explain to the rest of us about how there won't merely be problems that are going to distract governments and people, but there will be problems which could wipe out all of civilisation, the entirety of humanity. And if we

### 2m

want to think seriously about this, we're going to have to calculate the likelihood of these things occurring. When we're talking about existential risk, what we mean is the end to existence, to a large extent. The end of the existence of humanity. So we're talking about the biggest possible problems that we can conceive of. We're trying to understand and analyse and then predict the probability that these things are actually going to wipe us out at some point in the future. So what things appear in this category? Well, one thing that appears in this category is something called a super volcano. There is a super volcano, so we are told geologically, lurking beneath Yellowstone National Park in the United States. And this thing is so vast that should it erupt in the way in which some volcanologists think that it may one day, it could lead to an outgassing event. In other words, huge amounts of gas like carbon dioxide and various other

### 3m

noxious volatiles, not to mention the dust and ash that this sort of thing kicks up into the atmosphere. That volcano will be of such a magnitude that it will blot out the sun for some time. And this could lead to global catastrophe. It could lead to crop failure in particular. So that's one thing that might happen. We could have agriculture suffering across the entire planet. Not only agriculture suffering, but the vegetation that wild animals eat could suffer as well. And indeed, the aquatic environment might suffer because there won't be enough light getting down to the oceans where the algae is producing food for the fish to eat. That could happen. There's certain other apocalyptic scenarios from such a volcanic event like huge molten balls of rock being thrown up into the air and coming back down and setting ablaze large areas of forest. This might happen as well. And so that is only part of the problem. This could be an existential threat to human civilization. And even if it

### 4m

didn't kill everyone outright, it could cause such disruption to the economy that that could lead to wars and it could precipitate something that eventually leads to a huge decline in global progress, global well-being, and perhaps global population. So how could you calculate the probability of something like this happening? Well, you look through the geologic record. You look at what has hitherto happened in the past, and the naturally occurring events. When we think about whenever volcanoes have gone off, we can just simply look at the rocks. Rock strata are a wonderful record of being able to preserve precisely what was going on in the atmosphere at any point in the past. We know when there have been huge volcanic eruptions because we can see the ash deposits, the remnants of those volcanoes fall down to earth and then gradually get compacted between layers of different rock over time, over millions of years. Probability calculation on this account is a pretty straightforward exercise. You simply look

### 5m

for the evidence. You look for how frequently volcanoes have occurred in the past. And it might very well be the case, and indeed it is the case, that a planet like the Earth, being geologically active, becomes less geologically active over time, over the time scale of millions, hundreds of millions, billions of years. And there are good geophysical reasons for this. The Earth is gradually cooling over time. So in fact, the Earth is becoming geologically more stable over time. The number of huge volcanic eruptions is decreasing. But that aside, we do know that volcanoes will continue to erupt. And indeed, some big volcanoes we would expect to erupt bigger than they have in the history of any human that's currently alive. So apparently we can do some sort of probability. We can make some sort of calculation about when the next volcano is going to happen. And looking in the past, we can see there have been certain events that have caused the extinction of entire species, and vast numbers of species at that.

### 6m

I think it's relatively well known in the geology, paleontology, astrophysics community, that soon after the KT event, the event that wiped out the dinosaurs, this was the asteroid that crashed in the Yucatan Peninsula, that soon after that, there were large volcanic eruptions, sometime within a million years, I think, of that KT event. And it's caused debates in the community of geologists and paleontologists about what really was either the proximate or the ultimate cause of the demise of the dinosaurs. Was it indeed that asteroid crashing down to the Earth and kicking up a whole bunch of dust into the atmosphere? Was that the thing that wiped out the dinosaurs because the vegetation died? Or was it the fact that that asteroid actually precipitated a whole bunch of different volcanic eruptions around the world? And it was that, the volcanic eruptions, that actually caused the demise of the dinosaurs. I think the consensus now is it was the asteroid that was directly responsible for the extinction of the dinosaurs. But it's

### 7m

interesting that serious scientists did indeed come up with the idea that volcanoes could have, could have wiped out the dinosaurs. So volcanoes are in that category of existential threat to life on Earth. What else might we put here? Well, asteroids, as we just mentioned, it has certainly been the case over the geological, cosmological history of the universe and history of the planet, many, many species have been wiped out simultaneously by asteroid collisions. In fact, there's something like five or six so-called mass extinction events. Many of these may have been caused by asteroids. They may also have been caused by other cosmological events. One of my favorite examples is, of course, the possibility that just perhaps it was a supernova explosion that wiped out life on Earth. I've often thought that of all the disaster movies that have so far about volcanoes going off or disaster movies about a sudden rapid climate change or earthquakes or

### 8m

name the disaster and you will have a disaster movie about it. But as far as I know, we don't have a disaster movie yet about supernovae going off somewhere nearby. Because if a supernova went off somewhere nearby right now, well, when I say right now, if the explosion reached us right now, what would happen, depending upon the intensity of that supernova explosion, you would have irradiation on one side of the planet, and perhaps this would cause severe disruption to the atmosphere. But essentially, largely speaking, the people on the other side of the planet not facing the supernova explosion wouldn't be affected immediately. But they would gradually come to learn that as the Earth rotates, the supernova radiation will begin to exterminate things that came within sight of the supernova. So it could very well be the case, for example, that if Australia was facing the direction of the supernova blast, that everyone in Australia and on that side of the planet would be wiped out by the supernova explosion. But the frightening thing

### 9m

would be for the people on the other side of the planet, let's say in the United States, trying to phone or zoom with their friends, they would find that no one in the country of Australia would be answering because we would have been almost immediately exterminated by the supernova, depending upon the conditions. Who knows what this supernova is like? But for argument's sake, this is a possibility that you could have people wiped out on one side of the planet, and the other side of the planet, these people would have time to dig deep beneath the Earth and to try and do something. An escape from the supernova radiation. A supernova radiation would probably only last for hours to days, perhaps. So in theory, they could go beneath the surface of the Earth, escape from the worst effects. But while meantime, the people on the other side of the planet would have been largely exterminated, unless they happened to already be underground or perhaps inside. I'm not sure, but this is my idea for a disaster movie. But how do we think about the probability, the existential threat of supernovas? Well, we can do, we can rely upon the astrophysical knowledge of the community of astronomers out there,

### 10m

and we kind of have a good idea of the stability of stars that we can see. And there are some unstable stars. There are some stars that are large red giants. There are some stars that are variable stars. And we can see most of these, and we understand the astrophysical processes. We understand well what it takes for a star to explode in a supernova blast, such that the amount of radiation reaching the Earth would be a threat to life here on Earth. And there is nothing, there is no star within the minimum radius required that is likely to explode anytime within the next few hundred thousand years that we need to be worried about. All the stars that are relatively close to the Earth, within the danger zone, so to speak, are not really near the point where they're about to explode anytime soon, within the lifetime of anyone alive, or the lifetime of anyone's grandchildren who are alive. Now, what about asteroids? Well, the interesting thing about asteroids is they used to hit the Earth far more frequently than what they do today. Now, why is that? Well, the reason is

### 11m

the planets, as they orbit the Sun, hoover up or vacuum up all the debris in their orbit. So, along the orbit that Earth takes, which is in between Venus and Mars, obviously, once upon a time there were lots and lots of rocks, lots of debris, lots of asteroids out there. But they've long since been attracted to the Earth and have crashed into the Earth. There was this period billions of years ago called the late heavy bombardment, and it's just like it happened by huge asteroids crashing into the Earth. And this sterilized the surface of the planet pretty much. But since then, so many of the asteroids have either been shepherded into that region between Mars and Jupiter into the asteroid belt, and they tend not to leave that asteroid belt. They don't tend to be pushed towards the Earth. It can happen. It doesn't happen frequently. The other place to find asteroids, apart from the asteroid belt, is called the Kuiper belt. The Kuiper belt's out beyond the orbit of Neptune. Now, asteroids could be kicked out of their orbits from there and head towards the Earth as well. And then,

### 12m

of course, going out further beyond the Kuiper belt, we have the mysterious Oort cloud. And the Oort cloud is the place in the solar system, the very boundaries of the solar system. It certainly is part of the solar system because it's gravitationally bound to the sun, which makes it part of the solar system. But out there we have rocky, icy bodies, which, when they come towards the sun, end up being comets. And so they could crash into the Earth as well. And we know that this has happened in the past, and we know that these objects are still out there. And so we can kind of do some kind of calculation about the frequency with which objects like this come towards the Earth and potentially intersect with the Earth's orbit, and how often, out of all those intersections with the Earth's orbit, they actually intersect with the Earth itself, causing a mass extinction event. Now, of all the ways of considering existential risk, I do like analysis of the asteroid question. Because absent people, and this is a theme in the work of David Deutsch and in my attempts to explain it, absent people, we can do these kind of calculations. The frequency with which

### 13m

asteroids intersect with the Earth, crash into the Earth, and wipe out some huge proportion of the life on Earth. For example, at the time of the dinosaurs, 62 million years ago, when the asteroid crashed in the southern part of Mexico, in the Yucatan Peninsula. And that, we know, caused the extinction of the dinosaurs, and a whole bunch of other species as well, and related species. It was an extinction-level event. That kind of asteroid would be an existential threat. But is such an asteroid an existential threat today? Well, it would be if we didn't find out about it before it crashed into the Earth. But we do have people who are interested in searching for such asteroids. NASA has programs searching, scanning the skies for asteroids, for dangerous asteroids which might crash into the Earth. So can we put a number, a probability, you know, one in the thousand of this happening within the next century, of a huge asteroid coming and wiping out all life on Earth? Well, we can certainly put a number on the probability that such

### 14m

an asteroid could intersect with the point at which the Earth is on its orbit as it goes around the Sun. But let's say such a calculation is done, where we find, for example, that a particular asteroid is located, and this asteroid is large. Let's say it's 10 kilometers across. That's large by asteroids that crash into the Earth terms. And let's say we picked a number, one in 100. And that one in 100 was the chance that the asteroid was going to crash into the Earth 100 years from now. That's quite a high probability. And given the potential effects of such an asteroid crashing into the Earth, namely the wiping out of civilization, a 10 kilometer asteroid rushing at 10 times the speed of a bullet straight into the Earth, is going to do untold damage. If it hits the oceans, it's going to create tsunamis around the entire world. If it hits a particular country on land, it's going to kick up dust. Either way, it's going to kick up dust. Including large amounts of molten or fiery rock, which will come crashing down onto the forest of

### 15m

the world, setting them ablaze. And so simultaneously, we'll have forest fires all over the Earth caused by the fallout from such an asteroid. And the cloud of dust that will be kicked up into the atmosphere will blot out the Sun for perhaps some weeks, destroying much of the plant life on Earth, and certainly much of the food for animals and for people. So if we had such a probability event, a one in 100 chance, given the fact that it could potentially wipe out all of civilization, we would want to do something about that. And if we had a 100 year lead in time, we would start doing things about that. And probably the first thing we would do is we'd want to refine the probability. We'd have more astrophysicists with better telescopes refining the probability, because you need to be able to do extra precise calculations in order to refine that probability. And by refine that probability, I mean, find out if it really is one. 100 or something different. Now, if after 20 years, we built different telescopes, and we built better

### 16m

supercomputers, and we engage the help of ever brighter astrophysicists to come up with a new calculation. And if we found to our relative horror, that in fact, the probability wasn't one in 100, but it was one in three of this asteroid actually intersecting with the Earth, then I think we'd start to take steps to mitigate the effects. And by mitigate the effects, I mean, to push the asteroid off course. Our creativity would then begin to change that probability. And there's all sorts of interesting ways that engineers and astrophysicists and clever people have thought about in order to push an asteroid out of the way. This is an engineering problem. There's nothing in the laws of physics that says asteroids can't be pushed out of the way by human beings. It may be difficult, it may be a bit of an engineering challenge. But one can imagine people like Elon Musk and Jeff Bezos actually putting rockets up there. And that's the way we're going to do it. Putting rockets up there and just physically pushing the asteroid out of the way. And you don't have to push the asteroid very far. And you don't need much rocketry power in order to push an

### 17m

asteroid out of the way. Because an asteroid isn't being propelled anywhere. All it's doing is following a particular trajectory, which is affected almost solely by the gravitation of the Sun. It's following an orbit around the Sun. And sadly for us, that orbit intersects with the Earth's own orbit at the time when the Earth is actually at the place where the asteroid is going to intersect with that orbit. Indeed, scientists have done calculations where if you were to simply cover one half of the asteroid in aluminium foil or white paint, the differential effect of sunlight on such an object would be enough to push the asteroid hither or thither in such a way that it would no longer intersect with the orbit of the Earth. So there are solutions to this. And we know what these solutions are. And whatever the level of technology today, it's going to be vastly greater in 10 years. And if we know there's a one in three chance in 10 years from now, of the asteroid crashing into the Earth 90 years hence, then if we wait yet another 10 years,

### 18m

we might find that, oh, our calculation of one in three is different again. It might in fact be two in three. So it seems more and more likely that indeed this asteroid is going to crash into the Earth, in which case we will have to use our technology to get up there, to get rockets. We'll have a huge global effort in order to push this thing out of the way. And that chance would then change. It would no longer be two in three. It would be two in three, absent us doing anything. But we would do something. This is the way in which critical rationalism and optimism in the style of David Deutsch deals with existential risks. We have to have a start of problem solving. We cannot calculate the probability of things absent people because people exist and people actually have effects in the world. This is why many of us have a little giggle at things like the doomsday clock. The doomsday clock is a serious endeavor back invented back in 1947 by atomic scientists who are very concerned,

### 19m

rightly concerned with global nuclear war. And so in order to push home the point to the rest of society that these weapons were terribly catastrophic, terribly worrisome, the damage they could do could indeed lead to the collapse of civilization if they were used just like conventional weapons were. So the doomsday clock was supposed to shock people into realizing how scary nuclear weapons were, how seriously we should take the threat of nuclear war. And so saying that, for example, we're at five minutes to midnight was supposed to be a measure of how unseriously governments of the world and other people in the world were when it came to considering the seriousness of the threat of nuclear catastrophe. That via accident or indeed intention, governments, the world could precipitate a nuclear winter. They could precipitate something that no one in

### 20m

their right mind would want to happen. And so this was the seriousness of the doomsday clock. But since then, of course, the doomsday clock has been used for just about any kind of threat. And so all different threats are now incorporated into the doomsday clock. So now scientists get up in front of the cartoonish doomsday clock and even include things like climate change, climate change, energy, and the like, which could make for a true disaster. But in the same way, that it was always people who were in charge of whether or not the global nuclear winter was going to happen after global nuclear war, it is still people that are in charge of whether or not we're going to have catastrophic global climate change. And the scientists and others who set the doomsday clock, never consider that people's creativity has a real effect. And we should be optimistic about that. So instead of the minute hand getting ever closer to midnight, it should be getting further and further away from midnight.

### 21m

And this is why some of us giggle at it now. They're not taking seriously the notion that people are becoming more moral, more risk-averse. We are more willing to engage in serious solutions to the most pressing problems. So rather than us getting closer to global catastrophic climate change, we are getting further away. Despite all the political noise, despite the concerns about whether or not particular governments are enacting this or that policy, we are gaining the power, the knowledge and the power to literally change the thermostat of the planet. People already laugh at certain wealthy individuals like Bill Gates, I think was one, who suggested that we put a kind of aerosol into the atmosphere in order to reflect some of the sunlight. Now, this may not be a good idea. But at least in principle, people who are wealthy enough are thinking about doing this kind of thing. And it might potentially be the case that one day we have something similar.

### 22m

I wouldn't suggest aerosols because I don't know that people have done sufficient scientific work to figure out what negative effects the aerosol might have. We want, of course, to have a solution which fixes the problem of climate change without causing as a side effect a whole bunch of problems that are even worse than what climate change was. What other kinds of global risk are there? Of existential risk? Well, I've just gone to the Wikipedia page that is titled Global Catastrophic Risk, which is where you get taken if you do a Google search for existential risk. You end up at the Wikipedia page. And there is a section there on likelihood. And so let me read through some. These are the estimated probabilities for human extinction before 2100. Now, there are a whole bunch of respectable scientists and philosophers who make similar calculations. Now, Martin Rees says, The great astrophysicist, British cosmologist Martin Rees, who wrote a book called On the Future, Prospects for Humanity in 2018, which talks about all the ways in which we might die.

### 23m

And before that, in fact, he had our final hour. That was back in 2003. Both of these about the potential for human beings going extinct. And so he makes some calculations, some educated guesses about the potential for humans being wiped out through either their own. So whether it's actions or indeed inactions. So we're culpable either way. And I think he's right to say that we would be culpable either way. But can he make such a prediction, such a probability assessment? Now, someone else who speaks in these terms is, of course, Nick Bostrom. And Nick Bostrom has written books and papers. He's probably one of the world's foremost thinkers on this topic of existential risk. And so he's written a book called Global Catastrophic Risks. That's one of his books. has serious academic papers, for example, Existential Risks, Analyzing Human Extinction Scenarios, which has been cited by 641 people. And in terms of citations, that's pretty serious.

### 24m

We might have a look at that one shortly. There are other books as well, published just recently, one by Toby Ord. That book is called The Precipice, Existential Risk and the Future of Humanity. Now, I put all of these ways of speaking about existential risk into the same category as disaster movies. And I think that they sell really well and people are very excited about them because it is thrilling. I honestly think it's emotionally thrilling to go along to a disaster movie. I know I love it. I loved Deep Impact, the movie all about an asteroid. Was it a comet? I think it was a comet. Comet coming to Earth and literally crashing into the Earth. And you get to see the wonderful special effects and consider all the ways in which civilization could be upended and how impotent people are in the face of some cosmological event like this. It's fun. It's fun. But of course, in disaster movies, people fail to solve the problem. And often they fail to solve the problem because they just don't try hard enough. They just don't put the right effort into doing what needs to be done in time. Because otherwise there wouldn't be a movie.

### 25m

There wouldn't be much of a movie if the terrible event didn't actually happen. It would be an anti-climax. But of course, in our world, we want the anti-climax. We do not want the end to come. So we are going to put in more effort than what the people in any disaster movie ever do, and end up failing. And before I get to the example probabilities from the Wikipedia article, let me just skim through Nick Bostrom's professional, peer-reviewed, well-cited article on this topic. And let's just go through his specific examples of existential risks. So he has different categories of existential risks that he calls bangs, crunches, shrieks and whimpers. That is just a way of him saying how quickly the extinction event is going to happen. The bang is something that happens immediately and there's nothing anyone can do about it. It's a huge explosion across the entirety of the earth and we don't have enough time to respond. All the way through to a whimper, which is where some

### 26m

post-human civilization arises, but involves in a direction that leads gradually, but irrevocably, to either the complete disappearance of the things we value, or to a state where those things are realized to only a minuscule degree of what could have been achieved. In other words, what he's hinting at there is the gradual takeover of some kind of artificial intelligence of humanity. And that could happen so slowly and imperceptibly that by the time we think that the AI is a danger, it will be too late for us to do anything about it. Nick Bostrom is very animated by this kind of science fiction scenario. But his particular forms of bangs, these include deliberate misuses of nanotechnology. So nanotechnology, which might itself cause some sort of pandemic, some way of getting into our bloodstream and destroying our bodies. Nuclear holocaust, that favorite of people over many decades now. The fact that we're living in a simulation perhaps,

### 27m

and it gets shut down, that could be a bang event that causes the end of existence. This is a serious philosophical paper. Badly programmed super intelligence could indeed take over much more quickly than what people think. Genetically engineered biological agents, that's one category is something unforeseen. And so what he says about this, this is a serious suggestion for the way in which we might all go extinct. And we should consider this very seriously. He says, we need a catch-all category. It would be foolish to be confident that we have already imagined and anticipated all significant risks. Future technological or scientific developments may very well reveal novel ways of destroying the world. So that's interesting. And I couldn't agree with him more there. There could always be something unforeseen that could wipe us all out immediately. But of course, with any of these claims about something unforeseen that could wipe us out all immediately, something unforeseen could happen, namely the creative output of people that could solve that thing in ways in which Nick Bostrom

### 28m

hasn't thought, or which I haven't thought. But either you can go down the pessimistic route, the exciting, thrilling way of thinking that Nick Bostrom could be right and we're all going to die at some point. It's a pessimistic way to live your life. Or you could go down an alternative route where you think, yes, there are dangers out there, but people, people are grand, not exactly gods, but we share some features of what traditional gods are like. We do have control over the laws of nature to a very large extent. We're able to move matter, create knowledge, and solve problems. We are not being blown around like leaves in the wind. We actually have control. What else does Nick Bostrom have among his ways in which we might all die? Plain old physical masters. So for example, a particle accelerator experiment might produce something strange and unforeseen. This too is something that other physicists have said. When the Large Hadron Collider was switched on, people said, well, it might create little black holes which could

### 29m

swallow the Earth. Of course, the particle physicists should have been talking to the astrophysicists because astrophysicists knew that there were particles of much, much higher energy than anything the LHC, the Large Hadron Collider, was producing, crashing into the other upper atmosphere and have been crashing into the other upper atmosphere ever since the Earth began. And because we knew that this had been going on for billions of years and hitherto had never created any black hole, we know that this isn't going to happen. And so Nick Bostrom simply has the physics wrong there. There's nothing that a particle accelerator can do artificially, man-made, that already is not occurring in nature, even within, even on the Earth in the upper atmosphere. What else do we have on the list? Naturally occurring diseases. What if AIDS was as contagious as the common cold? This could be, this could wipe us out. Finally, he gets to asteroid and comet impacts. Runaway global warming. Resource depletion or ecological destruction. Misguided world governments or other static social equilibrium stops technological process. Yes, well, I agree

### 30m

with him there. And in fact, I would say that a lot of Nick Bostrom's own solutions to these questions of global catastrophic risk, in fact, involve stasis. They involve us not making further progress. Because progress is something that he regards as being particularly hazardous. So there's an irony lurking here. And he continues, take over by a transcend, a transcending upload. I think he's starting to repeat himself now. So this is more about artificial intelligence taking over the world. Oh, another one, which is in a separate category, flawed super intelligence. I think that's the same sort of thing. Repressive totalitarian global regime. Now he's really running out of ideas. Now he really is repeating himself. Something unforeseen again. Okay. He's really starting to wind down. I'm down into the category, which is labeled whimpers now. He mentions killed by an extraterrestrial civilization. So that makes the list as

### 31m

well. So we have to watch out for that. Okay. I think that will do for now. I'll leave behind Nick Bostrom's long, interesting list of science fiction scenarios. Or basically, interesting premises. I think that Hollywood producers could do well reading through such a list and finding good directors to make stories out of those ways in which we can die. I'd certainly watch some of those movies, but as for taking them seriously in a scientific sense, well, of course, we should take problems seriously, but it's those unforeseen ones I'd rather be focused on and not focused on by being fixated upon them, but focused on to the extent that it means we need to have continued scientific research. We need to have continued open flourishing societies. We need to give ourselves the best opportunity. To be able to create the knowledge in time when these unforeseen things happen. So back at Wikipedia, we have the estimated probabilities for human extinction before 2100.

### 32m

And the source for this, the source for this table in Wikipedia is the Future of Humanity Institute in something published in 2008. And so what they say is the chance of nuclear terrorism causing the extinction of human beings before 2100, the chance of that is 0.03%. A natural pandemic, 0.05%. A nanotechnology accident, 0.5%. Nuclear war, 1%. Engineered pandemic, 2%. All wars, including civil wars, 4%. Super intelligent AI, 5%. Molecular nanotechnology weapons, 5%. And the overall probability of taking all these things together and somehow summing them, etc. So there's a 19% chance of humans going extinct before 2100. Pfft. There's a question that looms here, isn't there? How do they know? How do they know any of this?

### 33m

Ha ha. So, we could probably have a reasonable way in which we could calculate the frequency at which asteroids collide with the Earth. And how that frequency has decreased for good astrophysical reasons over time. We know the reasons why they do that out of the vastness of the universe. And we know that the frequency of the earth and the earth that is Earth has different just because we have an asteroid in the reasons why the frequency has decreased over time. But nonetheless, we could indeed calculate, given what we know about the asteroids that exist in the solar system, what the chance is of an asteroid crashing into the Earth. I think the more scary asteroids, of course, are the ones, and there's been a recent example of asteroids from well outside the solar system. So goodness knows where these asteroids are coming from, other solar systems or parts of exploded stars and so on and so forth, traveling far more quickly at much higher velocity than anything within the solar system. So if these things start heading towards the Earth, well, that's a bit of a worry. I don't know how you mitigate against those. Some of these things might have relativistic speeds, something that is coming from a supernova traveling half the speed of light, traveling

### 34m

across the entire galaxy, or in fact, possibly from another galaxy. These are scary options to consider. They could crash into the Earth, and we probably wouldn't see it coming. That would be one of those bang events. Literally a bang event that Nick Bostrom's talked about. But I don't think it's worth worrying about. I mean, if you seriously tried to calculate the probability of something like that intersecting with the Earth, you'd probably need to wait many times the lifetime of the universe before an event like that actually happened, given the size of the universe and the small size of such an asteroid. Okay, but asteroids, at least we have the potential of trying to come up with the number of asteroids in the solar system. Indeed, you know, in the universe, we could probably have some sort of estimate of that, and the chance of any of those intersecting with the orbit of the Earth. But when it comes to something like nuclear terrorism, putting the number at 0.03%, how on Earth that is done? This is an in principle impossible number to use. It's an in principle impossible number

### 35m

to calculate. Why? Because nuclear terrorism is predicated upon bad ideologies, bad, bad ideas, people who have some ridiculous idea about how perhaps if they were to wipe a whole bunch of people out in a nuclear terrorist event, they're going to receive some metaphysical award. You know, they're going to be welcomed into heaven by the creator of the universe who wants them to do something like that. Now I don't know what the chance is of such ideologies taking hold on the planet. I'd like to think that as time goes on, the number of people subscribing to such ridiculous ideas, actually decreases over time. People become more enlightened, more moral. That's been the lesson of the history of humanity. But I don't know if in 10 years that everyone isn't converted to some terrible fundamentalist form of worshipping the great spaghetti monster. And

### 36m

perhaps these pastafarians, as they're called, these pastafarians become genocidal and they just want to wipe out all of humanity. The chance of that happening is not knowable. We don't know because people are creative and they can create terrible ideas and those terrible ideas can lead to things like terrorist acts. And in theory, those terrorist acts could be so severe, they could involve nuclear weapons that they could actually wipe out large portions of humanity. But that, one reason to think that that is not as likely as what people think is because the good guys have an advantage. The pastafarians, the people who want to, for example, the fundamentalist pastafarians, people who worship the great spaghetti monster, they are always at a disadvantage because number one, they're very, very focused on ensuring the purity of their religion of pastafarianism. And because they're so focused on the purity of being a pastafarian, they're not focused on how

### 37m

to get past the security, the security of an open dynamic society. And the open dynamic society governments are very, very focused and very, very skillful. They're very knowledgeable in trying to prevent terrorists from ever succeeding. And they do thwart the terrorists. It's not perfect. Of course, now and again, the terrorists actually do win. But by and large, people who are terrorists are not all that bright. They are at a disadvantage. Now, it's not to say that terrorists are uneducated. We know very well, Sam Harris speaks eloquently on the fact that there are a certain breed of terrorists who are highly educated, highly educated indeed. But this is the exception rather than the rule. Usually, people who become better at their understanding of science also become better at their understanding of at least a folk form of philosophy. They become better, therefore, in terms of reasoning in the moral sphere, too. They generally just become better people and less likely to be captured by

### 38m

terrible ideologies. Now, it's not to say that's impossible. We know of famous examples where, indeed, terrorists have been among the most educated people in society. But this will become diminishingly unlikely development. So how do you calculate something like the chance of nuclear terrorism wiping out humanity before 2100? This is called Mathematism. It's a form of scientism, really. It's just applying statistical methods to places where there is no business in applying them, because they cannot possibly account for all the ways in which people would use knowledge creation, their personal creativity, in order to mitigate the chance of this terrible thing happening. Now, the chance of super intelligent AI wiping us out by 2100, they're not going to be able to do that. They're going to have to be able to do that. But what does the chance of super intelligent AI arising at all within the next century? I don't know. How do they know? We don't know anything about what it takes to produce the super intelligent AI, much less the chance of super intelligent AI actually being an existential

### 39m

threat to humanity. So even if we did, whatever the chances of super intelligent AI arising, how do we know that the super intelligent AI is not going to make it less likely that the super is going to wipe out humanity. Maybe there will be many such super intelligent AIs. And just like people, maybe some of them will be genocidal and want to wipe out humanity. But maybe they'll be in the severe minority of super intelligent AIs. Maybe there'll be these other AIs who are good and who will keep in check the bad super intelligent AIs. So I don't know how we calculate the probability of any of this happening. It seems a little absurd. They say that wars have a four percent chance of wiping us out before 2100. But the number of wars keeps on decreasing and certainly the number of large-scale wars keeps on decreasing. As trade increases, people realize that everyone's boat rises with the same tide. And although at the moment, for example,

### 40m

there is much friction between, let's say, China and the United States, it's not like China and the United States are on the brink of nuclear war. I don't think they're anywhere near it. I think they're as far from having a nuclear war as any two countries have been over the existence of nuclear weapons. I don't think the war is about to become hot. The worst thing that the Chinese can do to their so-called opponents over in the U.S. is to have a trade embargo, is to affect trade. This is what the Chinese are doing in order to affect the United States. And people in the United States are far more concerned about that. And indeed, people in China are far more concerned about that too. Everyone wants to be healthier, wealthier, and wiser. They want to learn more, they want to make more widgets, they want things to be better for the people that are around them. They're not so interested in taking over more territory. Modulo, of course, yes, the Chinese are having issues in Taiwan, they're

### 41m

having issues in the seas around Southeast Asia. Yes, these political things happen. But it really is nothing like the Second World War. There's nothing about China, for example. That resembles anything like Germany at the beginning of the Second World War, just deciding to take over one territory after another. China knows that the free world is arrayed against them when it comes to taking over land. The free world remains far more powerful than China. There is a huge alliance of people, Australia, India, the United Kingdom, the United States, Canada, Europe, just like all other democracies, are arrayed against totalitarian regimes who might decide to encroach upon the territory of smaller nations. And insofar as it would rise to anywhere near the level of true conflict, that true conflict is played out diplomatically. It may be played out at worst in terms of

### 42m

trade, as I say. Sanctions might be put on one country or another. There might be mean things said in the political sphere. But there does not seem to be a real conflict. There does not seem to be a path to global war. So I don't think, not only is there not a path towards global war, there's not a path towards global war such that it would ever wipe out humanity. There's no 4% chance of wars wiping out civilization before 2100. Not only is this not known, but of all the explanations about the ways in which nations interact in the modern era, one with another, the last thing any of them want to do is go to war. No matter how much they're bellicose, no matter how much they talk big, it is all talk, it is all talk. And so it's kind of laughable that here we have an attempt at a calculation. But how? This number has plucked out of the air. Now, I'm just clicking on the reference for this

### 43m

table and I'm taken to another Wikipedia page. The Future of Humanity Institute. At Oxford University. Now why should I be unsurprised? laughing The future of humanity, really? The future of humanity? wow, wow, wow. You want me to go? It might be a bit more detailed at this point. here we go. See, I've got output space on the disciplineland. If you're looking at the plot plot, of humanity institute it turns out i didn't know who these people were i should have guessed oxford university its director is philosopher nick bostrom and its research staff and associates include toby ord of all the people that are in charge of this oh and here we go the future of humanity institute the logo is there it's a lovely diamond logo it looks like a looks like it could be a logo for a bank but um it says the introduction is sharing an office and working closely with the center for effective altruism the institute's stated objective is to focus research where it can make

### 44m

the greatest positive difference for humanity in the long term i'm sorry this is no this is very serious this is about existential risk can't laugh about this but why did i should have guessed that there are associated with the center for effective altruism now i've written and spoken about effective altruism before effective altruism is i i would say effectively socialism but putting that aside um they want to make a positive difference for humanity in the long term well the positive difference they want to make of course comes out of the deepest form of pessimism so if you can extract if you can extract pessimism if you can extract extract positivity out of pessimism well excellent wonderful i i i more power to them um but i think there's a better way i don't think we need to start with a premise about all the ways in which we are possibly going to die and from there somehow figure out ways in which to well let's be clear control society and to come up

### 45m

with solutions which no doubt are about trying to have these philosophers the philosopher kings advise governments on how to control society and how to control society and how to control society on what kind of policies will best redistribute wealth and control people because that is almost always what their prescriptions are now i shouldn't make light of it i think that we can come together i think that that optimists in the david deutsch sense or to some extent the stephen pinker sense to some extent the matt riley sense so we can indeed find common cause here we can indeed find common cause with what i would regard as the most pure pessimists that i know of in academia the people who are very very very fixated upon this notion of existential risk upon the ways in which people are going to destroy themselves either through creating technology so technology is always a thing that these people are worried

### 46m

about the ai is going to not only take our jobs it could potentially wipe us out so that's our actions doing things or nuclear weapons our actions doing things or our inaction so for example in not doing the thing that we're doing we're not doing the thing that we're doing that we need to do in time to prevent the global catastrophe so for example not solving the global pandemic when it comes not doing sufficient things about climate change in order to prevent catastrophes in the economy the prescriptions that are on offer and we won't go through the prescriptions this this podcast is already far longer than what i ever intended it to be but the prescriptions usually do come down to some kind of social control and although i have had serious disagreements with the ways in which governments around the world have handled the pandemic one thing you can take away from this is that people do want to try and solve a problem once it is apparent that the problem is affecting their lives their personal lives they are willing to do what it takes when the government says to lock down they lock down there wasn't some sort of

### 47m

civil huge civil unrest there wasn't huge amounts of civil unrest there were some protests here and there and i think that's certainly healthy that those sort of things and it wasn't like there was a huge movement against developing a vaccine quite the opposite people worked on making multiple vaccines faster than had ever been done before the lesson of this cannot go unnoticed we do not need a stance of pessimism we do not need to think that the only option here is completely upending the global economy and free trade and freedom and democracy in order to go down a totalitarian route which is what these people typically suggest but is instead to have more of what it took to do what we did which was freely enabling researchers to come together to come up with solutions and in this case it was of course a very fast production of a vaccine where did that come from by the way where did these vaccines come from two places the united states and great britain two of the freest countries in the world even even the

### 48m

technological powerhouses of asia weren't able to do it as quickly and why it's purely down to freedom and the way in which people think or are willing to think and criticize ideas it's not an accident that oxford university was one of the places that the vaccine was produced most quickly despite the fact that that's where nick bostrom is i don't know i don't know if nick bostrom might have been arguing uh if he ever had a chance to cross paths with one of these vaccine researchers that they should be very careful about producing the vaccine after all who knows what terrible existential threat a vaccine could have been if it was produced in a way that was completely different from the way the vaccine could be if it wasn't perfectly tuned to dealing with the coronavirus maybe the vaccine could cause all sorts of and of course there are people like this by the way of course there are the anti-vaxxer people who think that the cure is worse than the disease itself or think the cure is some sort of way of the government trying to get into your

### 49m

bloodstream and silly stuff like that so there will always be pessimists and naysayers all that the rest of us can do i suppose is to remain positive in the face of having existential risks listed we don't need to analyze them mathematically for the most part what we need to do is to have a stance not merely of focusing on any specific problem but of generally creating more knowledge in all areas and so that's why we need to continue to fund basic science basic research at the most fundamental level because the deeper our scientific understanding the more widely those solutions can be applied to practical applications okay i think that will do for today this was uh largely off the cuff as i say but um if you want to read more uh simply type in global existential risk or just existential risk and perhaps throw nick bostrom in there as well um he is a serious philosopher and despite the fact i laugh i think we we need to laugh sometimes because uh i think the topic is dealt with in a far too serious way when there are many things

### 50m

to be concerned about particular problems of let's say for example um local poverty uh disease um people dying too early uh this kind of thing is actually going on right now all the time we don't need to be fixated upon how the entire planet all simultaneously might get wiped out by the gray nanotechnology goo okay until next time bye

